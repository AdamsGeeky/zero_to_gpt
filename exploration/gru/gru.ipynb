{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [],
   "source": [
    "#!pip install torch torchtext sentencepiece datasets\n",
    "# Try opus books dataset for translation - https://huggingface.co/datasets/opus_books"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import functorch\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "sys.path.append(os.path.abspath(\"../../data\"))\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 1\n",
    "SP_VOCAB_SIZE = 1000\n",
    "VOCAB_SIZE = SP_VOCAB_SIZE + 2\n",
    "CHUNK_LENGTH = 15\n",
    "Y_CHUNK_LENGTH = 15"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset cnn_dailymail (/Users/vik/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de)\n",
      "100%|██████████| 3/3 [00:00<00:00, 143.64it/s]\n",
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=tokens.txt --model_prefix=cnn --vocab_size=1000 --model_type=unigram\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: tokens.txt\n",
      "  input_format: \n",
      "  model_prefix: cnn\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 1000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(319) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(174) LOG(INFO) Loading corpus: tokens.txt\n",
      "trainer_interface.cc(375) LOG(INFO) Loaded all 679 sentences\n",
      "trainer_interface.cc(390) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(390) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(390) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(395) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(456) LOG(INFO) all chars count=53179\n",
      "trainer_interface.cc(467) LOG(INFO) Done: 99.953% characters are covered.\n",
      "trainer_interface.cc(477) LOG(INFO) Alphabet size=71\n",
      "trainer_interface.cc(478) LOG(INFO) Final character coverage=0.99953\n",
      "trainer_interface.cc(510) LOG(INFO) Done! preprocessed 679 sentences.\n",
      "unigram_model_trainer.cc(138) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(142) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(193) LOG(INFO) Initialized 6916 seed sentencepieces\n",
      "trainer_interface.cc(516) LOG(INFO) Tokenizing input sentences with whitespace: 679\n",
      "trainer_interface.cc(526) LOG(INFO) Done! 3634\n",
      "unigram_model_trainer.cc(488) LOG(INFO) Using 3634 sentences for EM training\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=3104 obj=13.0083 num_tokens=8223 num_tokens/piece=2.64916\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=2744 obj=11.6276 num_tokens=8322 num_tokens/piece=3.0328\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=2054 obj=11.7263 num_tokens=8879 num_tokens/piece=4.32278\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=2049 obj=11.5921 num_tokens=8888 num_tokens/piece=4.33773\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=1536 obj=12.0737 num_tokens=9888 num_tokens/piece=6.4375\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=1536 obj=11.8959 num_tokens=9889 num_tokens/piece=6.43815\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=1152 obj=12.4735 num_tokens=11038 num_tokens/piece=9.5816\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=1152 obj=12.297 num_tokens=11062 num_tokens/piece=9.60243\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=1100 obj=12.3699 num_tokens=11219 num_tokens/piece=10.1991\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=1100 obj=12.3403 num_tokens=11226 num_tokens/piece=10.2055\n",
      "trainer_interface.cc(604) LOG(INFO) Saving model: cnn.model\n",
      "trainer_interface.cc(615) LOG(INFO) Saving vocabs: cnn.vocab\n"
     ]
    }
   ],
   "source": [
    "from cnn_data import generate_data, decode_batch\n",
    "\n",
    "train, valid, test = generate_data(train_length=10, valid_length=2, vocab_size=SP_VOCAB_SIZE, batch_size=BATCH_SIZE, chunk_length=CHUNK_LENGTH, y_chunk_length=Y_CHUNK_LENGTH, device=DEVICE)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "class GRUEncoderCell(nn.Module):\n",
    "    def __init__(self, input_units, hidden_units, output_units):\n",
    "        super(GRUEncoderCell, self).__init__()\n",
    "        self.input_units = input_units\n",
    "        self.hidden_units = hidden_units\n",
    "        self.output_units = output_units\n",
    "\n",
    "        k = 1e-2\n",
    "        self.input_weights = nn.Parameter(torch.rand(3, input_units, hidden_units) * 2 * k - k)\n",
    "\n",
    "        self.hidden_weights = nn.Parameter(torch.rand(3, hidden_units, hidden_units) * 2 * k - k)\n",
    "        self.hidden_biases = nn.Parameter(torch.rand(2, 1, hidden_units) * 2 * k - k)\n",
    "\n",
    "        self.output_weight = nn.Parameter(torch.rand(hidden_units, output_units) * 2 * k - k)\n",
    "        self.output_bias = nn.Parameter(torch.rand(1, output_units) * 2 * k - k)\n",
    "\n",
    "    def forward(self, x, prev_hidden):\n",
    "        # Compute the regular RNN forward pass\n",
    "        # Compute update and reset gates for GRU\n",
    "        update_gate = torch.sigmoid(x @ self.input_weights[0,] + prev_hidden @ self.hidden_weights[0,] + self.hidden_biases[0,])\n",
    "        reset_gate = torch.sigmoid(x @ self.input_weights[1,] + prev_hidden @ self.hidden_weights[1,] + self.hidden_biases[1,])\n",
    "\n",
    "        # This is a potential new state based on the reset gate\n",
    "        proposed_state = torch.tanh(x @ self.input_weights[2,] + (prev_hidden * reset_gate) @ self.hidden_weights[2,])\n",
    "        hidden_x = torch.tanh((1-update_gate) * prev_hidden + update_gate * proposed_state)\n",
    "        # Compute output vector\n",
    "        output_y = hidden_x @ self.output_weight + self.output_bias\n",
    "        return output_y, hidden_x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [],
   "source": [
    "class GRUDecoderCell(nn.Module):\n",
    "    def __init__(self, input_units, context_units, hidden_units, output_units):\n",
    "        super(GRUDecoderCell, self).__init__()\n",
    "        self.input_units = input_units\n",
    "        self.hidden_units = hidden_units\n",
    "        self.output_units = output_units\n",
    "\n",
    "        k = 1e-2\n",
    "\n",
    "        self.input_weights = nn.Parameter(torch.rand(3, input_units, hidden_units) * 2 * k - k)\n",
    "        self.hidden_weights = nn.Parameter(torch.rand(3, hidden_units, hidden_units) * 2 * k - k)\n",
    "\n",
    "        self.context_weights = nn.Parameter(torch.rand(3, context_units, hidden_units) * 2 * k - k)\n",
    "\n",
    "        self.hidden_biases = nn.Parameter(torch.rand(2, 1, hidden_units) * 2 * k - k)\n",
    "\n",
    "        self.hidden_attention_weight = nn.Parameter(torch.rand(hidden_units, hidden_units) * 2 * k - k)\n",
    "        self.context_attention_weight = nn.Parameter(torch.rand(context_units, hidden_units) * 2 * k - k)\n",
    "        self.attention_weight = nn.Parameter(torch.rand(1, hidden_units) * 2 * k - k)\n",
    "\n",
    "        self.output_weight = nn.Parameter(torch.rand(hidden_units, output_units) * 2 * k - k)\n",
    "        self.output_bias = nn.Parameter(torch.rand(1, output_units) * 2 * k - k)\n",
    "\n",
    "        self.batched_diag = functorch.vmap(torch.diag)\n",
    "\n",
    "    def forward(self, prev_y, prev_hidden, context):\n",
    "        # Compute attention between the encoder hidden states and the previous decoder hidden state\n",
    "\n",
    "        # Swap batch and sequence\n",
    "        batch_size = context.shape[1]\n",
    "\n",
    "        # Swap axes so the first dimension of context_attn is batch\n",
    "        context_attn = torch.bmm(context.swapaxes(0,1), self.context_attention_weight.unsqueeze(0).expand(batch_size,-1,-1))\n",
    "        # Swap back since prev_hidden is by batch.  This makes the first dim of cross sequence\n",
    "        cross = torch.tanh(context_attn.swapaxes(0,1) + prev_hidden @ self.hidden_attention_weight)\n",
    "        # This will be of dimension batch, sequence_length, 1\n",
    "        attention = torch.bmm(cross.swapaxes(0,1), self.attention_weight.T.unsqueeze(0).expand(batch_size, -1, -1))\n",
    "        # Drop the last singleton dimension\n",
    "        attention = attention.squeeze(2)\n",
    "        # Softmax the predictions across each batch\n",
    "        probs = torch.softmax(attention, 1)\n",
    "        diagonalized_probs = self.batched_diag(probs)\n",
    "        positional_contexts = torch.sum(torch.bmm(diagonalized_probs, context.swapaxes(0,1)), dim=1).reshape(batch_size, self.input_units)\n",
    "\n",
    "        # Compute GRU update and reset gates + proposed state and final hidden state\n",
    "        update_gate = torch.sigmoid(prev_y @ self.input_weights[0,] + prev_hidden @ self.hidden_weights[0,] + self.hidden_biases[0,] + positional_contexts @ self.context_weights[0,])\n",
    "        reset_gate = torch.sigmoid(prev_y @ self.input_weights[1,] + prev_hidden @ self.hidden_weights[1,] + self.hidden_biases[1,] + positional_contexts @ self.context_weights[1,])\n",
    "\n",
    "        proposed_state = torch.tanh(prev_y @ self.input_weights[2,] + (prev_hidden * reset_gate) @ self.hidden_weights[2,] + positional_contexts @ self.context_weights[2,])\n",
    "\n",
    "        hidden_x = torch.tanh((1-update_gate) * prev_hidden + update_gate * proposed_state)\n",
    "        # Compute output based on hidden state\n",
    "        output_y = hidden_x @ self.output_weight + self.output_bias\n",
    "        return output_y, hidden_x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, in_sequence_len, out_sequence_len, hidden_units=512, embedding_len=VOCAB_SIZE, layers=2):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.in_sequence_len = in_sequence_len\n",
    "        self.out_sequence_len = out_sequence_len\n",
    "        self.hidden_units = hidden_units\n",
    "        self.embedding_len = embedding_len\n",
    "        self.layers = layers\n",
    "\n",
    "        self.embedding = nn.Embedding(embedding_len, hidden_units)\n",
    "        self.encoders = nn.ModuleList([GRUEncoderCell(input_units=hidden_units, hidden_units=hidden_units, output_units=hidden_units) for _ in range(layers)])\n",
    "        self.decoders = nn.ModuleList([GRUDecoderCell(input_units=hidden_units, context_units=hidden_units, hidden_units=hidden_units, output_units=hidden_units) for _ in range(layers)])\n",
    "\n",
    "        self.linear = nn.Linear(in_features=hidden_units, out_features=embedding_len)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        batch_size = x.shape[0]\n",
    "        # Move batch to the second dimension, so sequence comes first\n",
    "        y = y.swapaxes(0,1)\n",
    "        # Embed the input sequence to reduce dimensionality\n",
    "        embedded = self.embedding(x).swapaxes(0,1)\n",
    "\n",
    "        # Encode the input sequence\n",
    "        # Both tensors will have sequence then batch\n",
    "        enc_hiddens = torch.zeros((1, self.layers, batch_size, self.hidden_units), device=DEVICE)\n",
    "        enc_outputs = torch.zeros((1, self.layers, batch_size, self.hidden_units), device=DEVICE)\n",
    "        for j in range(self.in_sequence_len):\n",
    "            seq_enc_hiddens = torch.zeros((1, batch_size, self.hidden_units), device=DEVICE)\n",
    "            seq_enc_outputs = embedded[j,:,:].unsqueeze(0).to(DEVICE)\n",
    "            for i in range(self.layers):\n",
    "                output, hidden = self.encoders[i](seq_enc_outputs[i,], enc_hiddens[j,i])\n",
    "                # Add first sequence axis\n",
    "                hidden = hidden.unsqueeze(0)\n",
    "                output = output.unsqueeze(0)\n",
    "                seq_enc_outputs = torch.cat((seq_enc_outputs, output), dim=0)\n",
    "                seq_enc_hiddens = torch.cat((seq_enc_hiddens, hidden), dim=0)\n",
    "\n",
    "            enc_hiddens = torch.cat((enc_hiddens, seq_enc_hiddens[1:].unsqueeze(0)), dim=0)\n",
    "            enc_outputs = torch.cat((enc_outputs, seq_enc_outputs[1:].unsqueeze(0)), dim=0)\n",
    "\n",
    "        # Decode to the output sequence\n",
    "        # Pass in context\n",
    "        context = enc_hiddens[1:,-1,:,:]\n",
    "        # Both tensors will have the first dimension be the sequence\n",
    "        dec_hiddens = torch.zeros(1, self.layers, batch_size, self.hidden_units, device=DEVICE)\n",
    "        dec_outputs = torch.zeros((1, self.layers, batch_size, self.hidden_units), device=DEVICE)\n",
    "        for j in range(self.out_sequence_len):\n",
    "            # Use either the actual previous y (from the input), or the generated y if the input sequence is shorter than the generation steps.\n",
    "            prev_y = y[j,:,:] if y.shape[0] > j else torch.softmax(dec_outputs[j,-1,:,:], dim=1)\n",
    "            # Run embedding over previous y state\n",
    "            prev_y = prev_y.argmax(dim=1).int()\n",
    "            prev_y = self.embedding(prev_y)\n",
    "\n",
    "            seq_dec_hiddens = torch.zeros((1, batch_size, self.hidden_units), device=DEVICE)\n",
    "            seq_dec_outputs = prev_y.unsqueeze(0).to(DEVICE)\n",
    "            for i in range(self.layers):\n",
    "                output, hidden = self.decoders[i](seq_dec_outputs[i,], dec_hiddens[j,i,], context)\n",
    "                # Add first sequence axis\n",
    "                hidden = hidden.unsqueeze(0)\n",
    "                output = output.unsqueeze(0)\n",
    "                seq_dec_outputs = torch.cat((seq_dec_outputs, output), dim=0)\n",
    "                seq_dec_hiddens = torch.cat((seq_dec_hiddens, hidden), dim=0)\n",
    "\n",
    "            dec_hiddens = torch.cat((dec_hiddens, seq_dec_hiddens[1:].unsqueeze(0)), dim=0)\n",
    "            dec_outputs = torch.cat((dec_outputs, seq_dec_outputs[1:].unsqueeze(0)), dim=0)\n",
    "\n",
    "        # Move batch back to axis 0\n",
    "        out_hiddens = dec_hiddens[1:,-1,:,:].swapaxes(0,1)\n",
    "        out_output = self.linear(dec_outputs[1:,-1,:,:].swapaxes(0,1))\n",
    "        return out_output, out_hiddens\n",
    "\n",
    "def generate(sequence, target, vocab_size):\n",
    "    pred, _ = model(sequence, target[:,0,:].unsqueeze(1))\n",
    "    prompts = decode_batch(sequence.cpu(), vocab_size=vocab_size)\n",
    "    texts = decode_batch(torch.argmax(pred, dim=2).cpu(), vocab_size=vocab_size)\n",
    "    correct_texts = decode_batch(torch.argmax(target, dim=2).cpu(), vocab_size=vocab_size)\n",
    "\n",
    "    displays = []\n",
    "    for p, t, ct in zip(prompts, texts, correct_texts):\n",
    "        displays.append(f\"{p} | {ct} | {t}\")\n",
    "    return displays"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "model = EncoderDecoder(CHUNK_LENGTH, Y_CHUNK_LENGTH, hidden_units=512, layers=1).to(DEVICE)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:01,  5.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty anti-tank weapon turns | Empty anti-tank weapon turn | HarEal:mal star Danank we diggg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:01,  6.13it/s]\n",
      "10it [00:01,  6.24it/s]\n",
      "1it [00:00,  3.08it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[84], line 16\u001B[0m\n\u001B[1;32m     14\u001B[0m     loss \u001B[38;5;241m=\u001B[39m loss_fn(pred, target)\n\u001B[1;32m     15\u001B[0m     loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m---> 16\u001B[0m     \u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     17\u001B[0m     train_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n",
      "File \u001B[0;32m~/.virtualenvs/nnets/lib/python3.10/site-packages/torch/optim/optimizer.py:140\u001B[0m, in \u001B[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    138\u001B[0m profile_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOptimizer.step#\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m.step\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(obj\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n\u001B[1;32m    139\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mrecord_function(profile_name):\n\u001B[0;32m--> 140\u001B[0m     out \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    141\u001B[0m     obj\u001B[38;5;241m.\u001B[39m_optimizer_step_code()\n\u001B[1;32m    142\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m out\n",
      "File \u001B[0;32m~/.virtualenvs/nnets/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001B[0m, in \u001B[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m     26\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclone():\n\u001B[0;32m---> 27\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.virtualenvs/nnets/lib/python3.10/site-packages/torch/optim/adamw.py:162\u001B[0m, in \u001B[0;36mAdamW.step\u001B[0;34m(self, closure)\u001B[0m\n\u001B[1;32m    158\u001B[0m             max_exp_avg_sqs\u001B[38;5;241m.\u001B[39mappend(state[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmax_exp_avg_sq\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m    160\u001B[0m         state_steps\u001B[38;5;241m.\u001B[39mappend(state[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstep\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m--> 162\u001B[0m     \u001B[43madamw\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparams_with_grad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    163\u001B[0m \u001B[43m          \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    164\u001B[0m \u001B[43m          \u001B[49m\u001B[43mexp_avgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    165\u001B[0m \u001B[43m          \u001B[49m\u001B[43mexp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    166\u001B[0m \u001B[43m          \u001B[49m\u001B[43mmax_exp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    167\u001B[0m \u001B[43m          \u001B[49m\u001B[43mstate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    168\u001B[0m \u001B[43m          \u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mamsgrad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    169\u001B[0m \u001B[43m          \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    170\u001B[0m \u001B[43m          \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta2\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    171\u001B[0m \u001B[43m          \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mlr\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    172\u001B[0m \u001B[43m          \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mweight_decay\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    173\u001B[0m \u001B[43m          \u001B[49m\u001B[43meps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43meps\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    174\u001B[0m \u001B[43m          \u001B[49m\u001B[43mmaximize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmaximize\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    175\u001B[0m \u001B[43m          \u001B[49m\u001B[43mforeach\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mforeach\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    176\u001B[0m \u001B[43m          \u001B[49m\u001B[43mcapturable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcapturable\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    178\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m loss\n",
      "File \u001B[0;32m~/.virtualenvs/nnets/lib/python3.10/site-packages/torch/optim/adamw.py:219\u001B[0m, in \u001B[0;36madamw\u001B[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001B[0m\n\u001B[1;32m    216\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    217\u001B[0m     func \u001B[38;5;241m=\u001B[39m _single_tensor_adamw\n\u001B[0;32m--> 219\u001B[0m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    220\u001B[0m \u001B[43m     \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    221\u001B[0m \u001B[43m     \u001B[49m\u001B[43mexp_avgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    222\u001B[0m \u001B[43m     \u001B[49m\u001B[43mexp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    223\u001B[0m \u001B[43m     \u001B[49m\u001B[43mmax_exp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    224\u001B[0m \u001B[43m     \u001B[49m\u001B[43mstate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    225\u001B[0m \u001B[43m     \u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mamsgrad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    226\u001B[0m \u001B[43m     \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    227\u001B[0m \u001B[43m     \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta2\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    228\u001B[0m \u001B[43m     \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlr\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    229\u001B[0m \u001B[43m     \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mweight_decay\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    230\u001B[0m \u001B[43m     \u001B[49m\u001B[43meps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43meps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    231\u001B[0m \u001B[43m     \u001B[49m\u001B[43mmaximize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmaximize\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    232\u001B[0m \u001B[43m     \u001B[49m\u001B[43mcapturable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcapturable\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.virtualenvs/nnets/lib/python3.10/site-packages/torch/optim/adamw.py:273\u001B[0m, in \u001B[0;36m_single_tensor_adamw\u001B[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001B[0m\n\u001B[1;32m    270\u001B[0m param\u001B[38;5;241m.\u001B[39mmul_(\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m lr \u001B[38;5;241m*\u001B[39m weight_decay)\n\u001B[1;32m    272\u001B[0m \u001B[38;5;66;03m# Decay the first and second moment running average coefficient\u001B[39;00m\n\u001B[0;32m--> 273\u001B[0m \u001B[43mexp_avg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmul_\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbeta1\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madd_\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgrad\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43malpha\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    274\u001B[0m exp_avg_sq\u001B[38;5;241m.\u001B[39mmul_(beta2)\u001B[38;5;241m.\u001B[39maddcmul_(grad, grad, value\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m beta2)\n\u001B[1;32m    276\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m capturable:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 2000\n",
    "DISPLAY_BATCHES = 8\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Run over the training examples\n",
    "    train_loss = 0\n",
    "    for batch, (sequence, target, prev_target) in tqdm(enumerate(train)):\n",
    "        optimizer.zero_grad()\n",
    "        forced_target = prev_target\n",
    "        # Alternate use of teacher forcing vs feeding back own inputs\n",
    "        if np.random.randint(2) == 0:\n",
    "            forced_target = prev_target[:,0,:].unsqueeze(1)\n",
    "        pred, hidden = model(sequence, forced_target)\n",
    "        loss = loss_fn(pred, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if epoch % 10 == 0:\n",
    "            # Show text generated from training prompt as an example\n",
    "            # Don't feed in all of the train y sequences, just the first token\n",
    "            # The other y tokens will be predicted by the model and fed back in\n",
    "            sents = generate(sequence[:DISPLAY_BATCHES], prev_target[:DISPLAY_BATCHES], SP_VOCAB_SIZE)\n",
    "            for sent in sents:\n",
    "                print(sent)\n",
    "\n",
    "            # Compute validation loss.  Unless you have a lot of training data, the validation loss won't decrease.\n",
    "            valid_loss = 0\n",
    "            for batch, (sequence, target, prev_target) in enumerate(valid):\n",
    "                # Only feed in the first token of the actual target\n",
    "                pred, hidden = model(sequence, prev_target[:,0,:].unsqueeze(1))\n",
    "                loss = loss_fn(pred, target)\n",
    "                valid_loss += loss.item()\n",
    "print(f\"Epoch {epoch} train loss: {train_loss} valid loss: {valid_loss}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [],
   "source": [
    "for batch, (sequence, target, prev_target) in enumerate(valid):\n",
    "    # Only feed in the first token of the actual target\n",
    "    pred, hidden = model(sequence, prev_target[:,0,:].unsqueeze(1))\n",
    "    loss = loss_fn(pred, target)\n",
    "    valid_loss += loss.item()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 39, 598,  69, 116, 347, 105, 530, 763,   3, 211,   3, 180,  28,  12,\n           5]])"
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(target, 2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(1.)"
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target[0,0,39]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.return_types.topk(\nvalues=tensor([[[14.9175, 14.7952, 14.5033, 13.5206, 12.9756],\n         [18.7519, 18.7142, 16.6163, 16.2393, 15.4375],\n         [21.8818, 18.9857, 17.8205, 16.4424, 15.3282],\n         [15.6278, 13.0920, 12.4367, 10.8949,  9.8699],\n         [13.8499, 13.5334, 13.3042, 13.1901, 13.1691],\n         [15.2125, 14.6531, 13.4854, 13.0979, 13.0947],\n         [16.4970, 16.4418, 15.5887, 14.8182, 14.6043],\n         [17.3594, 16.6106, 15.6978, 15.2031, 14.1592],\n         [16.8938, 15.9091, 13.8576, 13.3848, 11.3773],\n         [14.3949, 13.9979, 11.7632, 10.7555, 10.7294],\n         [12.5197, 11.4555, 10.6978, 10.5986, 10.3953],\n         [17.5390, 16.1387, 15.6357, 15.4904, 14.8591],\n         [23.3052, 21.8272, 19.7303, 19.0877, 16.8549],\n         [23.6913, 22.5223, 20.6066, 18.8158, 18.3974],\n         [24.6454, 23.5177, 21.7896, 20.0390, 18.2663]]],\n       grad_fn=<TopkBackward0>),\nindices=tensor([[[683, 386, 410, 170,  93],\n         [349,  68, 273, 357, 276],\n         [ 27, 103, 999, 168, 552],\n         [ 65,  34, 252,  43,  18],\n         [ 79, 390, 296, 820, 398],\n         [475,  40,  42,  63, 210],\n         [347, 121, 752, 496, 143],\n         [ 47,  17, 437,  28,  44],\n         [ 50, 323,  61, 176,  87],\n         [352,  26,  37, 358,  32],\n         [  6, 358,  60, 450, 234],\n         [234,  51, 933,  54, 582],\n         [ 54, 234, 933, 627, 582],\n         [ 54, 118, 234, 627, 162],\n         [118,  54, 162, 234, 627]]]))"
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.topk(pred, 5, 2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(-12.4157, grad_fn=<SelectBackward0>)"
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[0,0,10]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
