{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Encoder/decoder transformer, like in the paper\n",
    "\n",
    "# Single Encoder block\n",
    "\n",
    "# Embed input into a length 512 vector\n",
    "# Add positional encoding to the input\n",
    "# Alternating sin and cosine functions across the embedding dimension (512)\n",
    "# i = position in the embedding dimension\n",
    "#PE(pos,2i) = sin(pos/100002i/dmodel )\n",
    "#PE(pos,2i+1) = cos(pos/100002i/dmodel\n",
    "# Sum positional encoding and input\n",
    "# apply dropout with chance .1\n",
    "\n",
    "# Self-attention\n",
    "# Each token in the input sequence is a query\n",
    "# Every other token is a key\n",
    "# Project query and key into a new dimensional space with a linear transform (this is multi-head attention)\n",
    "# We do the following 16 times for multi-head attention\n",
    "# Dot the projects query with the projected key matrix\n",
    "# QK\n",
    "# Attention(Q, K, V ) = softmax( QKT / √dk)V\n",
    "# √dk is the square root of the value dimension\n",
    "# Keys and values are the same in this particular case\n",
    "# Basically, compute an attention score (scalar) between each query and each key, then scale the value by that number\n",
    "# So less relevant other tokens are minimized\n",
    "# Concat the results of the attention equation\n",
    "# Run through another linear layer to reproject\n",
    "# Each attention head outputs 1/16th of the input embedding len\n",
    "\n",
    "# After doing multi-head attention (16)\n",
    "# Apply dropout with chance .1\n",
    "# Add the input to the layer (original embedded sequences) and the output of attention\n",
    "# Run layer normalization (unclear which layer norm to use)\n",
    "\n",
    "# Run a feed forward network\n",
    "# Add input to the layer to the output of the ff network\n",
    "# Normalize again\n",
    "\n",
    "# Single decoder block\n",
    "\n",
    "# Shift outputs right, to start with start token\n",
    "# Do positional encoding\n",
    "# Do dropout with chance .1\n",
    "# Mask outputs, so queries can only see keys that came before the query\n",
    "# Run multi-head attention\n",
    "# Add and norm\n",
    "\n",
    "# Run multi-head attention again, but this time v,k is from encoder stack, and q is from decoder stack\n",
    "# Apply dropout with chance .1\n",
    "# When doing add and norm, add in the decoder stack input\n",
    "\n",
    "# Feed forward\n",
    "\n",
    "# At top of stack, do another linear layer and softmax\n",
    "\n",
    "# Might want to move layer norm inside the residual block - https://arxiv.org/pdf/2002.04745.pdf\n",
    "# Layer normalization - https://arxiv.org/pdf/1607.06450.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import functorch\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "sys.path.append(os.path.abspath(\"../../data\"))\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 1\n",
    "SP_VOCAB_SIZE = 1000\n",
    "TRAIN_SIZE = 500"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset cnn_dailymail (/Users/vik/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de)\n",
      "100%|██████████| 3/3 [00:00<00:00, 43.66it/s]\n",
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=tokens.txt --model_prefix=cnn_dailymail --vocab_size=1000 --model_type=unigram\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: tokens.txt\n",
      "  input_format: \n",
      "  model_prefix: cnn_dailymail\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 1000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(319) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(174) LOG(INFO) Loading corpus: tokens.txt\n",
      "trainer_interface.cc(375) LOG(INFO) Loaded all 2212 sentences\n",
      "trainer_interface.cc(390) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(390) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(390) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(395) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(456) LOG(INFO) all chars count=102097\n",
      "trainer_interface.cc(467) LOG(INFO) Done: 99.951% characters are covered.\n",
      "trainer_interface.cc(477) LOG(INFO) Alphabet size=68\n",
      "trainer_interface.cc(478) LOG(INFO) Final character coverage=0.99951\n",
      "trainer_interface.cc(510) LOG(INFO) Done! preprocessed 2212 sentences.\n",
      "unigram_model_trainer.cc(138) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(142) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(193) LOG(INFO) Initialized 11181 seed sentencepieces\n",
      "trainer_interface.cc(516) LOG(INFO) Tokenizing input sentences with whitespace: 2212\n",
      "trainer_interface.cc(526) LOG(INFO) Done! 6083\n",
      "unigram_model_trainer.cc(488) LOG(INFO) Using 6083 sentences for EM training\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=4830 obj=13.0777 num_tokens=13154 num_tokens/piece=2.7234\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=4246 obj=11.5317 num_tokens=13279 num_tokens/piece=3.12741\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=3180 obj=11.6092 num_tokens=14139 num_tokens/piece=4.44623\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=3174 obj=11.4972 num_tokens=14148 num_tokens/piece=4.45747\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=2380 obj=11.9049 num_tokens=15554 num_tokens/piece=6.53529\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=2380 obj=11.7689 num_tokens=15574 num_tokens/piece=6.5437\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=1785 obj=12.3535 num_tokens=17318 num_tokens/piece=9.70196\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=1785 obj=12.1988 num_tokens=17318 num_tokens/piece=9.70196\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=1338 obj=12.835 num_tokens=19216 num_tokens/piece=14.3617\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=1338 obj=12.6795 num_tokens=19245 num_tokens/piece=14.3834\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=1100 obj=13.1402 num_tokens=20544 num_tokens/piece=18.6764\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=1100 obj=13.0361 num_tokens=20545 num_tokens/piece=18.6773\n",
      "trainer_interface.cc(604) LOG(INFO) Saving model: cnn_dailymail.model\n",
      "trainer_interface.cc(615) LOG(INFO) Saving vocabs: cnn_dailymail.vocab\n"
     ]
    }
   ],
   "source": [
    "from text_data import CNNDatasetWrapper\n",
    "\n",
    "class Wrapper(CNNDatasetWrapper):\n",
    "    split_lengths = [TRAIN_SIZE, math.floor(TRAIN_SIZE * .1), 100]\n",
    "    x_length = 15\n",
    "    target_length = 14\n",
    "\n",
    "wrapper = Wrapper(SP_VOCAB_SIZE, DEVICE)\n",
    "\n",
    "datasets = wrapper.generate_datasets(BATCH_SIZE)\n",
    "train = datasets[\"train\"]\n",
    "valid = datasets[\"validation\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, input_units, attention_heads, mask=False):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.input_units = input_units\n",
    "        self.attention_heads = attention_heads\n",
    "        self.head_units = int(input_units/attention_heads)\n",
    "        self.mask = mask\n",
    "        if self.head_units * self.attention_heads != self.input_units:\n",
    "            raise Exception(\"Invalid input units and heads combo\")\n",
    "\n",
    "        k = math.sqrt(1/self.head_units)\n",
    "        self.query_weights = nn.Parameter(torch.rand(self.attention_heads, input_units, self.head_units) * 2 * k - k)\n",
    "        self.key_weights = nn.Parameter(torch.rand(self.attention_heads, input_units, self.head_units) * 2 * k - k)\n",
    "        self.value_weights = nn.Parameter(torch.rand(self.attention_heads, input_units, self.head_units) * 2 * k - k)\n",
    "\n",
    "    def forward(self, queries, context):\n",
    "        exp_queries = queries.repeat(self.attention_heads, 1, 1)\n",
    "        exp_context = context.repeat(self.attention_heads, 1, 1)\n",
    "        queries = torch.bmm(exp_queries, self.query_weights)\n",
    "        keys = torch.bmm(exp_context, self.key_weights)\n",
    "        values = torch.bmm(exp_context, self.value_weights)\n",
    "\n",
    "        # Sequence-wise softmax, so attention between one sequence and other sequences sums to 1\n",
    "        attention = torch.bmm(queries, keys.swapaxes(1,2)) / np.sqrt(self.head_units)\n",
    "        if self.mask:\n",
    "            # Prevent decoder queries from looking at tokens that come after\n",
    "            # Do this by setting attention to negative infinity, so it is softmaxed to zero in the next step\n",
    "            mask = torch.zeros((attention.shape[-2], attention.shape[-1]))\n",
    "            mask_indices = np.triu_indices(attention.shape[-2], k=1, m=attention.shape[-1])\n",
    "            mask[mask_indices] = -torch.inf\n",
    "            attention += mask\n",
    "\n",
    "        attention = torch.softmax(attention, dim=2)\n",
    "        weighted_values = torch.bmm(attention, values)\n",
    "        weighted_values = weighted_values.swapaxes(0,1).reshape(x.shape[0], -1)\n",
    "        return weighted_values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, input_units, attention_heads, hidden_units=2048):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        self.input_units = input_units\n",
    "        self.attention_heads = attention_heads\n",
    "        self.hidden_units = hidden_units\n",
    "\n",
    "        self.mha = MultiHeadAttention(self.input_units, self.attention_heads)\n",
    "        self.dropout = nn.Dropout(.1)\n",
    "        self.linear1 = nn.Linear(self.input_units, hidden_units)\n",
    "        self.linear2 = nn.Linear(hidden_units, self.input_units)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.lns = nn.ModuleList(nn.LayerNorm(self.input_units) for _ in range(2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        weighted_values = self.dropout(self.mha(x, x))\n",
    "        attn_output = self.lns[0](x + weighted_values)\n",
    "\n",
    "        reprojected = self.dropout(self.linear2(self.relu(self.linear1(attn_output))))\n",
    "        block_output = self.lns[1](attn_output + reprojected)\n",
    "\n",
    "        return block_output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, input_units, attention_heads, hidden_units=2048):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.input_units = input_units\n",
    "        self.attention_heads = attention_heads\n",
    "        self.hidden_units = hidden_units\n",
    "\n",
    "        self.in_attn = MultiHeadAttention(self.input_units, self.attention_heads, mask=True)\n",
    "        self.context_attn = MultiHeadAttention(self.input_units, self.attention_heads)\n",
    "        self.dropout = nn.Dropout(.1)\n",
    "        self.linear1 = nn.Linear(self.input_units, hidden_units)\n",
    "        self.linear2 = nn.Linear(hidden_units, self.input_units)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.lns = nn.ModuleList(nn.LayerNorm(self.input_units) for _ in range(3))\n",
    "\n",
    "    def forward(self, queries, context):\n",
    "        weighted_values = self.dropout(self.in_attn(queries, queries))\n",
    "        attn_output = self.lns[0](queries + weighted_values)\n",
    "\n",
    "        decoder_values = self.dropout(self.context_attn(attn_output, context))\n",
    "        decoder_output = self.lns[1](attn_output + decoder_values)\n",
    "\n",
    "        reprojected = self.dropout(self.linear2(self.relu(self.linear1(decoder_output))))\n",
    "        block_output = self.lns[2](decoder_output + reprojected)\n",
    "        return block_output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_units, hidden_units, attention_heads, blocks=2):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.input_units = input_units\n",
    "        self.hidden_units = hidden_units\n",
    "        self.attention_heads = attention_heads\n",
    "        self.blocks = blocks\n",
    "\n",
    "        k = math.sqrt(1/self.hidden_units)\n",
    "        self.embedding = nn.Embedding(self.input_units, self.hidden_units)\n",
    "        self.dropout = nn.Dropout(.1)\n",
    "        self.encoders = nn.ModuleList(EncoderBlock(hidden_units, attention_heads) for _ in range(self.blocks))\n",
    "        self.decoders = nn.ModuleList(DecoderBlock(hidden_units, attention_heads) for _ in range(self.blocks))\n",
    "        self.output_embedding = nn.Parameter(torch.rand(hidden_units, self.input_units) * 2 * k - k)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        embedded = self.embedding(x)\n",
    "        pos_encoding = torch.from_numpy(self.encoding(embedded.shape[0], self.hidden_units))\n",
    "        network_in = self.dropout(embedded + pos_encoding)\n",
    "        enc_outputs = network_in.unsqueeze(0)\n",
    "\n",
    "        for i in range(self.blocks):\n",
    "            block_output = self.encoders[i](enc_outputs[i])\n",
    "            enc_outputs = torch.cat((enc_outputs, block_output.unsqueeze(0)), dim=0)\n",
    "\n",
    "\n",
    "        dec_outputs = self.embedding(y).unsqueeze(0)\n",
    "        context = enc_outputs[-1]\n",
    "        for i in range(self.blocks):\n",
    "            block_output = self.decoders[i](dec_outputs[i], context)\n",
    "            dec_outputs = torch.cat((dec_outputs, block_output.unsqueeze(0)), dim=0)\n",
    "\n",
    "        token_vectors = dec_outputs[-1] @ self.output_embedding\n",
    "        return token_vectors\n",
    "\n",
    "    def encoding(self, seq_len, embed_len):\n",
    "        #PE(pos,2i) = sin(pos/10000 ^ 2i/dmodel )\n",
    "        #PE(pos,2i+1) = cos(pos/10000 ^ 2i/dmodel\n",
    "        # Pos 1 will embed to a vector, pos 2 will embed to a vector\n",
    "        # Adjacent vector positions will form a sin wave\n",
    "        # plt.plot(np.arange(0,100), np.sin(np.arange(0,100) / np.power(10000.0, (20.0/512))))\n",
    "        encodings = np.zeros((seq_len, embed_len), dtype=np.float32)\n",
    "        for i in range(seq_len):\n",
    "            evens = np.arange(0, embed_len, 2)\n",
    "            odds = np.arange(1, embed_len, 2)\n",
    "            all = np.power(np.full(embed_len, 10000), np.arange(0, embed_len) / embed_len)\n",
    "\n",
    "            sin_embed = np.sin(i/all)\n",
    "            cos_embed = np.cos(i/all)\n",
    "\n",
    "            sin_embed[odds] = 0\n",
    "            cos_embed[evens] = 0\n",
    "            encodings[i,:] = np.sum((sin_embed, cos_embed), axis=0)\n",
    "        return encodings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "outputs": [],
   "source": [
    "def generate(sequence, pred, target, wrapper):\n",
    "    prompts = wrapper.decode_batch(sequence.cpu())\n",
    "    texts = wrapper.decode_batch(torch.argmax(pred, dim=2).cpu())\n",
    "    correct_texts = wrapper.decode_batch(target.cpu())\n",
    "\n",
    "    displays = []\n",
    "    for p, t, ct in zip(prompts, texts, correct_texts):\n",
    "        displays.append(f\"{p} | {ct} | {t}\")\n",
    "    return displays"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2547172622.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;36m  Cell \u001B[0;32mIn[362], line 3\u001B[0;36m\u001B[0m\n\u001B[0;31m    tf =\u001B[0m\n\u001B[0m        ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "x, y, prev_y = train.dataset[0]\n",
    "\n",
    "tf =\n",
    "decoder_out = tf(x, prev_y)\n",
    "logits = torch.softmax(decoder_out, dim=1)\n",
    "generate(x, logits, y, wrapper)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "model = Transformer(wrapper.vocab_size, 512, 8).to(DEVICE)\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=wrapper.pad_token)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "466it [00:23, 20.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss: 5.84051977922988\n",
      "\"Kristen\" identified as aspiring singer | Ashley Youmans, 22 . S | ssss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "466it [00:22, 20.83it/s]\n",
      "466it [00:21, 21.46it/s]\n",
      "466it [00:21, 21.63it/s]\n",
      "466it [00:21, 21.60it/s]\n",
      "466it [00:21, 21.19it/s]\n",
      "466it [00:22, 21.02it/s]\n",
      "466it [00:22, 20.87it/s]\n",
      "466it [00:22, 21.14it/s]\n",
      "466it [00:21, 21.58it/s]\n",
      "466it [00:21, 21.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 train loss: 1.3104079853375583\n",
      "The Boeing 777 aircraft first enter | ed service on June 7, 1995 | ed tovvices June 7, 19 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "466it [00:22, 20.94it/s]\n",
      "466it [00:21, 21.25it/s]\n",
      "466it [00:21, 21.34it/s]\n",
      "466it [00:21, 21.37it/s]\n",
      "466it [00:22, 21.12it/s]\n",
      "466it [00:21, 21.60it/s]\n",
      "466it [00:21, 21.30it/s]\n",
      "466it [00:21, 21.23it/s]\n",
      "466it [00:22, 20.98it/s]\n",
      "466it [00:21, 21.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 train loss: 0.18544078123447977\n",
      "Disgruntled ex-employ | ee said teacher let class | e said said teacher let classs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "466it [00:21, 21.52it/s]\n",
      "466it [00:21, 21.77it/s]\n",
      "466it [00:21, 21.37it/s]\n",
      "466it [00:21, 21.29it/s]\n",
      "466it [00:21, 21.61it/s]\n",
      "466it [00:21, 21.72it/s]\n",
      "466it [00:21, 21.36it/s]\n",
      "466it [00:22, 21.13it/s]\n",
      "466it [00:22, 21.13it/s]\n",
      "466it [00:21, 21.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 train loss: 0.10088506395840625\n",
      "Ugandan officer reports tensions with Libyan | leader's guards during visit . Leaders | leader's guards during visit . Leaders\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "466it [00:21, 21.19it/s]\n",
      "466it [00:21, 21.35it/s]\n",
      "466it [00:22, 21.11it/s]\n",
      "466it [00:21, 21.64it/s]\n",
      "466it [00:22, 21.06it/s]\n",
      "466it [00:22, 21.16it/s]\n",
      "466it [00:21, 21.31it/s]\n",
      "466it [00:23, 20.01it/s]\n",
      "466it [00:21, 21.30it/s]\n",
      "466it [00:22, 20.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40 train loss: 0.08866790364308547\n",
      "Boy, 12, lost leg to car bombing in Iraq; | cousin was killed by blast . He is now  | cousin was killed by blast . He is now \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "466it [00:21, 21.19it/s]\n",
      "466it [00:22, 21.18it/s]\n",
      "466it [00:22, 21.07it/s]\n",
      "466it [00:21, 21.38it/s]\n",
      "466it [00:22, 21.11it/s]\n",
      "466it [00:22, 21.12it/s]\n",
      "466it [00:22, 21.01it/s]\n",
      "466it [00:22, 21.00it/s]\n",
      "466it [00:21, 22.05it/s]\n",
      "466it [00:20, 22.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50 train loss: 0.0632246179596617\n",
      "U.N. to send special envoy | to Myanmar amid reports of crackd | to Myanmardamidds of crackd\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "466it [00:20, 22.31it/s]\n",
      "466it [00:21, 21.94it/s]\n",
      "466it [00:20, 22.54it/s]\n",
      "466it [00:21, 22.06it/s]\n",
      "466it [00:20, 23.02it/s]\n",
      "466it [00:20, 23.03it/s]\n",
      "182it [00:08, 22.52it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[364], line 16\u001B[0m\n\u001B[1;32m     14\u001B[0m     loss \u001B[38;5;241m=\u001B[39m loss_fn(pred\u001B[38;5;241m.\u001B[39mreshape(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, pred\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]), target\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m))\n\u001B[1;32m     15\u001B[0m     loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m---> 16\u001B[0m     \u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     17\u001B[0m     train_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n",
      "File \u001B[0;32m~/.virtualenvs/nnets/lib/python3.10/site-packages/torch/optim/optimizer.py:140\u001B[0m, in \u001B[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    138\u001B[0m profile_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOptimizer.step#\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m.step\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(obj\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n\u001B[1;32m    139\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mrecord_function(profile_name):\n\u001B[0;32m--> 140\u001B[0m     out \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    141\u001B[0m     obj\u001B[38;5;241m.\u001B[39m_optimizer_step_code()\n\u001B[1;32m    142\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m out\n",
      "File \u001B[0;32m~/.virtualenvs/nnets/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001B[0m, in \u001B[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m     26\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclone():\n\u001B[0;32m---> 27\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.virtualenvs/nnets/lib/python3.10/site-packages/torch/optim/adamw.py:162\u001B[0m, in \u001B[0;36mAdamW.step\u001B[0;34m(self, closure)\u001B[0m\n\u001B[1;32m    158\u001B[0m             max_exp_avg_sqs\u001B[38;5;241m.\u001B[39mappend(state[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmax_exp_avg_sq\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m    160\u001B[0m         state_steps\u001B[38;5;241m.\u001B[39mappend(state[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstep\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m--> 162\u001B[0m     \u001B[43madamw\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparams_with_grad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    163\u001B[0m \u001B[43m          \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    164\u001B[0m \u001B[43m          \u001B[49m\u001B[43mexp_avgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    165\u001B[0m \u001B[43m          \u001B[49m\u001B[43mexp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    166\u001B[0m \u001B[43m          \u001B[49m\u001B[43mmax_exp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    167\u001B[0m \u001B[43m          \u001B[49m\u001B[43mstate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    168\u001B[0m \u001B[43m          \u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mamsgrad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    169\u001B[0m \u001B[43m          \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    170\u001B[0m \u001B[43m          \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta2\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    171\u001B[0m \u001B[43m          \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mlr\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    172\u001B[0m \u001B[43m          \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mweight_decay\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    173\u001B[0m \u001B[43m          \u001B[49m\u001B[43meps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43meps\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    174\u001B[0m \u001B[43m          \u001B[49m\u001B[43mmaximize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmaximize\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    175\u001B[0m \u001B[43m          \u001B[49m\u001B[43mforeach\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mforeach\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    176\u001B[0m \u001B[43m          \u001B[49m\u001B[43mcapturable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcapturable\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    178\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m loss\n",
      "File \u001B[0;32m~/.virtualenvs/nnets/lib/python3.10/site-packages/torch/optim/adamw.py:219\u001B[0m, in \u001B[0;36madamw\u001B[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001B[0m\n\u001B[1;32m    216\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    217\u001B[0m     func \u001B[38;5;241m=\u001B[39m _single_tensor_adamw\n\u001B[0;32m--> 219\u001B[0m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    220\u001B[0m \u001B[43m     \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    221\u001B[0m \u001B[43m     \u001B[49m\u001B[43mexp_avgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    222\u001B[0m \u001B[43m     \u001B[49m\u001B[43mexp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    223\u001B[0m \u001B[43m     \u001B[49m\u001B[43mmax_exp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    224\u001B[0m \u001B[43m     \u001B[49m\u001B[43mstate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    225\u001B[0m \u001B[43m     \u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mamsgrad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    226\u001B[0m \u001B[43m     \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    227\u001B[0m \u001B[43m     \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta2\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    228\u001B[0m \u001B[43m     \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlr\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    229\u001B[0m \u001B[43m     \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mweight_decay\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    230\u001B[0m \u001B[43m     \u001B[49m\u001B[43meps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43meps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    231\u001B[0m \u001B[43m     \u001B[49m\u001B[43mmaximize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmaximize\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    232\u001B[0m \u001B[43m     \u001B[49m\u001B[43mcapturable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcapturable\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.virtualenvs/nnets/lib/python3.10/site-packages/torch/optim/adamw.py:316\u001B[0m, in \u001B[0;36m_single_tensor_adamw\u001B[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001B[0m\n\u001B[1;32m    314\u001B[0m     denom \u001B[38;5;241m=\u001B[39m (max_exp_avg_sqs[i]\u001B[38;5;241m.\u001B[39msqrt() \u001B[38;5;241m/\u001B[39m bias_correction2_sqrt)\u001B[38;5;241m.\u001B[39madd_(eps)\n\u001B[1;32m    315\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 316\u001B[0m     denom \u001B[38;5;241m=\u001B[39m \u001B[43m(\u001B[49m\u001B[43mexp_avg_sq\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msqrt\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mbias_correction2_sqrt\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madd_\u001B[49m\u001B[43m(\u001B[49m\u001B[43meps\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    318\u001B[0m param\u001B[38;5;241m.\u001B[39maddcdiv_(exp_avg, denom, value\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39mstep_size)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 100\n",
    "DISPLAY_BATCHES = 2\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Run over the training examples\n",
    "    train_loss = 0\n",
    "    match_pct = 0\n",
    "    for batch, (sequence, target, prev_target) in tqdm(enumerate(train)):\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(sequence, prev_target)\n",
    "\n",
    "        # If you use a batch, need to reshape pred to be batch * sequence, embedding_len to be compatible\n",
    "        # Similar reshape with target to be batch * sequence vector of class indices\n",
    "        loss = loss_fn(pred.reshape(-1, pred.shape[-1]), target.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch} train loss: {train_loss / len(train)}\")\n",
    "            sents = generate(sequence, pred, target, wrapper)\n",
    "            for sent in sents:\n",
    "                print(sent)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
