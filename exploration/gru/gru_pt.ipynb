{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#!pip install torch torchtext sentencepiece datasets\n",
    "# Try opus books dataset for translation - https://huggingface.co/datasets/opus_books"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import functorch\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "sys.path.append(os.path.abspath(\"../../data\"))\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 32\n",
    "SP_VOCAB_SIZE = 1000\n",
    "CHUNK_LENGTH = 10\n",
    "Y_CHUNK_LENGTH = 5\n",
    "TRAIN_SIZE = 500"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from text_data import CNNDatasetWrapper\n",
    "\n",
    "class Wrapper(CNNDatasetWrapper):\n",
    "    split_lengths = [TRAIN_SIZE, math.floor(TRAIN_SIZE * .1), 100]\n",
    "    x_length = CHUNK_LENGTH\n",
    "    target_length = Y_CHUNK_LENGTH\n",
    "\n",
    "wrapper = Wrapper(SP_VOCAB_SIZE, DEVICE)\n",
    "\n",
    "datasets = wrapper.generate_datasets(BATCH_SIZE)\n",
    "train = datasets[\"train\"]\n",
    "valid = datasets[\"validation\"]\n",
    "test = datasets[\"test\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, in_sequence_len, out_sequence_len, embedding_len, hidden_units=512, layers=2):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.in_sequence_len = in_sequence_len\n",
    "        self.out_sequence_len = out_sequence_len\n",
    "        self.hidden_units = hidden_units\n",
    "        self.embedding_len = embedding_len\n",
    "        self.layers = layers\n",
    "\n",
    "        self.embedding = nn.Embedding(embedding_len, hidden_units)\n",
    "        self.encoder = nn.GRU(input_size=hidden_units, hidden_size=hidden_units, num_layers=layers)\n",
    "        self.decoder = nn.GRU(input_size=hidden_units * 2, hidden_size=hidden_units, num_layers=layers)\n",
    "        self.linear = nn.Linear(in_features=hidden_units, out_features=embedding_len)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        batch_size = x.shape[0]\n",
    "        # Move batch to the second dimension, so sequence comes first\n",
    "        y = y.swapaxes(0,1)\n",
    "        # Embed the input sequence to reduce dimensionality\n",
    "        embedded = self.embedding(x).swapaxes(0,1)\n",
    "\n",
    "        # Encode the input sequence\n",
    "        # Both tensors will have sequence then batch\n",
    "        initial_hidden = torch.zeros((1 * self.layers, batch_size, self.hidden_units), device=DEVICE)\n",
    "        enc_output, enc_hidden = self.encoder(embedded, initial_hidden)\n",
    "\n",
    "        # Decode to the output sequence\n",
    "        # Pass in context\n",
    "        context = enc_output\n",
    "        # Both tensors will have the first dimension be the sequence\n",
    "        dec_hiddens = torch.zeros(1, batch_size, self.hidden_units, device=DEVICE)\n",
    "        dec_outputs = torch.zeros((1, batch_size, self.hidden_units), device=DEVICE)\n",
    "        for j in range(self.out_sequence_len):\n",
    "            # Use either the actual previous y (from the input), or the generated y if the input sequence is shorter than the generation steps.\n",
    "            prev_y = y[j,:,:] if y.shape[0] > j else torch.softmax(dec_outputs[j,:,:], dim=1)\n",
    "            # Run embedding over previous y state\n",
    "            prev_y = prev_y.argmax(dim=1).int()\n",
    "            prev_y = self.embedding(prev_y)\n",
    "\n",
    "            output, hidden = self.decoder(torch.cat((prev_y, context[-1]), dim=1).unsqueeze(0), dec_hiddens[j,].unsqueeze(0),)\n",
    "            dec_hiddens = torch.cat((dec_hiddens, hidden), dim=0)\n",
    "            dec_outputs = torch.cat((dec_outputs, output), dim=0)\n",
    "\n",
    "        # Move batch back to axis 0\n",
    "        out_hiddens = dec_hiddens[1:,:,:].swapaxes(0,1)\n",
    "        out_output = self.linear(dec_outputs[1:,:,:].swapaxes(0,1))\n",
    "        return out_output, out_hiddens\n",
    "\n",
    "def generate(sequence, target, wrapper):\n",
    "    pred, _ = model(sequence, target[:,0,:].unsqueeze(1))\n",
    "    prompts = wrapper.decode_batch(sequence.cpu())\n",
    "    texts = wrapper.decode_batch(torch.argmax(pred, dim=2).cpu())\n",
    "    correct_texts = wrapper.decode_batch(torch.argmax(target, dim=2).cpu())\n",
    "\n",
    "    displays = []\n",
    "    for p, t, ct in zip(prompts, texts, correct_texts):\n",
    "        displays.append(f\"{p} | {ct} | {t}\")\n",
    "    return displays"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "model = EncoderDecoder(wrapper.x_length, wrapper.target_length, hidden_units=512, layers=1, embedding_len=wrapper.vocab_size).to(DEVICE)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "EPOCHS = 1000\n",
    "DISPLAY_BATCHES = 8\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Run over the training examples\n",
    "    train_loss = 0\n",
    "    match_pct = 0\n",
    "    for batch, (sequence, target, prev_target) in tqdm(enumerate(train)):\n",
    "        optimizer.zero_grad()\n",
    "        forced_target = prev_target\n",
    "        # Alternate use of teacher forcing vs feeding back own inputs\n",
    "        #if np.random.randint(2) == 0:\n",
    "        #    forced_target = prev_target[:,0,:].unsqueeze(1)\n",
    "        pred, hidden = model(sequence, forced_target)\n",
    "        loss = loss_fn(pred, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        match_pct += torch.sum(torch.argmax(target, 2) == torch.argmax(pred, 2)) / (Y_CHUNK_LENGTH * BATCH_SIZE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if epoch % 10 == 0:\n",
    "            # Show text generated from training prompt as an example\n",
    "            # Don't feed in all of the train y sequences, just the first token\n",
    "            # The other y tokens will be predicted by the model and fed back in\n",
    "            sents = generate(sequence[:DISPLAY_BATCHES], prev_target[:DISPLAY_BATCHES], wrapper)\n",
    "            for sent in sents:\n",
    "                print(sent)\n",
    "            \"\"\"\n",
    "            # Compute validation loss.  Unless you have a lot of training data, the validation loss won't decrease.\n",
    "            valid_loss = 0\n",
    "            for batch, (sequence, target, prev_target) in enumerate(valid):\n",
    "                # Only feed in the first token of the actual target\n",
    "                pred, hidden = model(sequence, prev_target[:,0,:].unsqueeze(1))\n",
    "                loss = loss_fn(pred, target)\n",
    "                valid_loss += loss.item()\n",
    "            print(f\"Epoch {epoch} train loss: {train_loss} valid loss: {valid_loss}\")\n",
    "            \"\"\"\n",
    "            print(f\"Epoch {epoch} train loss: {train_loss} match_pct: {match_pct / len(train)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
