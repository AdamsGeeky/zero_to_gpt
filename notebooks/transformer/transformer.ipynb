{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# Based on this paper - https://arxiv.org/pdf/1706.03762.pdf\n",
    "# Might want to move layer norm inside the residual block - https://arxiv.org/pdf/2002.04745.pdf\n",
    "# Layer normalization - https://arxiv.org/pdf/1607.06450.pdf\n",
    "#!pip install torch torchtext sentencepiece datasets wandb"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vik/.virtualenvs/nnets/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "sys.path.append(os.path.abspath(\"../../data\"))\n",
    "sys.path.append(os.path.abspath(\"../../nnets\"))\n",
    "from net_utils import get_module_list\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 32\n",
    "SP_VOCAB_SIZE = 5000\n",
    "TRAIN_SIZE = 5000"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset opus100 (/Users/vik/.cache/huggingface/datasets/opus100/en-es/0.0.0/256f3196b69901fb0c79810ef468e2c4ed84fbd563719920b1ff1fdc750f7704)\n",
      "100%|██████████| 3/3 [00:00<00:00, 242.66it/s]\n",
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=tokens.txt --model_prefix=opus100 --vocab_size=5000 --model_type=unigram\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: tokens.txt\n",
      "  input_format: \n",
      "  model_prefix: opus100\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 5000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(319) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(174) LOG(INFO) Loading corpus: tokens.txt\n",
      "trainer_interface.cc(375) LOG(INFO) Loaded all 9999 sentences\n",
      "trainer_interface.cc(390) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(390) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(390) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(395) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(456) LOG(INFO) all chars count=531332\n",
      "trainer_interface.cc(467) LOG(INFO) Done: 99.9513% characters are covered.\n",
      "trainer_interface.cc(477) LOG(INFO) Alphabet size=100\n",
      "trainer_interface.cc(478) LOG(INFO) Final character coverage=0.999513\n",
      "trainer_interface.cc(510) LOG(INFO) Done! preprocessed 9999 sentences.\n",
      "unigram_model_trainer.cc(138) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(142) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(193) LOG(INFO) Initialized 46136 seed sentencepieces\n",
      "trainer_interface.cc(516) LOG(INFO) Tokenizing input sentences with whitespace: 9999\n",
      "trainer_interface.cc(526) LOG(INFO) Done! 25155\n",
      "unigram_model_trainer.cc(488) LOG(INFO) Using 25155 sentences for EM training\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=16156 obj=12.9249 num_tokens=52196 num_tokens/piece=3.23075\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=14349 obj=10.894 num_tokens=52633 num_tokens/piece=3.66806\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=10759 obj=10.9789 num_tokens=56071 num_tokens/piece=5.21154\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=10752 obj=10.9021 num_tokens=56145 num_tokens/piece=5.22182\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=8063 obj=11.2522 num_tokens=61513 num_tokens/piece=7.62905\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=8063 obj=11.1555 num_tokens=61558 num_tokens/piece=7.63463\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=6047 obj=11.5819 num_tokens=67657 num_tokens/piece=11.1885\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=6047 obj=11.4839 num_tokens=67654 num_tokens/piece=11.188\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=5500 obj=11.618 num_tokens=69591 num_tokens/piece=12.6529\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=5500 obj=11.5897 num_tokens=69593 num_tokens/piece=12.6533\n",
      "trainer_interface.cc(604) LOG(INFO) Saving model: opus100.model\n",
      "trainer_interface.cc(615) LOG(INFO) Saving vocabs: opus100.vocab\n"
     ]
    }
   ],
   "source": [
    "from text_data import Opus100DatasetWrapper\n",
    "\n",
    "class Wrapper(Opus100DatasetWrapper):\n",
    "    split_lengths = [TRAIN_SIZE, math.floor(TRAIN_SIZE * .1), 100]\n",
    "    x_length = 40\n",
    "    target_length = 40\n",
    "\n",
    "wrapper = Wrapper(SP_VOCAB_SIZE)\n",
    "datasets = wrapper.generate_datasets(BATCH_SIZE)\n",
    "train = datasets[\"train\"]\n",
    "valid = datasets[\"validation\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, input_units, attention_heads, mask=False):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.input_units = input_units\n",
    "        self.attention_heads = attention_heads\n",
    "        self.head_units = int(input_units/attention_heads)\n",
    "        self.mask = mask\n",
    "\n",
    "        k = math.sqrt(1/self.input_units)\n",
    "        self.in_proj_weight = nn.Parameter(torch.rand(3, input_units, self.attention_heads * self.head_units) * 2 * k - k)\n",
    "        self.in_proj_bias = nn.Parameter(torch.rand(3, input_units) * 2 * k - k)\n",
    "\n",
    "        self.out_proj_weight = nn.Parameter(torch.rand(self.attention_heads * self.head_units, input_units) * 2 * k - k)\n",
    "        self.out_proj_bias = nn.Parameter(torch.rand(input_units) * 2 * k - k)\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        # convert to 4d tensor with batch_size, attn_heads, seq_len, embedding_dim\n",
    "        proj_queries = torch.einsum(\"...se, eo->...so\", queries, self.in_proj_weight[0]) + self.in_proj_bias[0]\n",
    "        proj_queries = proj_queries.view(queries.shape[0], queries.shape[1], self.attention_heads, self.head_units).swapaxes(1,2)\n",
    "\n",
    "        proj_keys = torch.einsum(\"...se, eo->...so\", keys, self.in_proj_weight[1]) + self.in_proj_bias[1]\n",
    "        proj_keys = proj_keys.view(keys.shape[0], keys.shape[1], self.attention_heads, self.head_units).swapaxes(1,2)\n",
    "\n",
    "        proj_values = torch.einsum(\"...se, eo->...so\", values, self.in_proj_weight[2]) + self.in_proj_bias[2]\n",
    "        proj_values = proj_values.view(values.shape[0], values.shape[1], self.attention_heads, self.head_units).swapaxes(1,2)\n",
    "\n",
    "        attention = torch.einsum(\"baqh, bahk->baqk\", proj_queries, torch.transpose(proj_keys, -1, -2)) / np.sqrt(proj_keys.shape[-1])\n",
    "        if self.mask:\n",
    "            # Prevent decoder queries from looking at tokens that come after\n",
    "            # Do this by setting attention to negative infinity, so it is softmaxed to zero in the next step\n",
    "            mask = torch.full((attention.shape[-2], attention.shape[-1]), -torch.inf, device=DEVICE)\n",
    "            attention += torch.triu(mask, diagonal=1)\n",
    "\n",
    "        # Softmax on last dimension\n",
    "        # Sequence-wise softmax, so attention between one sequence and other sequences sums to 1\n",
    "        attention = torch.softmax(attention, dim=-1)\n",
    "        weighted_values = torch.einsum(\"baqk, bake->baqe\", attention, proj_values)\n",
    "\n",
    "        # Swap attention head and sequence axis, then reshape to batch, seq, embedding\n",
    "        weighted_values = weighted_values.swapaxes(1,2).reshape(queries.shape[0], queries.shape[1], -1)\n",
    "        weighted_values = torch.einsum(\"...se, eo->...so\", weighted_values, self.out_proj_weight) + self.out_proj_bias\n",
    "        return weighted_values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, input_units, attention_heads, hidden_units=2048, dropout_p=.1):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        self.input_units = input_units\n",
    "        self.attention_heads = attention_heads\n",
    "        self.hidden_units = hidden_units\n",
    "\n",
    "        self.mha = MultiHeadAttention(self.input_units, self.attention_heads)\n",
    "        self.dropouts = get_module_list(2, nn.Dropout, dropout_p)\n",
    "        self.linear1 = nn.Linear(self.input_units, hidden_units)\n",
    "        self.linear2 = nn.Linear(hidden_units, self.input_units)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.lns = get_module_list(2, nn.LayerNorm, self.input_units)\n",
    "\n",
    "    def forward(self, x):\n",
    "        weighted_values = self.dropouts[0](self.mha(x, x, x))\n",
    "        x = self.lns[0](x + weighted_values)\n",
    "\n",
    "        reprojected = self.dropouts[1](self.linear2(self.relu(self.linear1(x))))\n",
    "        x = self.lns[1](x + reprojected)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, input_units, attention_heads, hidden_units=2048, dropout_p=.1):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.input_units = input_units\n",
    "        self.attention_heads = attention_heads\n",
    "        self.hidden_units = hidden_units\n",
    "\n",
    "        self.in_attn = MultiHeadAttention(self.input_units, self.attention_heads, mask=True)\n",
    "        self.context_attn = MultiHeadAttention(self.input_units, self.attention_heads)\n",
    "        self.dropouts = get_module_list(3, nn.Dropout, dropout_p)\n",
    "        self.linear1 = nn.Linear(self.input_units, hidden_units)\n",
    "        self.linear2 = nn.Linear(hidden_units, self.input_units)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.lns = get_module_list(3, nn.LayerNorm, self.input_units)\n",
    "\n",
    "    def forward(self, x, context):\n",
    "        weighted_values = self.dropouts[0](self.in_attn(x, x, x))\n",
    "        x = self.lns[0](x + weighted_values)\n",
    "\n",
    "        decoder_values = self.dropouts[1](self.context_attn(x, context, context))\n",
    "        x = self.lns[1](x + decoder_values)\n",
    "\n",
    "        reprojected = self.dropouts[2](self.linear2(self.relu(self.linear1(x))))\n",
    "        x = self.lns[2](x + reprojected)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_units, hidden_units, attention_heads, padding_idx, max_len=256, blocks=1):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.input_units = input_units\n",
    "        self.hidden_units = hidden_units\n",
    "        self.attention_heads = attention_heads\n",
    "        self.blocks = blocks\n",
    "\n",
    "        self.output_embedding = nn.Linear(hidden_units, input_units)\n",
    "        self.embedding = nn.Embedding(input_units, hidden_units, padding_idx=padding_idx)\n",
    "        self.dropouts = get_module_list(2, nn.Dropout, .1)\n",
    "        self.encoders = get_module_list(self.blocks, EncoderBlock, hidden_units, attention_heads)\n",
    "        self.decoders = get_module_list(self.blocks, DecoderBlock, hidden_units, attention_heads)\n",
    "        self.pos_encoding = self.encoding(max_len, self.hidden_units).to(DEVICE)\n",
    "\n",
    "    def forward(self, x, y, enc_outputs=None):\n",
    "        if enc_outputs is None:\n",
    "            # 3D with batch, seq, embeddings\n",
    "            # TODO: Tie input and output embedding weights\n",
    "            enc_outputs = self.dropouts[0](self.embedding(x) + self.pos_encoding[:x.shape[1]])\n",
    "\n",
    "            for i in range(self.blocks):\n",
    "                enc_outputs = self.encoders[i](enc_outputs)\n",
    "\n",
    "        dec_outputs = self.dropouts[1](self.embedding(y) + self.pos_encoding[:y.shape[1]])\n",
    "        for i in range(self.blocks):\n",
    "            dec_outputs = self.decoders[i](dec_outputs, enc_outputs)\n",
    "\n",
    "        token_vectors = self.output_embedding(dec_outputs)\n",
    "        return token_vectors, enc_outputs\n",
    "\n",
    "    def encoding(self, seq_len, embed_len):\n",
    "        encodings = torch.zeros((seq_len, embed_len))\n",
    "        for i in range(seq_len):\n",
    "            all = torch.exp(torch.arange(0, embed_len, 2) * (-math.log(10000.0) / embed_len))\n",
    "            encodings[i, 0::2] = torch.sin(i * all)\n",
    "            encodings[i, 1::2] = torch.cos(i * all)\n",
    "        return encodings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "outputs": [],
   "source": [
    "def generate(sequence, pred, target, wrapper):\n",
    "    prompts = wrapper.decode_batch(sequence.cpu())\n",
    "    texts = wrapper.decode_batch(torch.argmax(pred, dim=2).cpu())\n",
    "    correct_texts = wrapper.decode_batch(target.cpu())\n",
    "\n",
    "    displays = []\n",
    "    for p, t, ct in zip(prompts, texts, correct_texts):\n",
    "        displays.append(f\"{p} | {ct} | {t}\")\n",
    "    return displays\n",
    "\n",
    "def trim_padding(batch, pad_token, other_seq=None):\n",
    "    least_padding = (batch == pad_token).sum(axis=1).min()\n",
    "    if other_seq is not None:\n",
    "        least_padding = min(least_padding, (other_seq == pad_token).sum(axis=1).min())\n",
    "    if least_padding == 0:\n",
    "        return batch\n",
    "    return batch[:,:-least_padding]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Finishing last run (ID:9fbof4qf) before initializing another..."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>█▇▆▅▄▃▃▂▂▂▁▁</td></tr><tr><td>valid_loss</td><td>▁█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>0.04524</td></tr><tr><td>valid_loss</td><td>0.33364</td></tr></table><br/></div></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run <strong style=\"color:#cdcd00\">two-blocks</strong> at: <a href=\"https://wandb.ai/vikp/transformer/runs/9fbof4qf\" target=\"_blank\">https://wandb.ai/vikp/transformer/runs/9fbof4qf</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>./wandb/run-20230123_120805-9fbof4qf/logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Successfully finished last run (ID:9fbof4qf). Initializing new run:<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.13.9"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>/Users/vik/Personal/nnets/notebooks/transformer/wandb/run-20230123_121449-1t1shd3c</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href=\"https://wandb.ai/vikp/transformer/runs/1t1shd3c\" target=\"_blank\">six-blocks</a></strong> to <a href=\"https://wandb.ai/vikp/transformer\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href=\"https://wandb.ai/vikp/transformer\" target=\"_blank\">https://wandb.ai/vikp/transformer</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href=\"https://wandb.ai/vikp/transformer/runs/1t1shd3c\" target=\"_blank\">https://wandb.ai/vikp/transformer/runs/1t1shd3c</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[]"
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import wandb\n",
    "\n",
    "wandb.init(project=\"transformer\", notes=\"six blocks post-norming\", name=\"six-blocks\")\n",
    "\n",
    "# TODO: Profile and improve perf - https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html\n",
    "model = Transformer(wrapper.vocab_size, 512, 8, blocks=6, padding_idx=wrapper.pad_token).to(DEVICE)\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=wrapper.pad_token)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "wandb.watch(model, log_freq=100)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127it [01:14,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss: 0.19292951122982296\n",
      "¡Y una mierda! | Bullshit! | \n",
      "No quiero que llegues a ser como ella, degenerada y comunista. | I don't want you to become like her,... degenerate and communist. | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [00:39,  2.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss: 0.19162961741288503\n",
      "- Gracias. | - Thank you. | \n",
      "Te dejó su clínica. | He left you his practice. | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127it [01:16,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss: 0.1865668911633529\n",
      "Hecho en Bruselas, el 27 de abril de 2006. | Done at Brussels, 27 April 2006. | \n",
      "Si se activa, el servicio se basa en datagramas (UDP) en vez de basarse en flujos (TCP). | If set, then the service is datagram (UDP) based rather than stream (TCP) based. | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127it [01:22,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 train loss: 0.18590568381500996\n",
      "Pero quizás deberías llamarle. | LOOK, MAYBE YOU SHOULD CALL HIM. | \n",
      "Sadie, vámonos. | Sadie, come on. | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127it [01:21,  1.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 train loss: 0.18562944209951115\n",
      "- Bueno. | - Bruce: | \n",
      "Lo enchufas en la oficina... Y en unos minutos genera unos rayos... | You hook it up to the socket in the office, in a few minutes it generates these waves, | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [00:44,  1.60it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[263], line 12\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m batch, (sequence, target, prev_target) \u001B[38;5;129;01min\u001B[39;00m tqdm(\u001B[38;5;28menumerate\u001B[39m(train)):\n\u001B[1;32m     11\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mzero_grad(set_to_none\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m---> 12\u001B[0m     pred, _ \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrim_padding\u001B[49m\u001B[43m(\u001B[49m\u001B[43msequence\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwrapper\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpad_token\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mDEVICE\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrim_padding\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprev_target\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwrapper\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpad_token\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mother_seq\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtarget\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mDEVICE\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     14\u001B[0m     \u001B[38;5;66;03m# If you use a batch, need to reshape pred to be batch * sequence, embedding_len to be compatible\u001B[39;00m\n\u001B[1;32m     15\u001B[0m     \u001B[38;5;66;03m# Similar reshape with target to be batch * sequence vector of class indices\u001B[39;00m\n\u001B[1;32m     16\u001B[0m     loss \u001B[38;5;241m=\u001B[39m loss_fn(pred\u001B[38;5;241m.\u001B[39mreshape(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, pred\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]), trim_padding(target, wrapper\u001B[38;5;241m.\u001B[39mpad_token, other_seq\u001B[38;5;241m=\u001B[39mprev_target)\u001B[38;5;241m.\u001B[39mreshape(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39mto(DEVICE))\n",
      "File \u001B[0;32m~/.virtualenvs/nnets/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Cell \u001B[0;32mIn[257], line 27\u001B[0m, in \u001B[0;36mTransformer.forward\u001B[0;34m(self, x, y, enc_outputs)\u001B[0m\n\u001B[1;32m     25\u001B[0m dec_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropouts[\u001B[38;5;241m1\u001B[39m](\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membedding(y) \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpos_encoding[:y\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m]])\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mblocks):\n\u001B[0;32m---> 27\u001B[0m     dec_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecoders\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdec_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43menc_outputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     29\u001B[0m token_vectors \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput_embedding(dec_outputs)\n\u001B[1;32m     30\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m token_vectors, enc_outputs\n",
      "File \u001B[0;32m~/.virtualenvs/nnets/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Cell \u001B[0;32mIn[256], line 20\u001B[0m, in \u001B[0;36mDecoderBlock.forward\u001B[0;34m(self, x, context)\u001B[0m\n\u001B[1;32m     17\u001B[0m weighted_values \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropouts[\u001B[38;5;241m0\u001B[39m](\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39min_attn(x, x, x))\n\u001B[1;32m     18\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlns[\u001B[38;5;241m0\u001B[39m](x \u001B[38;5;241m+\u001B[39m weighted_values)\n\u001B[0;32m---> 20\u001B[0m decoder_values \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropouts[\u001B[38;5;241m1\u001B[39m](\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcontext_attn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcontext\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcontext\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m     21\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlns[\u001B[38;5;241m1\u001B[39m](x \u001B[38;5;241m+\u001B[39m decoder_values)\n\u001B[1;32m     23\u001B[0m reprojected \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropouts[\u001B[38;5;241m2\u001B[39m](\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlinear2(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrelu(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlinear1(x))))\n",
      "File \u001B[0;32m~/.virtualenvs/nnets/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Cell \u001B[0;32mIn[247], line 21\u001B[0m, in \u001B[0;36mMultiHeadAttention.forward\u001B[0;34m(self, queries, keys, values)\u001B[0m\n\u001B[1;32m     18\u001B[0m proj_queries \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39meinsum(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m...se, eo->...so\u001B[39m\u001B[38;5;124m\"\u001B[39m, queries, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39min_proj_weight[\u001B[38;5;241m0\u001B[39m]) \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39min_proj_bias[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m     19\u001B[0m proj_queries \u001B[38;5;241m=\u001B[39m proj_queries\u001B[38;5;241m.\u001B[39mview(queries\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m], queries\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m], \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mattention_heads, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhead_units)\u001B[38;5;241m.\u001B[39mswapaxes(\u001B[38;5;241m1\u001B[39m,\u001B[38;5;241m2\u001B[39m)\n\u001B[0;32m---> 21\u001B[0m proj_keys \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43meinsum\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m...se, eo->...so\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkeys\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43min_proj_weight\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39min_proj_bias[\u001B[38;5;241m1\u001B[39m]\n\u001B[1;32m     22\u001B[0m proj_keys \u001B[38;5;241m=\u001B[39m proj_keys\u001B[38;5;241m.\u001B[39mview(keys\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m], keys\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m], \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mattention_heads, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhead_units)\u001B[38;5;241m.\u001B[39mswapaxes(\u001B[38;5;241m1\u001B[39m,\u001B[38;5;241m2\u001B[39m)\n\u001B[1;32m     24\u001B[0m proj_values \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39meinsum(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m...se, eo->...so\u001B[39m\u001B[38;5;124m\"\u001B[39m, values, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39min_proj_weight[\u001B[38;5;241m2\u001B[39m]) \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39min_proj_bias[\u001B[38;5;241m2\u001B[39m]\n",
      "File \u001B[0;32m~/.virtualenvs/nnets/lib/python3.10/site-packages/torch/functional.py:378\u001B[0m, in \u001B[0;36meinsum\u001B[0;34m(*args)\u001B[0m\n\u001B[1;32m    373\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m einsum(equation, \u001B[38;5;241m*\u001B[39m_operands)\n\u001B[1;32m    375\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(operands) \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m2\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m opt_einsum\u001B[38;5;241m.\u001B[39menabled:\n\u001B[1;32m    376\u001B[0m     \u001B[38;5;66;03m# the path for contracting 0 or 1 time(s) is already optimized\u001B[39;00m\n\u001B[1;32m    377\u001B[0m     \u001B[38;5;66;03m# or the user has disabled using opt_einsum\u001B[39;00m\n\u001B[0;32m--> 378\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_VF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43meinsum\u001B[49m\u001B[43m(\u001B[49m\u001B[43mequation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moperands\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# type: ignore[attr-defined]\u001B[39;00m\n\u001B[1;32m    380\u001B[0m path \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    381\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m opt_einsum\u001B[38;5;241m.\u001B[39mis_available():\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 25\n",
    "DISPLAY_BATCHES = 2\n",
    "OUT_SEQUENCE_LEN = wrapper.y_length\n",
    "PRINT_VALID = True\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Run over the training examples\n",
    "    train_loss = 0\n",
    "    match_pct = 0\n",
    "    for batch, (sequence, target, prev_target) in tqdm(enumerate(train)):\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        pred, _ = model(trim_padding(sequence, wrapper.pad_token).to(DEVICE), trim_padding(prev_target, wrapper.pad_token, other_seq=target).to(DEVICE))\n",
    "\n",
    "        # If you use a batch, need to reshape pred to be batch * sequence, embedding_len to be compatible\n",
    "        # Similar reshape with target to be batch * sequence vector of class indices\n",
    "        loss = loss_fn(pred.reshape(-1, pred.shape[-1]), trim_padding(target, wrapper.pad_token, other_seq=prev_target).reshape(-1).to(DEVICE))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        mean_loss = train_loss / len(train) / BATCH_SIZE\n",
    "        wandb.log({\"loss\": mean_loss})\n",
    "        print(f\"Epoch {epoch} train loss: {mean_loss}\")\n",
    "        sents = generate(sequence, pred, target, wrapper)\n",
    "        for sent in sents[:DISPLAY_BATCHES]:\n",
    "            print(sent)\n",
    "\n",
    "        if PRINT_VALID and epoch % 10 ==0:\n",
    "            # Compute validation loss.  Unless you have a lot of training data, the validation loss won't decrease.\n",
    "            valid_loss = 0\n",
    "            # Deactivate dropout layers\n",
    "            model.eval()\n",
    "            for batch, (sequence, target, prev_target) in tqdm(enumerate(valid)):\n",
    "                # Inference token by tokens\n",
    "                sequence = sequence.to(DEVICE)\n",
    "                outputs = prev_target[:,0].unsqueeze(1).to(DEVICE)\n",
    "                enc_outputs = None\n",
    "                # TODO: Investigate memory leak with valid generation\n",
    "                for i in range(OUT_SEQUENCE_LEN):\n",
    "                    pred, enc_outputs = model(sequence, outputs, enc_outputs=enc_outputs)\n",
    "                    last_output = torch.argmax(pred, dim=2)\n",
    "                    outputs = torch.cat((outputs, last_output[:,-1:]), dim=1)\n",
    "                loss = loss_fn(pred.reshape(-1, pred.shape[-1]), target.view(-1).to(DEVICE))\n",
    "                valid_loss += loss.item()\n",
    "            mean_loss = valid_loss / len(valid) / BATCH_SIZE\n",
    "            wandb.log({\"valid_loss\": mean_loss})\n",
    "            print(f\"Valid loss: {mean_loss}\")\n",
    "            sents = generate(sequence, pred, target, wrapper)\n",
    "            for sent in sents[:DISPLAY_BATCHES]:\n",
    "                print(sent)\n",
    "            # Reactivate dropout\n",
    "            model.train()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "Transformer                              --\n",
      "├─Linear: 1-1                            2,565,513\n",
      "├─Embedding: 1-2                         2,560,512\n",
      "├─ModuleList: 1-3                        --\n",
      "│    └─Dropout: 2-1                      --\n",
      "│    └─Dropout: 2-2                      --\n",
      "├─ModuleList: 1-4                        --\n",
      "│    └─EncoderBlock: 2-3                 --\n",
      "│    │    └─MultiHeadAttention: 3-1      1,050,624\n",
      "│    │    └─ModuleList: 3-2              --\n",
      "│    │    └─Linear: 3-3                  1,050,624\n",
      "│    │    └─Linear: 3-4                  1,049,088\n",
      "│    │    └─ReLU: 3-5                    --\n",
      "│    │    └─ModuleList: 3-6              2,048\n",
      "│    └─EncoderBlock: 2-4                 --\n",
      "│    │    └─MultiHeadAttention: 3-7      1,050,624\n",
      "│    │    └─ModuleList: 3-8              --\n",
      "│    │    └─Linear: 3-9                  1,050,624\n",
      "│    │    └─Linear: 3-10                 1,049,088\n",
      "│    │    └─ReLU: 3-11                   --\n",
      "│    │    └─ModuleList: 3-12             2,048\n",
      "├─ModuleList: 1-5                        --\n",
      "│    └─DecoderBlock: 2-5                 --\n",
      "│    │    └─MultiHeadAttention: 3-13     1,050,624\n",
      "│    │    └─MultiHeadAttention: 3-14     1,050,624\n",
      "│    │    └─ModuleList: 3-15             --\n",
      "│    │    └─Linear: 3-16                 1,050,624\n",
      "│    │    └─Linear: 3-17                 1,049,088\n",
      "│    │    └─ReLU: 3-18                   --\n",
      "│    │    └─ModuleList: 3-19             3,072\n",
      "│    └─DecoderBlock: 2-6                 --\n",
      "│    │    └─MultiHeadAttention: 3-20     1,050,624\n",
      "│    │    └─MultiHeadAttention: 3-21     1,050,624\n",
      "│    │    └─ModuleList: 3-22             --\n",
      "│    │    └─Linear: 3-23                 1,050,624\n",
      "│    │    └─Linear: 3-24                 1,049,088\n",
      "│    │    └─ReLU: 3-25                   --\n",
      "│    │    └─ModuleList: 3-26             3,072\n",
      "=================================================================\n",
      "Total params: 19,838,857\n",
      "Trainable params: 19,838,857\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "print(summary(model))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "with profile(activities=[ProfilerActivity.CPU], record_shapes=True, ) as prof:\n",
    "    model(sequence.to(DEVICE), prev_target.to(DEVICE))\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
