{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vik/.virtualenvs/nnets/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Found cached dataset cnn_dailymail (/Users/vik/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de)\n",
      "100%|██████████| 3/3 [00:00<00:00, 154.84it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load from Huggingface datasets module\n",
    "data = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
    "train = data[\"train\"][\"highlights\"]\n",
    "valid = data[\"validation\"][\"highlights\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "STOP_TOKEN = 2\n",
    "START_TOKEN = 1\n",
    "UNK_TOKEN = 0\n",
    "MAX_INPUT_CHARS = 128"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def split_words(text):\n",
    "    return list(text)\n",
    "\n",
    "def clean_text(text):\n",
    "    return re.sub(r'[^a-zA-Z0-9_\\-\\n ]', '', text)\n",
    "\n",
    "def compute_tokens(texts, min_count=100):\n",
    "    token_counts = {}\n",
    "    for text in texts:\n",
    "        words = split_words(clean_text(text))\n",
    "        for word in words:\n",
    "            if word not in token_counts:\n",
    "                token_counts[word] = 0\n",
    "            token_counts[word] += 1\n",
    "\n",
    "    token_map = {\"<unk>\": UNK_TOKEN, \"<s>\": START_TOKEN, \"</s>\": STOP_TOKEN}\n",
    "    token_counter = 3\n",
    "    for k, v in token_counts.items():\n",
    "        if v >= min_count:\n",
    "            token_map[k] = token_counter\n",
    "            token_counter += 1\n",
    "    return token_map\n",
    "\n",
    "def tokenize(text, token_map):\n",
    "    # Add end token to stop generation\n",
    "    words = split_words(clean_text(text))\n",
    "    list_tokens = [token_map.get(w, UNK_TOKEN) for w in words] + [STOP_TOKEN]\n",
    "    tokens = jnp.array(list_tokens)\n",
    "    # Remove leading zeros to avoid sequence issues\n",
    "    tokens = jnp.trim_zeros(tokens, 'f')\n",
    "    return tokens\n",
    "\n",
    "def reverse_tokenize(tokens, reverse_map):\n",
    "    words = map(lambda x: reverse_map[x], tokens)\n",
    "    return \" \".join(words)\n",
    "\n",
    "def encode(tokens, token_map, max_seq_len):\n",
    "    mat = np.zeros((max_seq_len, len(token_map)))\n",
    "    for i in range(tokens.shape[0]):\n",
    "        mat[i,tokens[i]] = 1\n",
    "    for i in range(tokens.shape[0], max_seq_len):\n",
    "        # Add padding token if sequence is too short\n",
    "        mat[i,STOP_TOKEN] = 1\n",
    "    return mat\n",
    "\n",
    "train = [t for t in train if len(t) < MAX_INPUT_CHARS]\n",
    "valid = [v for v in valid if len(v) < MAX_INPUT_CHARS]\n",
    "\n",
    "token_map = compute_tokens(train + valid, min_count=max(2, len(train) / 1000))\n",
    "reverse_token_map = {v:k for k,v in token_map.items()}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68\n",
      "7553\n"
     ]
    }
   ],
   "source": [
    "print(len(token_map))\n",
    "print(len(train))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap, tree_map, value_and_grad, debug\n",
    "from statistics import mean\n",
    "\n",
    "def init_params(layer_conf):\n",
    "    layers = []\n",
    "    for i in range(len(layer_conf)):\n",
    "        if layer_conf[i][\"type\"] == \"input\":\n",
    "            continue\n",
    "        elif layer_conf[i][\"type\"] == \"encoder\":\n",
    "            np.random.seed(0)\n",
    "            k = 1/math.sqrt(layer_conf[i][\"hidden\"])\n",
    "            input_weights = np.random.rand(layer_conf[i-1][\"units\"], layer_conf[i][\"hidden\"]) * 2 * k - k\n",
    "\n",
    "            hidden_weights = np.random.rand(layer_conf[i][\"hidden\"], layer_conf[i][\"hidden\"]) * 2 * k - k\n",
    "            hidden_bias = np.random.rand(1, layer_conf[i][\"hidden\"]) * 2 * k - k\n",
    "\n",
    "            output_weights = np.random.rand(layer_conf[i][\"hidden\"], layer_conf[i][\"output\"]) * 2 * k - k\n",
    "            output_bias = np.random.rand(1, layer_conf[i][\"output\"]) * 2 * k - k\n",
    "\n",
    "            layers.append(\n",
    "                [[input_weights], [hidden_weights, hidden_bias], [output_weights, output_bias]]\n",
    "            )\n",
    "    return layers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def encoder_fwd(params, prev_hidden, x_example):\n",
    "    [i_weight], [h_weight, h_bias], [o_weight, o_bias] = params\n",
    "    input_x = x_example @ i_weight\n",
    "    hidden_x = input_x + prev_hidden @ h_weight + h_bias\n",
    "    # Activation.  tanh avoids outputs getting larger and larger.\n",
    "    hidden_x = jnp.tanh(hidden_x)\n",
    "\n",
    "    # Output layer\n",
    "    output_x = hidden_x @ o_weight + o_bias\n",
    "    return hidden_x, output_x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def decoder_fwd(params, prev_hidden, context):\n",
    "    [c_weight], [h_weight, h_bias], [o_weight, o_bias] = params\n",
    "    input_x = context @ c_weight\n",
    "    hidden_x = input_x + prev_hidden @ h_weight + h_bias\n",
    "    # Activation.  tanh avoids outputs getting larger and larger.\n",
    "    hidden_x = jnp.tanh(hidden_x)\n",
    "\n",
    "    # Activation\n",
    "    output_x = hidden_x @ o_weight + o_bias\n",
    "    return hidden_x, output_x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def softmax(preds):\n",
    "    tol = 1e-6\n",
    "    preds = jnp.exp(preds - jnp.max(preds))\n",
    "    return preds / (jnp.sum(preds) - tol)\n",
    "\n",
    "def log_loss(encoder_params, prev_hidden, x, y):\n",
    "    hidden, output = encoder_fwd(encoder_params, prev_hidden, x)\n",
    "    output = softmax(output)\n",
    "    tol = 1e-6\n",
    "    cross_entropy = jnp.multiply(y, jnp.log(output + tol))\n",
    "    loss = -jnp.sum(cross_entropy)\n",
    "    return loss, {\"hidden\": hidden, \"output\": output}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def add_pytrees(pytree1, pytree2, x_ended):\n",
    "  return tree_map(lambda pt1, pt2: pt1 + (pt2 * x_ended), pytree1, pytree2)\n",
    "\n",
    "@jit\n",
    "def update(params, x, y, lr):\n",
    "    encoder_params = params[0]\n",
    "    # Initialize with hidden shape\n",
    "    prev_hidden = jnp.zeros((1, 512))\n",
    "    losses = []\n",
    "    total_grad = None\n",
    "    for j in range(x.shape[0]):\n",
    "        [loss, state], grad = value_and_grad(log_loss, has_aux=True)(encoder_params, prev_hidden, x[j,:], y[j,:])\n",
    "        prev_hidden = state[\"hidden\"]\n",
    "        losses.append(loss)\n",
    "\n",
    "        # If the sequence is over, we won't contribute to the gradient.  This happens when we detect a stop token.\n",
    "        x_ended = (np.argmax(x[j,:]) != STOP_TOKEN).astype(int)\n",
    "        if total_grad is None:\n",
    "            total_grad = grad\n",
    "        else:\n",
    "            total_grad = add_pytrees(total_grad, grad, x_ended)\n",
    "\n",
    "    params[0] = tree_map(\n",
    "        lambda param, grad: param - lr * grad, encoder_params, total_grad\n",
    "    )\n",
    "    return params, losses\n",
    "\n",
    "def batch_update(params, x, y, lr):\n",
    "    return vmap(update, in_axes=[None, 0, 0, None])(params, x, y, lr)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "tokenized = [tokenize(train[z], token_map) for z in range(len(train))]\n",
    "max_seq_len = max([len(t) for t in tokenized])\n",
    "\n",
    "train_x = np.stack([encode(tokenized[z], token_map, max_seq_len) for z in range(len(train))])\n",
    "train_y = np.zeros(train_x.shape)\n",
    "train_y[:,:-1,:] = train_x[:,1:,:]\n",
    "train_y[:,-1,STOP_TOKEN] = 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Epoch 0 loss: 3.4200481286277244\n",
      "Epoch 1\n",
      "Epoch 1 loss: 3.120500461174747\n",
      "Epoch 2\n",
      "Epoch 2 loss: 2.9578498426130215\n",
      "Epoch 3\n",
      "Epoch 3 loss: 2.8791061945997494\n",
      "Epoch 4\n",
      "Epoch 4 loss: 2.803405121890928\n",
      "Epoch 5\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "epochs = 20\n",
    "lr = 1e-3\n",
    "batch_size = 1\n",
    "\n",
    "\n",
    "layer_conf = [\n",
    "    {\"type\":\"input\", \"units\": len(token_map)},\n",
    "    {\"type\": \"encoder\", \"hidden\": 512, \"output\": len(token_map)}\n",
    "]\n",
    "params = init_params(layer_conf)\n",
    "\n",
    "for i in range(epochs):\n",
    "    sequence_len = 7\n",
    "    print(f\"Epoch {i}\")\n",
    "    epoch_loss = []\n",
    "    for j in tqdm(range(0, len(train), batch_size)):\n",
    "        batch_inds = list(range(j, j+batch_size))\n",
    "        seq_x = train_x[j,:,:]\n",
    "        seq_y = train_y[j,:,:]\n",
    "        #params, loss = batch_update(params, seq_x, seq_y, lr)\n",
    "        params, loss = update(params, seq_x, seq_y, lr)\n",
    "        epoch_loss.append(mean([float(jnp.mean(l)) for l in loss]))\n",
    "\n",
    "    print(f\"Epoch {i} loss: {mean(epoch_loss)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gen_length = 60\n",
    "start_text = train[1]\n",
    "tokenized = tokenize(start_text, token_map)\n",
    "encoded = encode(tokenized, token_map, max_seq_len)\n",
    "\n",
    "prev_hidden = jnp.zeros((1, 512))\n",
    "out_seq = []\n",
    "out_text = \"\"\n",
    "for j in range(encoded.shape[0]):\n",
    "    hidden, output = encoder_fwd(params[0], prev_hidden, encoded[j,:])\n",
    "    prev_hidden = hidden\n",
    "    output = softmax(output)\n",
    "    out_ind = int(np.argmax(output))\n",
    "    out_seq.append(out_ind)\n",
    "    out_text += reverse_token_map[out_ind] + \" \""
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
