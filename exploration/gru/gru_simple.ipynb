{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "#!pip install torch torchtext sentencepiece datasets\n",
    "# Try opus books dataset for translation - https://huggingface.co/datasets/opus_books"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vik/.virtualenvs/nnets/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import functorch\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "sys.path.append(os.path.abspath(\"../../data\"))\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 32\n",
    "SP_VOCAB_SIZE = 1000\n",
    "TRAIN_SIZE = 500"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset cnn_dailymail (/Users/vik/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de)\n",
      "100%|██████████| 3/3 [00:00<00:00, 152.11it/s]\n",
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=tokens.txt --model_prefix=cnn_dailymail --vocab_size=1000 --model_type=unigram\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: tokens.txt\n",
      "  input_format: \n",
      "  model_prefix: cnn_dailymail\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 1000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(319) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(174) LOG(INFO) Loading corpus: tokens.txt\n",
      "trainer_interface.cc(375) LOG(INFO) Loaded all 2161 sentences\n",
      "trainer_interface.cc(390) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(390) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(390) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(395) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(456) LOG(INFO) all chars count=101295\n",
      "trainer_interface.cc(467) LOG(INFO) Done: 99.9526% characters are covered.\n",
      "trainer_interface.cc(477) LOG(INFO) Alphabet size=68\n",
      "trainer_interface.cc(478) LOG(INFO) Final character coverage=0.999526\n",
      "trainer_interface.cc(510) LOG(INFO) Done! preprocessed 2161 sentences.\n",
      "unigram_model_trainer.cc(138) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(142) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(193) LOG(INFO) Initialized 11093 seed sentencepieces\n",
      "trainer_interface.cc(516) LOG(INFO) Tokenizing input sentences with whitespace: 2161\n",
      "trainer_interface.cc(526) LOG(INFO) Done! 6025\n",
      "unigram_model_trainer.cc(488) LOG(INFO) Using 6025 sentences for EM training\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=4758 obj=13.0431 num_tokens=13019 num_tokens/piece=2.73623\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=4170 obj=11.4914 num_tokens=13140 num_tokens/piece=3.15108\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=3123 obj=11.5758 num_tokens=14016 num_tokens/piece=4.48799\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=3120 obj=11.4633 num_tokens=14040 num_tokens/piece=4.5\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=2340 obj=11.859 num_tokens=15424 num_tokens/piece=6.59145\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=2340 obj=11.721 num_tokens=15431 num_tokens/piece=6.59444\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=1755 obj=12.3254 num_tokens=17151 num_tokens/piece=9.77265\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=1755 obj=12.1667 num_tokens=17155 num_tokens/piece=9.77493\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=1316 obj=12.8195 num_tokens=19020 num_tokens/piece=14.4529\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=1316 obj=12.6626 num_tokens=19020 num_tokens/piece=14.4529\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=1100 obj=13.101 num_tokens=20116 num_tokens/piece=18.2873\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=1100 obj=13.0108 num_tokens=20117 num_tokens/piece=18.2882\n",
      "trainer_interface.cc(604) LOG(INFO) Saving model: cnn_dailymail.model\n",
      "trainer_interface.cc(615) LOG(INFO) Saving vocabs: cnn_dailymail.vocab\n"
     ]
    }
   ],
   "source": [
    "from text_data import CNNDatasetWrapper\n",
    "\n",
    "class Wrapper(CNNDatasetWrapper):\n",
    "    split_lengths = [TRAIN_SIZE, math.floor(TRAIN_SIZE * .1), 100]\n",
    "    x_length = 15\n",
    "    target_length = 15\n",
    "\n",
    "wrapper = Wrapper(SP_VOCAB_SIZE, DEVICE)\n",
    "\n",
    "datasets = wrapper.generate_datasets(BATCH_SIZE)\n",
    "train = datasets[\"train\"]\n",
    "valid = datasets[\"validation\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class GRUCell(nn.Module):\n",
    "    def __init__(self, input_units, hidden_units, output_units):\n",
    "        super(GRUCell, self).__init__()\n",
    "        self.input_units = input_units\n",
    "        self.hidden_units = hidden_units\n",
    "        self.output_units = output_units\n",
    "\n",
    "        k = math.sqrt(1/hidden_units)\n",
    "        self.input_weights = nn.Parameter(torch.rand(3, input_units, hidden_units) * 2 * k - k)\n",
    "        self.input_biases = nn.Parameter(torch.rand(3, 1, hidden_units) * 2 * k - k)\n",
    "\n",
    "        self.hidden_weights = nn.Parameter(torch.rand(3, hidden_units, hidden_units) * 2 * k - k)\n",
    "        self.hidden_biases = nn.Parameter(torch.rand(3, 1, hidden_units) * 2 * k - k)\n",
    "\n",
    "    def forward(self, x, prev_hidden):\n",
    "        # Compute the regular RNN forward pass\n",
    "        # Compute update and reset gates for GRU\n",
    "        reset_gate = torch.sigmoid(x @ self.input_weights[0,] + self.input_biases[0,] + prev_hidden @ self.hidden_weights[0,] + self.hidden_biases[0,])\n",
    "        update_gate = torch.sigmoid(x @ self.input_weights[1,] + self.input_biases[1,] + prev_hidden @ self.hidden_weights[1,] + self.hidden_biases[1,])\n",
    "        new_gate = torch.tanh(x @ self.input_weights[2,] + self.input_biases[2,] + torch.mul(reset_gate, prev_hidden @ self.hidden_weights[2,] + self.hidden_biases[2,]))\n",
    "\n",
    "        hidden_x = torch.mul((1 - update_gate), new_gate) + torch.mul(update_gate, new_gate)\n",
    "        return hidden_x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, in_sequence_len, out_sequence_len, embedding_len, hidden_units=512, layers=2):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.in_sequence_len = in_sequence_len\n",
    "        self.out_sequence_len = out_sequence_len\n",
    "        self.hidden_units = hidden_units\n",
    "        self.embedding_len = embedding_len\n",
    "        self.layers = layers\n",
    "\n",
    "        self.embedding = nn.Embedding(embedding_len, hidden_units)\n",
    "        self.encoders = nn.ModuleList([GRUCell(input_units=hidden_units, hidden_units=hidden_units, output_units=hidden_units) for _ in range(layers)])\n",
    "        self.decoders = nn.ModuleList([GRUCell(input_units=hidden_units * 2, hidden_units=hidden_units, output_units=hidden_units) for _ in range(layers)])\n",
    "\n",
    "        self.linear = nn.Linear(in_features=hidden_units, out_features=embedding_len)\n",
    "\n",
    "        k = math.sqrt(1/hidden_units)\n",
    "        self.hidden_attention_weight = nn.Parameter(torch.rand(hidden_units, hidden_units) * 2 * k - k)\n",
    "        self.context_attention_weight = nn.Parameter(torch.rand(hidden_units, hidden_units) * 2 * k - k)\n",
    "        self.attention_weight = nn.Parameter(torch.rand(1, hidden_units) * 2 * k - k)\n",
    "        self.batched_diag = functorch.vmap(torch.diag)\n",
    "\n",
    "    def attention(self, context, prev_hidden, batch_size):\n",
    "        # Swap axes so the first dimension of context_attn is batch\n",
    "        context_attn = torch.bmm(context.swapaxes(0,1), self.context_attention_weight.unsqueeze(0).expand(batch_size,-1,-1))\n",
    "        # Swap back since prev_hidden is by batch.  This makes the first dim of cross sequence\n",
    "        cross = torch.tanh(context_attn.swapaxes(0,1) + prev_hidden @ self.hidden_attention_weight)\n",
    "        # This will be of dimension batch, sequence_length, 1\n",
    "        attention = torch.bmm(cross.swapaxes(0,1), self.attention_weight.T.unsqueeze(0).expand(batch_size, -1, -1))\n",
    "        # Drop the last singleton dimension\n",
    "        attention = attention.squeeze(2)\n",
    "        # Softmax the predictions across each batch\n",
    "        probs = torch.softmax(attention, 1)\n",
    "        diagonalized_probs = self.batched_diag(probs)\n",
    "        positional_contexts = torch.sum(torch.bmm(diagonalized_probs, context.swapaxes(0,1)), dim=1).reshape(batch_size, self.hidden_units)\n",
    "        return positional_contexts\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        batch_size = x.shape[0]\n",
    "        # Move batch to the second dimension, so sequence comes first\n",
    "        y = y.swapaxes(0,1)\n",
    "        # Embed the input sequence to reduce dimensionality\n",
    "        embedded = self.embedding(x).swapaxes(0,1)\n",
    "\n",
    "        # Encode the input sequence\n",
    "        # Both tensors will have sequence then batch\n",
    "        enc_hiddens = torch.zeros((1, self.layers, batch_size, self.hidden_units), device=DEVICE)\n",
    "        for j in range(self.in_sequence_len):\n",
    "            seq_enc_hiddens = embedded[j,:].unsqueeze(0)\n",
    "            for i in range(self.layers):\n",
    "                hidden = self.encoders[i](seq_enc_hiddens[i,], enc_hiddens[j,i])\n",
    "                # Add first sequence axis\n",
    "                hidden = hidden.unsqueeze(0)\n",
    "                seq_enc_hiddens = torch.cat((seq_enc_hiddens, hidden), dim=0)\n",
    "\n",
    "            enc_hiddens = torch.cat((enc_hiddens, seq_enc_hiddens[1:].unsqueeze(0)), dim=0)\n",
    "\n",
    "        # Decode to the output sequence\n",
    "        # Pass in context\n",
    "        context = enc_hiddens[1:,-1,:,:]\n",
    "        # Both tensors will have the first dimension be the sequence\n",
    "        dec_hiddens = torch.zeros(1, self.layers, batch_size, self.hidden_units, device=DEVICE)\n",
    "        outputs = torch.zeros(1, batch_size, self.embedding_len, device=DEVICE)\n",
    "        for j in range(self.out_sequence_len):\n",
    "            # Use either the actual previous y (from the input), or the generated y if the input sequence is shorter than the generation steps.\n",
    "            if y.shape[0] > j:\n",
    "                prev_y = y[j,:]\n",
    "            else:\n",
    "                prev_y = outputs[j,:,:]\n",
    "                prev_y = prev_y.argmax(dim=1).int()\n",
    "\n",
    "            # Run embedding over previous y state\n",
    "            prev_y = self.embedding(prev_y)\n",
    "            seq_dec_hiddens = prev_y.unsqueeze(0)\n",
    "            for i in range(self.layers):\n",
    "                positional_context = self.attention(context, dec_hiddens[j,i,], batch_size)\n",
    "                hidden = self.decoders[i](torch.cat((seq_dec_hiddens[i,], positional_context), dim=1), dec_hiddens[j,i,],)\n",
    "                # Add first sequence axis\n",
    "                hidden = hidden.unsqueeze(0)\n",
    "                seq_dec_hiddens = torch.cat((seq_dec_hiddens, hidden), dim=0)\n",
    "\n",
    "            # Swap sequence and batch axes to apply linear transform, then swap back\n",
    "            prev_output = self.linear(seq_dec_hiddens[-1].unsqueeze(0).swapaxes(0,1)).swapaxes(0,1)\n",
    "            outputs = torch.cat((outputs, prev_output), dim=0)\n",
    "            dec_hiddens = torch.cat((dec_hiddens, seq_dec_hiddens[1:].unsqueeze(0)), dim=0)\n",
    "\n",
    "        # Move batch back to axis 0\n",
    "        out_hiddens = dec_hiddens[1:,-1,:,:].swapaxes(0,1)\n",
    "        out_output = outputs[1:,].swapaxes(0,1)\n",
    "        return out_output, out_hiddens\n",
    "\n",
    "def generate(sequence, prev_target, target, wrapper):\n",
    "    pred, _ = model(sequence, prev_target[:,0].unsqueeze(1))\n",
    "    prompts = wrapper.decode_batch(sequence.cpu())\n",
    "    texts = wrapper.decode_batch(torch.argmax(pred, dim=2).cpu())\n",
    "    correct_texts = wrapper.decode_batch(target.cpu())\n",
    "\n",
    "    displays = []\n",
    "    for p, t, ct in zip(prompts, texts, correct_texts):\n",
    "        displays.append(f\"{p} | {ct} | {t}\")\n",
    "    return displays"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "model = EncoderDecoder(wrapper.x_length, wrapper.y_length, hidden_units=512, layers=1, embedding_len=wrapper.vocab_size).to(DEVICE)\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=wrapper.pad_token)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14it [00:05,  2.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrests made after ATF agents set | up a sting operation . Affidavit: AT | \n",
      "NEW: New video proves Bhutto shot, widow | er says . Doctors claim Pakistani police prevented | \n",
      "This month Art of Life looks at motor | bike, planes, DJs and Rock id | \n",
      "NEW: Bridge reopens Friday morn | ing after highway engineers give OK . Four killed | \n",
      "Moore criticized a report Gupta did on | CNN Monday on \"Sicko\" Gupta's report question | i upmergencys underlso protest Argentinlesgain helpScause policyra women\n",
      "NEW: British counter-terrorism expert | s re-inspect Bhutto's vehicle . NEW: Bhutto | \n",
      "NEW: 17-year-old alleged shooter appears in | Miami courtroom . NEW: Eric Rive | \n",
      "Rinko Kikuchi was Oscar- | nominated for her performance in the | \n",
      "Epoch 0 train loss: 0.21559224277734756 match_pct: 0.005859375 valid loss: 0.21120067685842514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14it [00:05,  2.72it/s]\n",
      "14it [00:05,  2.69it/s]\n",
      "14it [00:05,  2.56it/s]\n",
      "14it [00:05,  2.70it/s]\n",
      "14it [00:05,  2.76it/s]\n",
      "14it [00:05,  2.72it/s]\n",
      "14it [00:05,  2.67it/s]\n",
      "14it [00:05,  2.69it/s]\n",
      "14it [00:05,  2.64it/s]\n",
      "14it [00:05,  2.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fire chief: \"I think it's a miracle | that we haven't seen some serious injuries\" Officials | ss\n",
      "Stolen art can be lost for decades . S | oft targets like museums  | ssssssssssssssss\n",
      "At least 10 people have died in torrential rain | s in Ecuador, officials say . Authorities say the | ss\n",
      "Savers at leading UK mortgage bank | lined up to empty their accounts . N | ss\n",
      "\"Inspiring Impressionism\" | looks at Old Masters, other influences on | ss\n",
      "NEW: Woman says husband didn't show up | at a party on suspected date of killing . N | ss\n",
      "Sheriff: Possible tornado caused | heavy damage in Prosperity, South Carolin | ss\n",
      "President Bush says Tony Snow \"will battle c | ancer and win\" Job of press secre | sss\n",
      "Epoch 10 train loss: 0.17134868140731538 match_pct: 0.0736607164144516 valid loss: 0.17988649755716324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14it [00:05,  2.66it/s]\n",
      "14it [00:05,  2.68it/s]\n",
      "14it [00:05,  2.69it/s]\n",
      "14it [00:05,  2.64it/s]\n",
      "14it [00:05,  2.65it/s]\n",
      "14it [00:05,  2.67it/s]\n",
      "14it [00:05,  2.60it/s]\n",
      "14it [00:05,  2.45it/s]\n",
      "14it [00:05,  2.54it/s]\n",
      "14it [00:05,  2.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEW: Tennessee man describes diving to the flo | or as his house blows away . The tornad | ssssss\n",
      "Atlanta surpasses LA, Philadel | phia as city with most bank heists . FBI says it | sassss\n",
      "Erik Prince: \"There was definite | ly incoming small arms fire from insurg | sssssss\n",
      "Red Cross says it became aware of the re | lationship 10 days ago . Relationship allegedly | ssssssssssssssss\n",
      "Woman sentenced to 200 lashes and six months | in jail under Islamic law . Judge more than doubled 19- | sa . N\n",
      "BUPA was founded in 1947 in response | to plans to establish the NHS . | ssssss\n",
      "Mom thinks girl was abused while in the care of a | baby sitter, attorney says . Mother had no | sa . N\n",
      "NEW: 17-year-old alleged shooter appears in | Miami courtroom . NEW: Eric Rive | sa . N\n",
      "Epoch 20 train loss: 0.16391602903604507 match_pct: 0.0890066996216774 valid loss: 0.17916938662528992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14it [00:05,  2.61it/s]\n",
      "14it [00:05,  2.58it/s]\n",
      "14it [00:05,  2.66it/s]\n",
      "14it [00:05,  2.63it/s]\n",
      "14it [00:05,  2.65it/s]\n",
      "14it [00:05,  2.51it/s]\n",
      "14it [00:05,  2.58it/s]\n",
      "14it [00:05,  2.66it/s]\n",
      "14it [00:05,  2.62it/s]\n",
      "14it [00:05,  2.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "National Hurricane Center director B | ill Proenza has left his position . Nearly | saged to eeee . The\n",
      "The car was driving on a runway at 3:4 | 5 a.m. when the driver hit the brak | the red to r\n",
      "Lance corporal due to give birth at | any time, sheriff says . Marine's car found Monday at | ssssssss . The\n",
      "Iraqi forces detain the suspected leader of a terrorist ce | ll network . Cell is believed to be fund | to red to rs . NEW:\n",
      "Woman, boyfriend arrested after a tip led | to search . Police believe child found dead in box is Riley | sing to ru . He\n",
      "Remittances to Mexico fell $1 | 00 million in January, according to Bank of | s,,,,,,,\n",
      "Phrase in Obama speech similar | to that of Massachusetts Gov. | ssssssssss\n",
      "NEW: Police say they have more than one confes | sion in the case . NEW: Investigation | ssssssss .\n",
      "Epoch 30 train loss: 0.15292869401829584 match_pct: 0.1294642835855484 valid loss: 0.1812349185347557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14it [00:05,  2.63it/s]\n",
      "14it [00:05,  2.60it/s]\n",
      "14it [00:05,  2.62it/s]\n",
      "14it [00:05,  2.66it/s]\n",
      "14it [00:05,  2.65it/s]\n",
      "14it [00:05,  2.67it/s]\n",
      "14it [00:05,  2.61it/s]\n",
      "14it [00:05,  2.65it/s]\n",
      "14it [00:05,  2.61it/s]\n",
      "14it [00:05,  2.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "University of Memphis athlete Taylor | Bradford, 21, was shot Sept | ,,,,,,,,,,,,\n",
      "Experts say Lewis Hamilton is set to earn more than | Michael Schumacher . David Beck | to  to  to  to  . . . . . . . .\n",
      "NEW: NFL chief, Atlanta Falcons | owner critical of Michael Vick's conduct . N | s,,,,,,,,,,,,, . .\n",
      "President Bush will have a routine colonos | copy Saturday . While he's anesthetized | s a rs a rs a r\n",
      "President Bush to address the Veterans of | Foreign Wars on Wednesday . Bush to say that withdraw | ssssssssss . . .\n",
      "\"Desperate Housewives\" actress E | va Longoria Parker gives pee | seareeeeeeeeee\n",
      "U.N. agency appeals for medical reset | tlement of Palestinians in Iraq camps . About | sssssssssssss .\n",
      "NEW: 17-year-old alleged shooter appears in | Miami courtroom . NEW: Eric Rive | toingrrrrrrr\n",
      "Epoch 40 train loss: 0.13671733226094926 match_pct: 0.189453125 valid loss: 0.182972714304924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14it [00:05,  2.53it/s]\n",
      "9it [00:03,  2.72it/s]"
     ]
    }
   ],
   "source": [
    "EPOCHS = 1000\n",
    "DISPLAY_BATCHES = 8\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Run over the training examples\n",
    "    train_loss = 0\n",
    "    match_pct = 0\n",
    "    for batch, (sequence, target, prev_target) in tqdm(enumerate(train)):\n",
    "        optimizer.zero_grad()\n",
    "        forced_target = prev_target\n",
    "        # Alternate use of teacher forcing vs feeding back own inputs\n",
    "        if np.random.randint(2) == 0:\n",
    "            forced_target = prev_target[:,0].unsqueeze(1)\n",
    "        pred, hidden = model(sequence, forced_target)\n",
    "\n",
    "        # Need to reshape pred to be batch * sequence, embedding_len to be compatible\n",
    "        # Similar reshape with target to be batch * sequence vector of class indices\n",
    "        loss = loss_fn(pred.reshape(-1, pred.shape[-1]), target.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() / BATCH_SIZE\n",
    "        match_pct += torch.sum(target == torch.argmax(pred, 2)) / (wrapper.y_length * BATCH_SIZE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if epoch % 10 == 0:\n",
    "            # Show text generated from training prompt as an example\n",
    "            # Don't feed in all of the train y sequences, just the first token\n",
    "            # The other y tokens will be predicted by the model and fed back in\n",
    "            sents = generate(sequence[:DISPLAY_BATCHES], prev_target[:DISPLAY_BATCHES], target[:DISPLAY_BATCHES], wrapper)\n",
    "            for sent in sents:\n",
    "                print(sent)\n",
    "\n",
    "            # Compute validation loss.  Unless you have a lot of training data, the validation loss won't decrease.\n",
    "            valid_loss = 0\n",
    "            for batch, (sequence, target, prev_target) in enumerate(valid):\n",
    "                # Only feed in the first token of the actual target\n",
    "                pred, hidden = model(sequence, prev_target[:,0].unsqueeze(1))\n",
    "                loss = loss_fn(pred.reshape(-1, pred.shape[-1]), target.view(-1))\n",
    "                valid_loss += loss.item() / BATCH_SIZE\n",
    "\n",
    "            print(f\"Epoch {epoch} train loss: {train_loss / len(train)} match_pct: {match_pct / len(train)} valid loss: {valid_loss / len(valid)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pred.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "target.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
