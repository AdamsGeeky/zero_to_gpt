{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data = pd.read_csv(\"../../data/clean_weather.csv\")\n",
    "data = data.ffill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "PREDICTORS = [\"tmax\", \"tmin\", \"rain\"]\n",
    "TARGET = \"tmax_tomorrow\"\n",
    "\n",
    "scaler = StandardScaler()\n",
    "data[PREDICTORS] = scaler.fit_transform(data[PREDICTORS])\n",
    "\n",
    "split_data = np.split(data, [int(.7*len(data)), int(.85*len(data))])\n",
    "(train_x, train_y), (valid_x, valid_y), (test_x, test_y) = [[d[PREDICTORS].to_numpy(), d[[TARGET]].to_numpy()] for d in split_data]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "# Rnn\n",
    "# Input -> hidden\n",
    "# hidden -> hidden\n",
    "# hidden -> output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "def init_params(layer_conf):\n",
    "    layers = []\n",
    "    for i in range(len(layer_conf)):\n",
    "        if layer_conf[i][\"type\"] == \"input\":\n",
    "            continue\n",
    "        elif layer_conf[i][\"type\"] == \"rnn\":\n",
    "            np.random.seed(0)\n",
    "            input_weights = np.random.rand(layer_conf[i-1][\"units\"], layer_conf[i][\"hidden\"])\n",
    "\n",
    "            hidden_weights = np.random.rand(layer_conf[i][\"hidden\"], layer_conf[i][\"hidden\"])\n",
    "            hidden_bias = np.random.rand(1, layer_conf[i][\"hidden\"])\n",
    "\n",
    "            output_weights = np.random.rand(layer_conf[i][\"hidden\"], layer_conf[i][\"output\"])\n",
    "            output_bias = np.random.rand(1, layer_conf[i][\"output\"])\n",
    "\n",
    "            layers.append(\n",
    "                [[input_weights], [hidden_weights, hidden_bias], [output_weights, output_bias]]\n",
    "            )\n",
    "    return layers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "def forward(params, x, layer_conf):\n",
    "    hiddens = []\n",
    "    outputs = []\n",
    "    for i in range(len(params)):\n",
    "        if layer_conf[i+1][\"type\"] == \"rnn\":\n",
    "            [i_weight], [h_weight, h_bias], [o_weight, o_bias] = params[i]\n",
    "            hidden = np.zeros((x.shape[0], i_weight.shape[1]))\n",
    "            output = np.zeros((x.shape[0], o_weight.shape[1]))\n",
    "            for j in range(x.shape[0]):\n",
    "                input_x = x[j,:] @ i_weight\n",
    "                hidden_x = input_x + hidden[max(j-1,0),:] @ h_weight + h_bias\n",
    "                # Activation.  tanh avoids outputs getting larger and larger.\n",
    "                hidden_x = np.tanh(hidden_x)\n",
    "                # Store hidden for use in backprop\n",
    "                hidden[j,:] = hidden_x.copy()\n",
    "\n",
    "                # Activation\n",
    "                output_x = hidden_x @ o_weight + o_bias\n",
    "                output[j,:] = output_x.copy()\n",
    "            hiddens.append(hidden)\n",
    "            outputs.append(output)\n",
    "    return hiddens, outputs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "def mse(actual, predicted):\n",
    "    return np.mean((actual-predicted)**2)\n",
    "\n",
    "def mse_grad(actual, predicted):\n",
    "    return (predicted - actual)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [],
   "source": [
    "def backward(params, x, lr, grad, hiddens, layer_conf):\n",
    "    for i in range(len(params)):\n",
    "        if layer_conf[i+1][\"type\"] == \"rnn\":\n",
    "            [i_weight], [h_weight, h_bias], [o_weight, o_bias] = params[i]\n",
    "            hidden = hiddens[i]\n",
    "            next_h_grad = None\n",
    "            i_weight_grad, h_weight_grad, h_bias_grad, o_weight_grad, o_bias_grad = [0] * 5\n",
    "\n",
    "            for j in range(x.shape[0] - 1, -1, -1):\n",
    "                out = grad[j,:][:,np.newaxis]\n",
    "                ho_grad = o_weight @ out\n",
    "                if x.shape[0] - 1 > j:\n",
    "                    tanh_deriv_next = 1 - hidden[j+1] ** 2\n",
    "                    hh_grad = h_weight.T @ np.diag(tanh_deriv_next) @ next_h_grad\n",
    "                    h_grad = ho_grad + hh_grad\n",
    "                else:\n",
    "                    h_grad = ho_grad\n",
    "\n",
    "                next_h_grad = h_grad.copy()\n",
    "                tanh_deriv = 1 - hidden[j] ** 2\n",
    "\n",
    "                if j > 0:\n",
    "                    h_bias_grad += (np.diag(tanh_deriv) @ h_grad).T\n",
    "                    h_weight_grad += (np.diag(tanh_deriv) @ h_grad @ hidden[j-1][:,np.newaxis].T).T\n",
    "\n",
    "                o_bias_grad += out\n",
    "                o_weight_grad += (out @ hidden[j][:,np.newaxis].T).T\n",
    "\n",
    "                i_weight_grad += (np.diag(tanh_deriv) @ h_grad @ x[j,:][:,np.newaxis].T).T\n",
    "\n",
    "            i_weight -= i_weight_grad * lr\n",
    "            h_weight -= h_weight_grad * lr\n",
    "            h_bias -= h_bias_grad * lr\n",
    "            o_weight -= o_weight_grad * lr\n",
    "            o_bias -= o_bias_grad * lr\n",
    "            params[i] = [[i_weight], [h_weight, h_bias], [o_weight, o_bias]]\n",
    "    return params"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "outputs": [],
   "source": [
    "def backward2(params, x, lr, grad, hiddens, layer_conf):\n",
    "    for i in range(len(params)):\n",
    "        if layer_conf[i+1][\"type\"] == \"rnn\":\n",
    "            [i_weight], [h_weight, h_bias], [o_weight, o_bias] = params[i]\n",
    "            hidden = hiddens[i]\n",
    "            next_h_grad = None\n",
    "            i_weight_grad, h_weight_grad, h_bias_grad, o_weight_grad, o_bias_grad = [0] * 5\n",
    "\n",
    "            for j in range(x.shape[0] - 1, -1, -1):\n",
    "                # 1,1\n",
    "                out_grad = grad[j,:][:,np.newaxis]\n",
    "\n",
    "                # Output updates\n",
    "                # (1,1 @ 1,3).T = 3,1\n",
    "                o_weight_grad += (out_grad @ hidden[j,:][np.newaxis, :]).T\n",
    "                # 1,1\n",
    "                o_bias_grad += out_grad\n",
    "\n",
    "                # Propagate gradient to hidden unit\n",
    "                # (3,1 @ 1,1).T = 1,3\n",
    "                ho_grad = (o_weight @ out_grad).T\n",
    "\n",
    "                if j < x.shape[0] - 1:\n",
    "                    tanh_deriv_next = 1 - hidden[j+1] ** 2\n",
    "                    # 1,3 @ 3,3 @ 3,3\n",
    "                    hh_grad = next_h_grad @ np.diag(tanh_deriv_next) @ h_weight\n",
    "                    h_grad = hh_grad + ho_grad\n",
    "                else:\n",
    "                    h_grad = ho_grad\n",
    "\n",
    "                next_h_grad = h_grad.copy()\n",
    "\n",
    "                tanh_deriv = 1 - hidden[j] ** 2\n",
    "                # 1,3.T @ (1,3 @ 3,3)\n",
    "                i_weight_grad += x[j,:][:,np.newaxis] @ (h_grad @ np.diag(tanh_deriv))\n",
    "\n",
    "                if j > 0:\n",
    "                    # (1,3 @ 3,3).T = 3,1 @ 1,3\n",
    "                    h_weight_grad += (h_grad @ np.diag(tanh_deriv)).T @ hidden[j-1][np.newaxis,:]\n",
    "                    # (1,3) @ 3,3\n",
    "                    h_bias_grad += h_grad @ np.diag(tanh_deriv)\n",
    "\n",
    "            i_weight -= i_weight_grad * lr\n",
    "            h_weight -= h_weight_grad * lr\n",
    "            h_bias -= h_bias_grad * lr\n",
    "            o_weight -= o_weight_grad * lr\n",
    "            o_bias -= o_bias_grad * lr\n",
    "            params[i] = [[i_weight], [h_weight, h_bias], [o_weight, o_bias]]\n",
    "    return params"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 valid loss 168.417877944543\n",
      "Epoch: 1 valid loss 65.52713661084552\n",
      "Epoch: 2 valid loss 60.22897861645778\n",
      "Epoch: 3 valid loss 35.38865763220721\n",
      "Epoch: 4 valid loss 29.288836892861138\n",
      "Epoch: 5 valid loss 27.40697507389607\n",
      "Epoch: 6 valid loss 26.825447763177724\n",
      "Epoch: 7 valid loss 26.715082656554888\n",
      "Epoch: 8 valid loss 26.785892827778163\n",
      "Epoch: 9 valid loss 26.92674368242466\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "lr = 1e-5\n",
    "sequence_len = 7\n",
    "\n",
    "layer_conf = [\n",
    "    {\"type\":\"input\", \"units\": 3},\n",
    "    {\"type\": \"rnn\", \"hidden\": 2, \"output\": 1}\n",
    "]\n",
    "params = init_params(layer_conf)\n",
    "\n",
    "for i in range(epochs):\n",
    "    for j in range(train_x.shape[0] - sequence_len):\n",
    "        seq_x = train_x[j:(j+sequence_len),]\n",
    "        seq_y = train_y[j:(j+sequence_len),]\n",
    "        hiddens, outputs = forward(params, seq_x, layer_conf)\n",
    "        grad = mse_grad(seq_y, outputs[0])\n",
    "        params = backward2(params, seq_x, lr, grad, hiddens, layer_conf)\n",
    "\n",
    "    _, outputs = forward(params, valid_x, layer_conf)\n",
    "    loss = mse(valid_y, outputs[0])\n",
    "\n",
    "    print(f\"Epoch: {i} valid loss {loss}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Original backward\n",
    "x = train_x[:7,]\n",
    "i = 0\n",
    "lr = 1e-4\n",
    "\n",
    "[i_weight], [h_weight, h_bias], [o_weight, o_bias] = params[i]\n",
    "hidden = hiddens[i]\n",
    "h_grads = np.zeros((x.shape[0], h_weight.shape[1]))\n",
    "next_h_grad = None\n",
    "i_weight_grad, h_weight_grad, h_bias_grad, o_weight_grad, o_bias_grad = [0] * 5\n",
    "\n",
    "for j in range(x.shape[0] - 1, -1, -1):\n",
    "    out = grad[j,:][:,np.newaxis]\n",
    "    ho_grad = o_weight @ out\n",
    "    if x.shape[0] - 1 > j:\n",
    "        tanh_deriv_next = 1 - hidden[j+1] ** 2\n",
    "        hh_grad = h_weight.T @ np.diag(tanh_deriv_next) @ next_h_grad\n",
    "        h_grad = ho_grad + hh_grad\n",
    "    else:\n",
    "        h_grad = ho_grad\n",
    "\n",
    "    next_h_grad = h_grad.copy()\n",
    "    tanh_deriv = 1 - hidden[j] ** 2\n",
    "\n",
    "    if j > 0:\n",
    "        h_bias_grad += (np.diag(tanh_deriv) @ h_grad).T\n",
    "        h_weight_grad += (np.diag(tanh_deriv) @ h_grad @ hidden[j-1][:,np.newaxis].T).T\n",
    "\n",
    "    o_bias_grad += out\n",
    "    o_weight_grad += (out @ hidden[j][:,np.newaxis].T).T\n",
    "\n",
    "    i_weight_grad += (np.diag(tanh_deriv) @ h_grad @ x[j,:][:,np.newaxis].T).T\n",
    "\n",
    "i_weight -= i_weight_grad * lr\n",
    "h_weight -= h_weight_grad * lr\n",
    "h_bias -= h_bias_grad * lr\n",
    "o_weight -= o_weight_grad * lr\n",
    "o_bias -= o_bias_grad * lr\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Backward 2\n",
    "x = train_x[:sequence_len,]\n",
    "i = 0\n",
    "lr = 1e-4\n",
    "\n",
    "[i_weight], [h_weight, h_bias], [o_weight, o_bias] = params[i]\n",
    "hidden = hiddens[i]\n",
    "h_grads = np.zeros((x.shape[0], h_weight.shape[1]))\n",
    "next_h_grad = None\n",
    "i_weight_grad, h_weight_grad, h_bias_grad, o_weight_grad, o_bias_grad = [0] * 5\n",
    "\n",
    "for j in range(x.shape[0] - 1, -1, -1):\n",
    "    # 1,1\n",
    "    out_grad = grad[j,:][:,np.newaxis]\n",
    "\n",
    "    # Output updates\n",
    "    # (1,1 @ 1,3).T = 3,1\n",
    "    o_weight_grad += (out_grad @ hidden[j,:][np.newaxis, :]).T\n",
    "    # 1,1\n",
    "    o_bias_grad += out_grad\n",
    "\n",
    "    # Propagate gradient to hidden unit\n",
    "    # (3,1 @ 1,1).T = 1,3\n",
    "    ho_grad = (o_weight @ out_grad).T\n",
    "\n",
    "    if j < x.shape[0] - 1:\n",
    "        tanh_deriv_next = 1 - hidden[j+1] ** 2\n",
    "        # 1,3 @ 3,3 @ 3,3\n",
    "        hh_grad = next_h_grad @ np.diag(tanh_deriv_next) @ h_weight\n",
    "\n",
    "        if j > 0:\n",
    "            # (1,3 @ 3,3).T = 3,1 @ 1,3\n",
    "            tanh_deriv = 1 - hidden[j] ** 2\n",
    "            h_weight_grad += (hh_grad @ np.diag(tanh_deriv)).T @ hidden[j-1][np.newaxis,:]\n",
    "            h_bias_grad += hh_grad @ np.diag(tanh_deriv)\n",
    "\n",
    "        h_grad = hh_grad + ho_grad\n",
    "    else:\n",
    "        h_grad = ho_grad\n",
    "\n",
    "    next_h_grad = h_grad.copy()\n",
    "\n",
    "    # 1,3 @ 1,3\n",
    "    i_weight_grad += x[j,:][:,np.newaxis] @ h_grad\n",
    "\n",
    "i_weight -= i_weight_grad * lr\n",
    "h_weight -= h_weight_grad * lr\n",
    "h_bias -= h_bias_grad * lr\n",
    "o_weight -= o_weight_grad * lr\n",
    "o_bias -= o_bias_grad * lr\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
