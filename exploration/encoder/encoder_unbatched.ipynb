{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset cnn_dailymail (/Users/vik/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de)\n",
      "100%|██████████| 3/3 [00:00<00:00, 190.03it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "TRAIN_LENGTH = 4000\n",
    "VALID_LENGTH = 250\n",
    "SP_VOCAB_SIZE = 1000\n",
    "STOP_TOKEN = SP_VOCAB_SIZE + 1\n",
    "START_TOKEN = SP_VOCAB_SIZE + 2\n",
    "VOCAB_SIZE = SP_VOCAB_SIZE + 3\n",
    "\n",
    "CHUNK_LENGTH = 12\n",
    "\n",
    "# Load from Huggingface datasets module\n",
    "data = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
    "train = data[\"train\"][\"highlights\"][:TRAIN_LENGTH]\n",
    "valid = data[\"validation\"][\"highlights\"][:VALID_LENGTH]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=tokens.txt --model_prefix=cnn --vocab_size=1000 --model_type=unigram\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: tokens.txt\n",
      "  input_format: \n",
      "  model_prefix: cnn\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 1000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(319) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(174) LOG(INFO) Loading corpus: tokens.txt\n",
      "trainer_interface.cc(375) LOG(INFO) Loaded all 15150 sentences\n",
      "trainer_interface.cc(390) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(390) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(390) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(395) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(456) LOG(INFO) all chars count=1089675\n",
      "trainer_interface.cc(467) LOG(INFO) Done: 99.9526% characters are covered.\n",
      "trainer_interface.cc(477) LOG(INFO) Alphabet size=69\n",
      "trainer_interface.cc(478) LOG(INFO) Final character coverage=0.999526\n",
      "trainer_interface.cc(510) LOG(INFO) Done! preprocessed 15150 sentences.\n",
      "unigram_model_trainer.cc(138) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(142) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(193) LOG(INFO) Initialized 45328 seed sentencepieces\n",
      "trainer_interface.cc(516) LOG(INFO) Tokenizing input sentences with whitespace: 15150\n",
      "trainer_interface.cc(526) LOG(INFO) Done! 27997\n",
      "unigram_model_trainer.cc(488) LOG(INFO) Using 27997 sentences for EM training\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=18569 obj=11.9783 num_tokens=57437 num_tokens/piece=3.09317\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=15787 obj=10.0207 num_tokens=57906 num_tokens/piece=3.66795\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=11839 obj=10.0245 num_tokens=61511 num_tokens/piece=5.19562\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=11835 obj=9.97666 num_tokens=61585 num_tokens/piece=5.20363\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=8876 obj=10.1944 num_tokens=67578 num_tokens/piece=7.61356\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=8876 obj=10.1327 num_tokens=67581 num_tokens/piece=7.6139\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=6657 obj=10.4392 num_tokens=74270 num_tokens/piece=11.1567\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=6657 obj=10.3662 num_tokens=74264 num_tokens/piece=11.1558\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=4992 obj=10.742 num_tokens=81504 num_tokens/piece=16.3269\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=4992 obj=10.6641 num_tokens=81530 num_tokens/piece=16.3321\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=3744 obj=11.115 num_tokens=88544 num_tokens/piece=23.6496\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=3744 obj=11.0248 num_tokens=88537 num_tokens/piece=23.6477\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=2808 obj=11.5374 num_tokens=95174 num_tokens/piece=33.8939\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=2808 obj=11.4401 num_tokens=95270 num_tokens/piece=33.9281\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=2106 obj=11.9906 num_tokens=101284 num_tokens/piece=48.0931\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=2106 obj=11.8866 num_tokens=101279 num_tokens/piece=48.0907\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=1579 obj=12.4902 num_tokens=107124 num_tokens/piece=67.8429\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=1579 obj=12.3785 num_tokens=107132 num_tokens/piece=67.848\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=1184 obj=13.0283 num_tokens=112531 num_tokens/piece=95.0431\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=1184 obj=12.9126 num_tokens=112537 num_tokens/piece=95.0481\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=1100 obj=13.0667 num_tokens=114173 num_tokens/piece=103.794\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=1100 obj=13.035 num_tokens=114183 num_tokens/piece=103.803\n",
      "trainer_interface.cc(604) LOG(INFO) Saving model: cnn.model\n",
      "trainer_interface.cc(615) LOG(INFO) Saving vocabs: cnn.vocab\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data.functional import generate_sp_model\n",
    "with open(\"tokens.txt\", \"w+\") as f:\n",
    "    f.write(\"\\n\".join(train) + \"\\n\".join(valid))\n",
    "\n",
    "generate_sp_model(\"tokens.txt\", vocab_size=SP_VOCAB_SIZE, model_prefix=\"cnn\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "import numpy as np\n",
    "from torchtext.data.functional import load_sp_model, sentencepiece_numericalizer\n",
    "import torch\n",
    "\n",
    "sp_base = spm.SentencePieceProcessor(model_file=\"cnn.model\")\n",
    "sp_model = load_sp_model(\"cnn.model\")\n",
    "encoding_generator = sentencepiece_numericalizer(sp_model)\n",
    "\n",
    "def chunk_text(texts):\n",
    "    return [(t[:CHUNK_LENGTH], t[CHUNK_LENGTH:(2 * CHUNK_LENGTH)]) for t in texts if (CHUNK_LENGTH * 2) < len(t)]\n",
    "\n",
    "def decode_ids(ids):\n",
    "    if isinstance(ids, torch.Tensor):\n",
    "        ids = [int(i) for i in list(ids.numpy()) if int(i) not in [START_TOKEN, STOP_TOKEN]]\n",
    "    return sp_base.decode(ids)\n",
    "\n",
    "def encode(tokens):\n",
    "    mat = np.zeros((len(tokens), VOCAB_SIZE))\n",
    "    for i in range(len(tokens)):\n",
    "        mat[i,tokens[i]] = 1\n",
    "    return mat\n",
    "\n",
    "train_ids = list(encoding_generator(train))\n",
    "valid_ids = list(encoding_generator(valid))\n",
    "train_ids = chunk_text(train_ids)\n",
    "valid_ids = chunk_text(valid_ids)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "import math\n",
    "\n",
    "class CNNDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.dataset = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.dataset[idx][0]).int()\n",
    "        y_list = [START_TOKEN] + self.dataset[idx][1] + [STOP_TOKEN]\n",
    "        y = torch.tensor(encode(y_list)).float()\n",
    "        return x.to(device), y.to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1\n",
    "\n",
    "train_dataset = CNNDataset(train_ids)\n",
    "valid_dataset = CNNDataset(valid_ids)\n",
    "\n",
    "train = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "valid = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_units, hidden_units, output_units):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.input_units = input_units\n",
    "        self.hidden_units = hidden_units\n",
    "\n",
    "        k = 1/math.sqrt(hidden_units)\n",
    "        self.input_weight = nn.Parameter(torch.rand(input_units, hidden_units) * 2 * k - k)\n",
    "\n",
    "        self.hidden_weight = nn.Parameter(torch.rand(hidden_units, hidden_units) * 2 * k - k)\n",
    "        self.hidden_bias = nn.Parameter(torch.rand(1, hidden_units) * 2 * k - k)\n",
    "\n",
    "        self.output_weight = nn.Parameter(torch.rand(hidden_units, output_units) * 2 * k - k)\n",
    "        self.output_bias = nn.Parameter(torch.rand(1, output_units) * 2 * k - k)\n",
    "\n",
    "    def forward(self, x, prev_hidden):\n",
    "        # Compute the regular RNN forward pass\n",
    "        # Input times weights\n",
    "        input_x = x @ self.input_weight\n",
    "        # Sum input with previous hidden state, and add nonlinearity\n",
    "        # Tanh prevents gradients exploding\n",
    "        hidden_x = torch.tanh(input_x + prev_hidden @ self.hidden_weight + self.hidden_bias)\n",
    "\n",
    "        # Compute output vector\n",
    "        output_y = hidden_x @ self.output_weight + self.output_bias\n",
    "        return hidden_x, output_y"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_units, hidden_units, output_units=VOCAB_SIZE):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.input_units = input_units\n",
    "        self.hidden_units = hidden_units\n",
    "        self.output_units = output_units\n",
    "\n",
    "        k = 1/math.sqrt(hidden_units)\n",
    "        self.hidden_attention_weight = nn.Parameter(torch.rand(hidden_units, hidden_units) * 2 * k - k)\n",
    "        self.context_attention_weight = nn.Parameter(torch.rand(input_units, hidden_units) * 2 * k - k)\n",
    "        self.attention_weight = nn.Parameter(torch.rand(1, hidden_units) * 2 * k - k)\n",
    "\n",
    "        self.context_hidden_weight = nn.Parameter(torch.rand(input_units + output_units, hidden_units) * 2 * k - k)\n",
    "        self.hidden_weight = nn.Parameter(torch.rand(hidden_units, hidden_units) * 2 * k - k)\n",
    "        self.hidden_bias = nn.Parameter(torch.rand(1, hidden_units) * 2 * k - k)\n",
    "\n",
    "        self.output_weight = nn.Parameter(torch.rand(hidden_units, output_units) * 2 * k - k)\n",
    "        self.output_bias = nn.Parameter(torch.rand(1, output_units) * 2 * k - k)\n",
    "\n",
    "    def forward(self, prev_y, prev_hidden, context):\n",
    "        # Compute attention between the encoder hidden states and the previous decoder hidden state\n",
    "        context_attns = context @ self.context_attention_weight\n",
    "        cross = torch.tanh(context_attns + prev_hidden @ self.hidden_attention_weight)\n",
    "        attention = cross @ self.attention_weight.T\n",
    "\n",
    "        # Compute probability for each encoder hidden state, and use it to weight and sum the states\n",
    "        probs = torch.softmax(attention, 0).reshape(context.shape[0])\n",
    "        positional_context = torch.sum(torch.diag(probs) @ context, dim=0).reshape(1, self.input_units)\n",
    "\n",
    "        # Compute a regular rnn.  Cat the context vector and the previous y state.\n",
    "        input_x = torch.cat([positional_context, prev_y], dim=1) @ self.context_hidden_weight\n",
    "        hidden_x = torch.tanh(input_x + prev_hidden @ self.hidden_weight + self.hidden_bias)\n",
    "\n",
    "        output_y = hidden_x @ self.output_weight + self.output_bias\n",
    "        return hidden_x, output_y"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, in_sequence_len, out_sequence_len, hidden_units=512, embedding_len=VOCAB_SIZE):\n",
    "        super(Network, self).__init__()\n",
    "        self.in_sequence_len = in_sequence_len\n",
    "        self.out_sequence_len = out_sequence_len\n",
    "        self.hidden_units = hidden_units\n",
    "        self.embedding_len = embedding_len\n",
    "\n",
    "        self.enc_embedding = nn.Embedding(embedding_len, hidden_units)\n",
    "        self.encoder = Encoder(input_units=hidden_units, hidden_units=hidden_units, output_units=embedding_len)\n",
    "\n",
    "        self.dec_embedding= nn.Embedding(embedding_len, hidden_units)\n",
    "        self.decoder = Decoder(input_units=hidden_units, hidden_units=hidden_units, output_units=embedding_len)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        batch_size = 1\n",
    "        embedded = self.enc_embedding(x)\n",
    "\n",
    "        # Encode the input sequence\n",
    "        enc_hiddens = torch.zeros((batch_size, self.hidden_units))\n",
    "        enc_outputs = torch.zeros((batch_size, self.embedding_len))\n",
    "        for j in range(self.in_sequence_len):\n",
    "            hidden, output = self.encoder(embedded[j,:], enc_hiddens[j])\n",
    "            enc_hiddens = torch.cat((enc_hiddens, hidden), dim=0)\n",
    "            enc_outputs = torch.cat((enc_outputs, output), dim=0)\n",
    "\n",
    "        # Decode to the output sequence\n",
    "        # Pass in context\n",
    "        context = enc_hiddens[1:,:]\n",
    "        dec_hiddens = torch.zeros(batch_size, self.hidden_units)\n",
    "        dec_outputs = torch.zeros((batch_size, self.embedding_len))\n",
    "        for j in range(self.out_sequence_len):\n",
    "            # Use either the actual previous y (from the input), or the generated y if the input sequence is shorter than the generation steps.\n",
    "            prev_y = y[j,:] if y.shape[0] > j else torch.softmax(dec_outputs[j,:], dim=0)\n",
    "            prev_y = torch.unsqueeze(prev_y, 0)\n",
    "            hidden, output = self.decoder(prev_y, dec_hiddens[j,:], context)\n",
    "            dec_hiddens = torch.cat((dec_hiddens, hidden), dim=0)\n",
    "            dec_outputs = torch.cat((dec_outputs, output), dim=0)\n",
    "\n",
    "        return dec_hiddens[1:], dec_outputs[1:]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "model = Network(CHUNK_LENGTH, CHUNK_LENGTH + 2, hidden_units=512).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4000it [02:50, 23.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: Bernard Kerik is accused of | failing to report more than $500,000 in in | sss,, . . . . . .\n",
      "Epoch 0 train loss: 30.804135053157808 valid loss: 31.360035591125488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4000it [02:49, 23.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: Bernard Kerik is accused of | failing to report more than $500,000 in in | yyyyy . . . .\n",
      "Epoch 1 train loss: 26.937010143756865 valid loss: 31.075331840515137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4000it [02:53, 23.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: Bernard Kerik is accused of | failing to report more than $500,000 in in | y theyyyyyud . .\n",
      "Epoch 2 train loss: 23.88312827682495 valid loss: 30.931558448791503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4000it [02:52, 23.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: Bernard Kerik is accused of | failing to report more than $500,000 in in | de theyyingyingxuuing\n",
      "Epoch 3 train loss: 21.32853400802612 valid loss: 30.810572944641113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4000it [02:48, 23.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: Bernard Kerik is accused of | failing to report more than $500,000 in in | de theveongongongingxxuing\n",
      "Epoch 4 train loss: 19.36348403930664 valid loss: 30.732706970214842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4000it [02:48, 23.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: Bernard Kerik is accused of | failing to report more than $500,000 in in | deiveongongongxx P eter\n",
      "Epoch 5 train loss: 17.757305727481842 valid loss: 30.684510650634767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4000it [02:47, 23.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: Bernard Kerik is accused of | failing to report more than $500,000 in in | stveongongongxulul eter\n",
      "Epoch 6 train loss: 16.376285362243653 valid loss: 30.659583000183105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4000it [02:47, 23.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: Bernard Kerik is accused of | failing to report more than $500,000 in in | deveongongong asulul eter\n",
      "Epoch 7 train loss: 15.147891200065613 valid loss: 30.648789009094237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4000it [02:47, 23.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: Bernard Kerik is accused of | failing to report more than $500,000 in in | deveongongong asulul eter\n",
      "Epoch 8 train loss: 14.051488743662834 valid loss: 30.668934272766112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4000it [02:47, 23.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: Bernard Kerik is accused of | failing to report more than $500,000 in in | deveongongongatululateter\n",
      "Epoch 9 train loss: 13.077831833124161 valid loss: 30.752536170959473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4000it [02:47, 23.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: Bernard Kerik is accused of | failing to report more than $500,000 in in | deveongongongatululateter\n",
      "Epoch 10 train loss: 12.205075413584709 valid loss: 30.844026260375976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4000it [02:46, 24.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: Bernard Kerik is accused of | failing to report more than $500,000 in in | deveongongveratululate\n",
      "Epoch 11 train loss: 11.415740810751915 valid loss: 31.028409423828126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4000it [02:45, 24.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: Bernard Kerik is accused of | failing to report more than $500,000 in in | hongongongveratulverver\n",
      "Epoch 12 train loss: 10.691717862308025 valid loss: 31.207223571777345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4000it [02:45, 24.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: Bernard Kerik is accused of | failing to report more than $500,000 in in | hveongongververulverver\n",
      "Epoch 13 train loss: 10.029021150767804 valid loss: 31.505417671203613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4000it [02:45, 24.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: Bernard Kerik is accused of | failing to report more than $500,000 in in | hveongongverat Obama Obamaob\n",
      "Epoch 14 train loss: 9.412598944529892 valid loss: 31.83864638519287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4000it [02:41, 24.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: Bernard Kerik is accused of | failing to report more than $500,000 in in | hveongong herat Obama Obamaob\n",
      "Epoch 15 train loss: 8.814672158554197 valid loss: 31.99827053833008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4000it [02:41, 24.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: Bernard Kerik is accused of | failing to report more than $500,000 in in | h al afterong her from Obama Obamaob\n",
      "Epoch 16 train loss: 8.261500295057893 valid loss: 32.279905960083006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4000it [02:41, 24.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: Bernard Kerik is accused of | failing to report more than $500,000 in in | h al afterong her from Obama eob\n",
      "Epoch 17 train loss: 7.733529932945967 valid loss: 32.59251876068115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4000it [02:41, 24.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: Bernard Kerik is accused of | failing to report more than $500,000 in in | ong al afterongong from g eD\n",
      "Epoch 18 train loss: 7.231996523179114 valid loss: 32.75771340179443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4000it [02:41, 24.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: Bernard Kerik is accused of | failing to report more than $500,000 in in | op al afterongong from g eand\n",
      "Epoch 19 train loss: 6.748180015310645 valid loss: 33.12603221130371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4000it [02:41, 24.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: Bernard Kerik is accused of | failing to report more than $500,000 in in | op al alongong from g eand\n",
      "Epoch 20 train loss: 6.267940382130444 valid loss: 33.87939823150635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4000it [02:41, 24.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: Bernard Kerik is accused of | failing to report more than $500,000 in in | ong alongongong from g eand\n",
      "Epoch 21 train loss: 5.792333689756691 valid loss: 34.240970634460446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4000it [02:41, 24.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: Bernard Kerik is accused of | failing to report more than $500,000 in in | ong al along are from g eD\n",
      "Epoch 22 train loss: 5.339578919008374 valid loss: 34.88249142456055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4000it [02:41, 24.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: Bernard Kerik is accused of | failing to report more than $500,000 in in | ong al along from from g eD\n",
      "Epoch 23 train loss: 4.943872916348279 valid loss: 35.18660298919678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4000it [02:41, 24.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: Bernard Kerik is accused of | failing to report more than $500,000 in in | op al along are from g evi\n",
      "Epoch 24 train loss: 4.570649360887706 valid loss: 35.50818719482422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4000it [02:43, 24.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: Bernard Kerik is accused of | failing to report more than $500,000 in in | op al alum from from g Obamavi\n",
      "Epoch 25 train loss: 4.188996695689857 valid loss: 36.046335983276364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4000it [02:51, 23.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: Bernard Kerik is accused of | failing to report more than $500,000 in in | op al along from from g eD\n",
      "Epoch 26 train loss: 3.8400335571169855 valid loss: 36.628919715881345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4000it [02:50, 23.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: Bernard Kerik is accused of | failing to report more than $500,000 in in | and al alill than four gviD\n",
      "Epoch 27 train loss: 3.536618724256754 valid loss: 36.822291473388674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4000it [02:49, 23.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: Bernard Kerik is accused of | failing to report more than $500,000 in in | op al along from Thursday Obama InD\n",
      "Epoch 28 train loss: 3.2408486842513082 valid loss: 37.280677223205565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4000it [02:49, 23.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: Bernard Kerik is accused of | failing to report more than $500,000 in in | op al al time from four gviDated\n",
      "Epoch 29 train loss: 2.969234650429338 valid loss: 37.558863594055175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4000it [02:49, 23.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: Bernard Kerik is accused of | failing to report more than $500,000 in in | chop al along are crash g eviated\n",
      "Epoch 30 train loss: 2.7537641016617416 valid loss: 37.753771484375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4000it [02:49, 23.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: Bernard Kerik is accused of | failing to report more than $500,000 in in | new5 al al or New Mo g InD\n",
      "Epoch 31 train loss: 2.520031101923436 valid loss: 38.7116742401123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4000it [02:49, 23.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: Bernard Kerik is accused of | failing to report more than $500,000 in in | op after al her Maport Obama InD\n",
      "Epoch 32 train loss: 2.363046087026596 valid loss: 38.49000239562988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1314it [00:55, 23.25it/s]"
     ]
    }
   ],
   "source": [
    "EPOCHS = 100\n",
    "for epoch in range(EPOCHS):\n",
    "    # Run over the training examples\n",
    "    train_loss = 0\n",
    "    for batch, (sequence, target) in tqdm(enumerate(train)):\n",
    "        optimizer.zero_grad()\n",
    "        hidden, pred = model(sequence[0,:], target[0,:])\n",
    "\n",
    "        pred = pred.reshape(1, CHUNK_LENGTH + 2, VOCAB_SIZE)\n",
    "        loss = loss_fn(pred, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # Show text generated from training prompt as an example\n",
    "    # Don't feed in all of the train y sequence, just the first token\n",
    "    # The other y tokens will be predicted by the model and fed back in\n",
    "    _, pred = model(sequence[0,:], target[0,0,:].unsqueeze(0))\n",
    "    pred = pred.reshape(1, CHUNK_LENGTH + 2, VOCAB_SIZE)\n",
    "    prompt = decode_ids(sequence[0,:])\n",
    "    text = decode_ids(torch.argmax(pred[0,:], dim=1))\n",
    "    correct_text = decode_ids(torch.argmax(target[0,:], dim=1))\n",
    "    print(f\"Example: {prompt} | {correct_text} | {text}\")\n",
    "\n",
    "    # Compute validation loss.  Unless you have a lot of training data, it won't be able to generalize.\n",
    "    valid_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch, (sequence, target) in enumerate(valid):\n",
    "            # Only feed in the first token of the actual target\n",
    "            hidden, pred = model(sequence[0,:], target[0,0,:].unsqueeze(0))\n",
    "            pred = pred.reshape(1, CHUNK_LENGTH + 2, VOCAB_SIZE)\n",
    "            loss = loss_fn(pred, target)\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch} train loss: {train_loss / len(train)} valid loss: {valid_loss / len(valid)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
