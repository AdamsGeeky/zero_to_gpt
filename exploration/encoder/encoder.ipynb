{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset cnn_dailymail (/Users/vik/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de)\n",
      "100%|██████████| 3/3 [00:00<00:00, 171.17it/s]\n"
     ]
    }
   ],
   "source": [
    "#!pip install torch torchtext datasets sentencepiece numpy\n",
    "from datasets import load_dataset\n",
    "\n",
    "TRAIN_LENGTH = 10000\n",
    "VALID_LENGTH = 250\n",
    "SP_VOCAB_SIZE = 1000\n",
    "STOP_TOKEN = SP_VOCAB_SIZE + 1\n",
    "START_TOKEN = SP_VOCAB_SIZE + 2\n",
    "VOCAB_SIZE = SP_VOCAB_SIZE + 3\n",
    "\n",
    "CHUNK_LENGTH = 12\n",
    "\n",
    "# Load from Huggingface datasets module\n",
    "data = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
    "train = data[\"train\"][\"highlights\"][:TRAIN_LENGTH]\n",
    "valid = data[\"validation\"][\"highlights\"][:VALID_LENGTH]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=tokens.txt --model_prefix=cnn --vocab_size=1000 --model_type=unigram\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: tokens.txt\n",
      "  input_format: \n",
      "  model_prefix: cnn\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 1000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(319) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(174) LOG(INFO) Loading corpus: tokens.txt\n",
      "trainer_interface.cc(375) LOG(INFO) Loaded all 36997 sentences\n",
      "trainer_interface.cc(390) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(390) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(390) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(395) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(456) LOG(INFO) all chars count=2699421\n",
      "trainer_interface.cc(467) LOG(INFO) Done: 99.9541% characters are covered.\n",
      "trainer_interface.cc(477) LOG(INFO) Alphabet size=69\n",
      "trainer_interface.cc(478) LOG(INFO) Final character coverage=0.999541\n",
      "trainer_interface.cc(510) LOG(INFO) Done! preprocessed 36997 sentences.\n",
      "unigram_model_trainer.cc(138) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(142) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(193) LOG(INFO) Initialized 74087 seed sentencepieces\n",
      "trainer_interface.cc(516) LOG(INFO) Tokenizing input sentences with whitespace: 36997\n",
      "trainer_interface.cc(526) LOG(INFO) Done! 48737\n",
      "unigram_model_trainer.cc(488) LOG(INFO) Using 48737 sentences for EM training\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=30134 obj=11.7602 num_tokens=100033 num_tokens/piece=3.31961\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=25442 obj=9.70324 num_tokens=100716 num_tokens/piece=3.95865\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=19076 obj=9.69593 num_tokens=106618 num_tokens/piece=5.58912\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=19067 obj=9.66085 num_tokens=106650 num_tokens/piece=5.59343\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=14299 obj=9.82406 num_tokens=116548 num_tokens/piece=8.15078\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=14299 obj=9.78077 num_tokens=116574 num_tokens/piece=8.1526\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=10724 obj=10.0186 num_tokens=127853 num_tokens/piece=11.9221\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=10724 obj=9.96494 num_tokens=127848 num_tokens/piece=11.9217\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=8043 obj=10.2667 num_tokens=139593 num_tokens/piece=17.3558\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=8043 obj=10.204 num_tokens=139579 num_tokens/piece=17.3541\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=6032 obj=10.5625 num_tokens=150957 num_tokens/piece=25.026\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=6032 obj=10.4923 num_tokens=150980 num_tokens/piece=25.0298\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=4524 obj=10.9142 num_tokens=161625 num_tokens/piece=35.7261\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=4524 obj=10.8353 num_tokens=161699 num_tokens/piece=35.7425\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=3393 obj=11.3128 num_tokens=172155 num_tokens/piece=50.7383\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=3393 obj=11.2246 num_tokens=172172 num_tokens/piece=50.7433\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=2544 obj=11.748 num_tokens=181563 num_tokens/piece=71.3691\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=2544 obj=11.6517 num_tokens=181714 num_tokens/piece=71.4285\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=1908 obj=12.2306 num_tokens=191288 num_tokens/piece=100.256\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=1908 obj=12.1211 num_tokens=191310 num_tokens/piece=100.267\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=1431 obj=12.7455 num_tokens=200333 num_tokens/piece=139.995\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=1431 obj=12.6295 num_tokens=200373 num_tokens/piece=140.023\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=1100 obj=13.2077 num_tokens=208888 num_tokens/piece=189.898\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=1100 obj=13.0967 num_tokens=208929 num_tokens/piece=189.935\n",
      "trainer_interface.cc(604) LOG(INFO) Saving model: cnn.model\n",
      "trainer_interface.cc(615) LOG(INFO) Saving vocabs: cnn.vocab\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data.functional import generate_sp_model\n",
    "with open(\"tokens.txt\", \"w+\") as f:\n",
    "    f.write(\"\\n\".join(train) + \"\\n\".join(valid))\n",
    "\n",
    "generate_sp_model(\"tokens.txt\", vocab_size=SP_VOCAB_SIZE, model_prefix=\"cnn\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "import numpy as np\n",
    "from torchtext.data.functional import load_sp_model, sentencepiece_numericalizer\n",
    "import torch\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "sp_base = spm.SentencePieceProcessor(model_file=\"cnn.model\")\n",
    "sp_model = load_sp_model(\"cnn.model\")\n",
    "encoding_generator = sentencepiece_numericalizer(sp_model)\n",
    "\n",
    "def chunk_text(texts):\n",
    "    return [(t[:CHUNK_LENGTH], t[CHUNK_LENGTH:(2 * CHUNK_LENGTH)]) for t in texts if (CHUNK_LENGTH * 2) < len(t)]\n",
    "\n",
    "def decode_ids(ids):\n",
    "    if isinstance(ids, torch.Tensor):\n",
    "        ids = [int(i) for i in list(ids.numpy()) if int(i) not in [START_TOKEN, STOP_TOKEN]]\n",
    "    return sp_base.decode(ids)\n",
    "\n",
    "def decode_batch(id_tensor):\n",
    "    decoded = []\n",
    "    for i in range(id_tensor.shape[0]):\n",
    "        decoded.append(decode_ids(id_tensor[i,:]))\n",
    "    return decoded\n",
    "\n",
    "def encode(tokens):\n",
    "    mat = np.zeros((len(tokens), VOCAB_SIZE))\n",
    "    for i in range(len(tokens)):\n",
    "        mat[i,tokens[i]] = 1\n",
    "    return mat\n",
    "\n",
    "train_ids = list(encoding_generator(train))\n",
    "valid_ids = list(encoding_generator(valid))\n",
    "train_ids = chunk_text(train_ids)\n",
    "valid_ids = chunk_text(valid_ids)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "import math\n",
    "\n",
    "class CNNDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.dataset = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.dataset[idx][0]).int()\n",
    "        y_list = [START_TOKEN] + self.dataset[idx][1] + [STOP_TOKEN]\n",
    "        y = torch.tensor(encode(y_list)).float()\n",
    "        return x.to(DEVICE), y.to(DEVICE)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataset = CNNDataset(train_ids)\n",
    "valid_dataset = CNNDataset(valid_ids)\n",
    "\n",
    "train = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "valid = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_units, hidden_units, output_units):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.input_units = input_units\n",
    "        self.hidden_units = hidden_units\n",
    "\n",
    "        k = 1/math.sqrt(hidden_units)\n",
    "        self.input_weight = nn.Parameter(torch.rand(input_units, hidden_units) * 2 * k - k)\n",
    "\n",
    "        self.hidden_weight = nn.Parameter(torch.rand(hidden_units, hidden_units) * 2 * k - k)\n",
    "        self.hidden_bias = nn.Parameter(torch.rand(1, hidden_units) * 2 * k - k)\n",
    "\n",
    "        self.output_weight = nn.Parameter(torch.rand(hidden_units, output_units) * 2 * k - k)\n",
    "        self.output_bias = nn.Parameter(torch.rand(1, output_units) * 2 * k - k)\n",
    "\n",
    "    def forward(self, x, prev_hidden):\n",
    "        # Compute the regular RNN forward pass\n",
    "        # Input times weights\n",
    "        input_x = x @ self.input_weight\n",
    "        # Sum input with previous hidden state, and add nonlinearity\n",
    "        # Tanh prevents gradients exploding\n",
    "        hidden_x = torch.tanh(input_x + prev_hidden @ self.hidden_weight + self.hidden_bias)\n",
    "        # Compute output vector\n",
    "        output_y = hidden_x @ self.output_weight + self.output_bias\n",
    "        return hidden_x, output_y"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_units, hidden_units, output_units=VOCAB_SIZE):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.input_units = input_units\n",
    "        self.hidden_units = hidden_units\n",
    "        self.output_units = output_units\n",
    "\n",
    "        k = 1/math.sqrt(hidden_units)\n",
    "        self.hidden_attention_weight = nn.Parameter(torch.rand(hidden_units, hidden_units) * 2 * k - k)\n",
    "        self.context_attention_weight = nn.Parameter(torch.rand(input_units, hidden_units) * 2 * k - k)\n",
    "        self.attention_weight = nn.Parameter(torch.rand(1, hidden_units) * 2 * k - k)\n",
    "\n",
    "        self.context_hidden_weight = nn.Parameter(torch.rand(input_units + output_units, hidden_units) * 2 * k - k)\n",
    "        self.hidden_weight = nn.Parameter(torch.rand(hidden_units, hidden_units) * 2 * k - k)\n",
    "        self.hidden_bias = nn.Parameter(torch.rand(1, hidden_units) * 2 * k - k)\n",
    "\n",
    "        self.output_weight = nn.Parameter(torch.rand(hidden_units, output_units) * 2 * k - k)\n",
    "        self.output_bias = nn.Parameter(torch.rand(1, output_units) * 2 * k - k)\n",
    "\n",
    "    def forward(self, prev_y, prev_hidden, context):\n",
    "        # Compute attention between the encoder hidden states and the previous decoder hidden state\n",
    "        # Loop over batches\n",
    "        # Swap batch and sequence\n",
    "        \"\"\"\n",
    "        context_attn = torch.bmm(context.swapaxes(0,1), self.context_attention_weight.unsqueeze(0).repeat(context.shape[1],1,1))\n",
    "        cross = torch.tanh(context_attn.swapaxes(0,1) + prev_hidden @ self.hidden_attention_weight)\n",
    "        aw_unrolled = self.attention_weight.repeat(context.shape[1], 1,1).swapaxes(2,1)\n",
    "        attention = torch.bmm(cross, aw_unrolled)\n",
    "        positional_contexts = torch.zeros(1, context.shape[2], device=DEVICE)\n",
    "        for i in range(context.shape[1]):\n",
    "            # Compute probability for each encoder hidden state, and use it to weight and sum the states\n",
    "            probs = torch.softmax(attention[i,:,:], 0).reshape(context.shape[0])\n",
    "            positional_context = torch.sum(torch.diag(probs) @ context[:,i,:], dim=0).reshape(1, self.input_units)\n",
    "            positional_contexts = torch.cat((positional_contexts, positional_context), dim=0)\n",
    "        \"\"\"\n",
    "        positional_contexts = torch.zeros(1, context.shape[2]).to(DEVICE)\n",
    "        for i in range(context.shape[1]):\n",
    "            context_attn = context[:,i,:] @ self.context_attention_weight\n",
    "            cross = torch.tanh(context_attn + prev_hidden[i,:].unsqueeze(0) @ self.hidden_attention_weight)\n",
    "            attention = cross @ self.attention_weight.T\n",
    "\n",
    "            # Compute probability for each encoder hidden state, and use it to weight and sum the states\n",
    "            probs = torch.softmax(attention, 0).reshape(context.shape[0])\n",
    "            positional_context = torch.sum(torch.diag(probs) @ context[:,i,:], dim=0).reshape(1, self.input_units)\n",
    "            positional_contexts = torch.cat((positional_contexts, positional_context), dim=0)\n",
    "\n",
    "        # Compute a regular rnn.  Cat the context vector and the previous y state.\n",
    "        input_x = torch.cat([positional_contexts[1:], prev_y], dim=1) @ self.context_hidden_weight\n",
    "        hidden_x = torch.tanh(input_x + prev_hidden @ self.hidden_weight + self.hidden_bias)\n",
    "\n",
    "        output_y = hidden_x @ self.output_weight + self.output_bias\n",
    "        return hidden_x, output_y"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, in_sequence_len, out_sequence_len, hidden_units=512, embedding_len=VOCAB_SIZE):\n",
    "        super(Network, self).__init__()\n",
    "        self.in_sequence_len = in_sequence_len\n",
    "        self.out_sequence_len = out_sequence_len\n",
    "        self.hidden_units = hidden_units\n",
    "        self.embedding_len = embedding_len\n",
    "\n",
    "        self.enc_embedding = nn.Embedding(embedding_len, hidden_units)\n",
    "        self.encoder = Encoder(input_units=hidden_units, hidden_units=hidden_units, output_units=embedding_len)\n",
    "\n",
    "        self.dec_embedding= nn.Embedding(embedding_len, hidden_units)\n",
    "        self.decoder = Decoder(input_units=hidden_units, hidden_units=hidden_units, output_units=embedding_len)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        batch_size = x.shape[0]\n",
    "        # Move batch to the second dimension, so sequence comes first\n",
    "        y = y.swapaxes(0,1)\n",
    "        # Embed the input sequence to reduce dimensionality\n",
    "        embedded = self.enc_embedding(x).swapaxes(0,1)\n",
    "\n",
    "        # Encode the input sequence\n",
    "        # Both tensors will have sequence then batch\n",
    "        enc_hiddens = torch.zeros((1, batch_size, self.hidden_units), device=DEVICE)\n",
    "        enc_outputs = torch.zeros((1, batch_size, self.embedding_len), device=DEVICE)\n",
    "        for j in range(self.in_sequence_len):\n",
    "            hidden, output = self.encoder(embedded[j,:,:], enc_hiddens[j,:,:])\n",
    "            # Add first sequence axis\n",
    "            hidden = hidden.unsqueeze(0)\n",
    "            output = output.unsqueeze(0)\n",
    "            enc_hiddens = torch.cat((enc_hiddens, hidden), dim=0)\n",
    "            enc_outputs = torch.cat((enc_outputs, output), dim=0)\n",
    "\n",
    "        # Decode to the output sequence\n",
    "        # Pass in context\n",
    "        context = enc_hiddens[1:,:,:]\n",
    "        # Both tensors will have the first dimension be the sequence\n",
    "        dec_hiddens = torch.zeros(1, batch_size, self.hidden_units, device=DEVICE)\n",
    "        dec_outputs = torch.zeros((1, batch_size, self.embedding_len), device=DEVICE)\n",
    "        for j in range(self.out_sequence_len):\n",
    "            # Use either the actual previous y (from the input), or the generated y if the input sequence is shorter than the generation steps.\n",
    "            prev_y = y[j,:,:] if y.shape[0] > j else torch.softmax(dec_outputs[j,:,:], dim=1)\n",
    "            hidden, output = self.decoder(prev_y, dec_hiddens[j,:,:], context)\n",
    "            # Add first sequence axis\n",
    "            hidden = hidden.unsqueeze(0)\n",
    "            output= output.unsqueeze(0)\n",
    "            dec_hiddens = torch.cat((dec_hiddens, hidden), dim=0)\n",
    "            dec_outputs = torch.cat((dec_outputs, output), dim=0)\n",
    "\n",
    "        # Move batch back to axis 0\n",
    "        out_hiddens = dec_hiddens[1:].swapaxes(0,1)\n",
    "        out_output = dec_outputs[1:].swapaxes(0,1)\n",
    "        return out_hiddens, out_output\n",
    "\n",
    "def generate(sequence, target):\n",
    "    _, pred = model(sequence, target[:,0,:].unsqueeze(1))\n",
    "    prompts = decode_batch(sequence.cpu())\n",
    "    texts = decode_batch(torch.argmax(pred, dim=2).cpu())\n",
    "    correct_texts = decode_batch(torch.argmax(target, dim=2).cpu())\n",
    "\n",
    "    displays = []\n",
    "    for p, t, ct in zip(prompts, texts, correct_texts):\n",
    "        displays.append(f\"{p} | {ct} | {t}\")\n",
    "    return displays"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "model = Network(CHUNK_LENGTH, CHUNK_LENGTH + 2, hidden_units=512).to(DEVICE)\n",
    "loss_fn = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected size for first two dimensions of batch2 tensor to be: [12, 512] but got: [32, 512].",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[40], line 9\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m batch, (sequence, target) \u001B[38;5;129;01min\u001B[39;00m tqdm(\u001B[38;5;28menumerate\u001B[39m(train)):\n\u001B[1;32m      8\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m----> 9\u001B[0m     hidden, pred \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43msequence\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     10\u001B[0m     loss \u001B[38;5;241m=\u001B[39m loss_fn(pred, target)\n\u001B[1;32m     11\u001B[0m     loss\u001B[38;5;241m.\u001B[39mbackward()\n",
      "File \u001B[0;32m~/.virtualenvs/nnets/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Cell \u001B[0;32mIn[38], line 43\u001B[0m, in \u001B[0;36mNetwork.forward\u001B[0;34m(self, x, y)\u001B[0m\n\u001B[1;32m     40\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m j \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mout_sequence_len):\n\u001B[1;32m     41\u001B[0m     \u001B[38;5;66;03m# Use either the actual previous y (from the input), or the generated y if the input sequence is shorter than the generation steps.\u001B[39;00m\n\u001B[1;32m     42\u001B[0m     prev_y \u001B[38;5;241m=\u001B[39m y[j,:,:] \u001B[38;5;28;01mif\u001B[39;00m y\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m>\u001B[39m j \u001B[38;5;28;01melse\u001B[39;00m torch\u001B[38;5;241m.\u001B[39msoftmax(dec_outputs[j,:,:], dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m---> 43\u001B[0m     hidden, output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecoder\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprev_y\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdec_hiddens\u001B[49m\u001B[43m[\u001B[49m\u001B[43mj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m:\u001B[49m\u001B[43m,\u001B[49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcontext\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     44\u001B[0m     \u001B[38;5;66;03m# Add first sequence axis\u001B[39;00m\n\u001B[1;32m     45\u001B[0m     hidden \u001B[38;5;241m=\u001B[39m hidden\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m0\u001B[39m)\n",
      "File \u001B[0;32m~/.virtualenvs/nnets/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Cell \u001B[0;32mIn[37], line 27\u001B[0m, in \u001B[0;36mDecoder.forward\u001B[0;34m(self, prev_y, prev_hidden, context)\u001B[0m\n\u001B[1;32m     25\u001B[0m cross \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtanh(context_attn\u001B[38;5;241m.\u001B[39mswapaxes(\u001B[38;5;241m0\u001B[39m,\u001B[38;5;241m1\u001B[39m) \u001B[38;5;241m+\u001B[39m prev_hidden \u001B[38;5;241m@\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhidden_attention_weight)\n\u001B[1;32m     26\u001B[0m aw_unrolled \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mattention_weight\u001B[38;5;241m.\u001B[39mrepeat(context\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m], \u001B[38;5;241m1\u001B[39m,\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39mswapaxes(\u001B[38;5;241m2\u001B[39m,\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m---> 27\u001B[0m attention \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbmm\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcross\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maw_unrolled\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     28\u001B[0m positional_contexts \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mzeros(\u001B[38;5;241m1\u001B[39m, context\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m2\u001B[39m], device\u001B[38;5;241m=\u001B[39mDEVICE)\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(context\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m]):\n\u001B[1;32m     30\u001B[0m     \u001B[38;5;66;03m# Compute probability for each encoder hidden state, and use it to weight and sum the states\u001B[39;00m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Expected size for first two dimensions of batch2 tensor to be: [12, 512] but got: [32, 512]."
     ]
    }
   ],
   "source": [
    "EPOCHS = 100\n",
    "DISPLAY_BATCHES = 8\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Run over the training examples\n",
    "    train_loss = 0\n",
    "    for batch, (sequence, target) in tqdm(enumerate(train)):\n",
    "        optimizer.zero_grad()\n",
    "        hidden, pred = model(sequence, target)\n",
    "        loss = loss_fn(pred, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # Show text generated from training prompt as an example\n",
    "    # Don't feed in all of the train y sequences, just the first token\n",
    "    # The other y tokens will be predicted by the model and fed back in\n",
    "    sents = generate(sequence[:DISPLAY_BATCHES], target[:DISPLAY_BATCHES])\n",
    "    for sent in sents:\n",
    "        print(sent)\n",
    "\n",
    "    # Compute validation loss.  Unless you have a lot of training data, the validation loss won't decrease.\n",
    "    valid_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch, (sequence, target) in enumerate(valid):\n",
    "            # Only feed in the first token of the actual target\n",
    "            hidden, pred = model(sequence, target[:,0,:].unsqueeze(1))\n",
    "            loss = loss_fn(pred, target)\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch} train loss: {train_loss / len(train_dataset)} valid loss: {valid_loss / len(valid_dataset)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "import torch\n",
    "attention_weight = torch.rand(1, 512)\n",
    "\n",
    "nw = attention_weight.repeat(2,5,1).swapaxes(1,2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 512, 5])"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nw.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
