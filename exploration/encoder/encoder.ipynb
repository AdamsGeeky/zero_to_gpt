{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset cnn_dailymail (/Users/vik/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de)\n",
      "100%|██████████| 3/3 [00:00<00:00, 188.23it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load from Huggingface datasets module\n",
    "data = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
    "train = data[\"train\"][\"highlights\"]\n",
    "valid = data[\"validation\"][\"highlights\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "STOP_TOKEN = 2\n",
    "START_TOKEN = 1\n",
    "UNK_TOKEN = 0\n",
    "CHUNK_LENGTH = 12\n",
    "VOCAB_SIZE = 1000\n",
    "TRAIN_LENGTH = 10000\n",
    "VALID_LENGTH = 500"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=tokens.txt --model_prefix=cnn --vocab_size=1000 --model_type=unigram\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: tokens.txt\n",
      "  input_format: \n",
      "  model_prefix: cnn\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 1000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(319) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(174) LOG(INFO) Loading corpus: tokens.txt\n",
      "trainer_interface.cc(136) LOG(INFO) Loaded 1000000 lines\n",
      "trainer_interface.cc(113) LOG(WARNING) Too many sentences are loaded! (1143075), which may slow down training.\n",
      "trainer_interface.cc(115) LOG(WARNING) Consider using --input_sentence_size=<size> and --shuffle_input_sentence=true.\n",
      "trainer_interface.cc(118) LOG(WARNING) They allow to randomly sample <size> sentences from the entire corpus.\n",
      "trainer_interface.cc(375) LOG(INFO) Loaded all 1143075 sentences\n",
      "trainer_interface.cc(390) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(390) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(390) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(395) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(456) LOG(INFO) all chars count=89305322\n",
      "trainer_interface.cc(467) LOG(INFO) Done: 99.9543% characters are covered.\n",
      "trainer_interface.cc(477) LOG(INFO) Alphabet size=73\n",
      "trainer_interface.cc(478) LOG(INFO) Final character coverage=0.999543\n",
      "trainer_interface.cc(510) LOG(INFO) Done! preprocessed 1143075 sentences.\n",
      "unigram_model_trainer.cc(138) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(142) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(193) LOG(INFO) Initialized 428480 seed sentencepieces\n",
      "trainer_interface.cc(516) LOG(INFO) Tokenizing input sentences with whitespace: 1143075\n",
      "trainer_interface.cc(526) LOG(INFO) Done! 404220\n",
      "unigram_model_trainer.cc(488) LOG(INFO) Using 404220 sentences for EM training\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=175060 obj=10.9811 num_tokens=900457 num_tokens/piece=5.14371\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=147539 obj=8.80162 num_tokens=904667 num_tokens/piece=6.13171\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=110644 obj=8.76611 num_tokens=941025 num_tokens/piece=8.50498\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=110607 obj=8.75779 num_tokens=941312 num_tokens/piece=8.51042\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=82955 obj=8.79043 num_tokens=998829 num_tokens/piece=12.0406\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=82953 obj=8.78059 num_tokens=998748 num_tokens/piece=12.0399\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=62214 obj=8.83284 num_tokens=1063009 num_tokens/piece=17.0863\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=62214 obj=8.82228 num_tokens=1062855 num_tokens/piece=17.0839\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=46660 obj=8.90102 num_tokens=1129688 num_tokens/piece=24.2111\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=46660 obj=8.88666 num_tokens=1129565 num_tokens/piece=24.2084\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=34995 obj=8.99277 num_tokens=1197789 num_tokens/piece=34.2274\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=34995 obj=8.97313 num_tokens=1197666 num_tokens/piece=34.2239\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=26246 obj=9.11473 num_tokens=1266306 num_tokens/piece=48.2476\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=26246 obj=9.0884 num_tokens=1266205 num_tokens/piece=48.2437\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=19684 obj=9.27099 num_tokens=1335078 num_tokens/piece=67.8255\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=19684 obj=9.23772 num_tokens=1335001 num_tokens/piece=67.8216\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=14763 obj=9.46124 num_tokens=1402908 num_tokens/piece=95.0287\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=14763 obj=9.41969 num_tokens=1402902 num_tokens/piece=95.0282\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=11072 obj=9.6842 num_tokens=1471816 num_tokens/piece=132.931\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=11072 obj=9.63366 num_tokens=1472142 num_tokens/piece=132.961\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=8304 obj=9.94806 num_tokens=1539595 num_tokens/piece=185.404\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=8304 obj=9.8881 num_tokens=1539687 num_tokens/piece=185.415\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=6228 obj=10.2513 num_tokens=1608117 num_tokens/piece=258.208\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=6228 obj=10.1824 num_tokens=1609102 num_tokens/piece=258.366\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=4671 obj=10.595 num_tokens=1677565 num_tokens/piece=359.145\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=4671 obj=10.5154 num_tokens=1677904 num_tokens/piece=359.217\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=3503 obj=10.9607 num_tokens=1743988 num_tokens/piece=497.856\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=3503 obj=10.8727 num_tokens=1744288 num_tokens/piece=497.941\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=2627 obj=11.3602 num_tokens=1810596 num_tokens/piece=689.226\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=2627 obj=11.2667 num_tokens=1810892 num_tokens/piece=689.338\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=1970 obj=11.8119 num_tokens=1875083 num_tokens/piece=951.819\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=1970 obj=11.7066 num_tokens=1875342 num_tokens/piece=951.95\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=1477 obj=12.2687 num_tokens=1952535 num_tokens/piece=1321.96\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=1477 obj=12.1563 num_tokens=1952999 num_tokens/piece=1322.27\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=1107 obj=12.7543 num_tokens=2025975 num_tokens/piece=1830.15\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=1107 obj=12.626 num_tokens=2026261 num_tokens/piece=1830.41\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=1100 obj=12.6349 num_tokens=2032549 num_tokens/piece=1847.77\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=1100 obj=12.6294 num_tokens=2032663 num_tokens/piece=1847.88\n",
      "trainer_interface.cc(604) LOG(INFO) Saving model: cnn.model\n",
      "trainer_interface.cc(615) LOG(INFO) Saving vocabs: cnn.vocab\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data.functional import generate_sp_model\n",
    "with open(\"tokens.txt\", \"w+\") as f:\n",
    "    f.write(\"\\n\".join(train) + \"\\n\".join(valid))\n",
    "\n",
    "generate_sp_model(\"tokens.txt\", vocab_size=VOCAB_SIZE, model_prefix=\"cnn\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "import numpy as np\n",
    "from torchtext.data.functional import load_sp_model, sentencepiece_numericalizer\n",
    "import torch\n",
    "\n",
    "sp_base = spm.SentencePieceProcessor(model_file=\"cnn.model\")\n",
    "sp_model = load_sp_model(\"cnn.model\")\n",
    "encoding_generator = sentencepiece_numericalizer(sp_model)\n",
    "\n",
    "def chunk_text(texts):\n",
    "    return [(t[:CHUNK_LENGTH], t[CHUNK_LENGTH:(2 * CHUNK_LENGTH)]) for t in texts if (CHUNK_LENGTH * 2) < len(t)]\n",
    "\n",
    "def decode_ids(ids):\n",
    "    if isinstance(ids, torch.Tensor):\n",
    "        ids = [int(i) for i in list(ids.numpy())]\n",
    "    return sp_base.decode(ids)\n",
    "\n",
    "def encode(tokens):\n",
    "    mat = np.zeros((len(tokens), VOCAB_SIZE))\n",
    "    for i in range(len(tokens)):\n",
    "        mat[i,tokens[i]] = 1\n",
    "    return mat\n",
    "\n",
    "train_ids = list(encoding_generator(train[:TRAIN_LENGTH]))\n",
    "valid_ids = list(encoding_generator(valid[:VALID_LENGTH]))\n",
    "train_ids = chunk_text(train_ids)\n",
    "valid_ids = chunk_text(valid_ids)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "import math\n",
    "\n",
    "class CNNDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.dataset = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(encode(self.dataset[idx][0])).float()\n",
    "        y = torch.tensor(encode(self.dataset[idx][1])).float()\n",
    "        return x, y"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1\n",
    "\n",
    "train_dataset = CNNDataset(train_ids)\n",
    "valid_dataset = CNNDataset(valid_ids)\n",
    "\n",
    "train = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "valid = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_units=VOCAB_SIZE, hidden_units=512, sequence_len=CHUNK_LENGTH):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.input_units = input_units\n",
    "        self.hidden_units = hidden_units\n",
    "        self.sequence_len = sequence_len\n",
    "\n",
    "        k = 1/math.sqrt(hidden_units)\n",
    "        self.input_weight = nn.Parameter(torch.rand(input_units, hidden_units) * 2 * k - k)\n",
    "\n",
    "        self.hidden_weight = nn.Parameter(torch.rand(hidden_units, hidden_units) * 2 * k - k)\n",
    "        self.hidden_bias = nn.Parameter(torch.rand(1, hidden_units) * 2 * k - k)\n",
    "\n",
    "    def forward(self, x):\n",
    "        hiddens = [torch.zeros((1, self.hidden_units))]\n",
    "        for j in range(self.sequence_len):\n",
    "            prev_hidden_index = max(0, j-1)\n",
    "\n",
    "            input_x = x[j,:] @ self.input_weight\n",
    "            hiddens.append(torch.tanh(input_x + hiddens[prev_hidden_index] @ self.hidden_weight + self.hidden_bias))\n",
    "        return torch.cat(hiddens[1:], dim=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_units=512, hidden_units=512, output_units=VOCAB_SIZE, in_sequence_len=CHUNK_LENGTH, out_sequence_len=CHUNK_LENGTH):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.input_units = input_units\n",
    "        self.hidden_units = hidden_units\n",
    "        self.output_units = output_units\n",
    "        self.in_sequence_len = in_sequence_len\n",
    "        self.out_sequence_len = out_sequence_len\n",
    "\n",
    "        k = 1/math.sqrt(hidden_units)\n",
    "        self.hidden_attention_weight = nn.Parameter(torch.rand(hidden_units, hidden_units) * 2 * k - k)\n",
    "        self.input_attention_weight = nn.Parameter(torch.rand(input_units, hidden_units) * 2 * k - k)\n",
    "        self.attention_weight = nn.Parameter(torch.rand(1, hidden_units) * 2 * k - k)\n",
    "\n",
    "\n",
    "        self.context_hidden_weight = nn.Parameter(torch.rand(input_units, hidden_units) * 2 * k - k)\n",
    "        self.hidden_weight = nn.Parameter(torch.rand(hidden_units, hidden_units) * 2 * k - k)\n",
    "        self.hidden_bias = nn.Parameter(torch.rand(1, hidden_units) * 2 * k - k)\n",
    "\n",
    "        self.output_weight = nn.Parameter(torch.rand(hidden_units, output_units) * 2 * k - k)\n",
    "        self.output_bias = nn.Parameter(torch.rand(1, output_units) * 2 * k - k)\n",
    "\n",
    "    def forward(self, context):\n",
    "        hiddens = [torch.zeros(1, self.hidden_units)]\n",
    "        outputs = [torch.zeros(1, self.output_units)]\n",
    "        context_attns = context @ self.input_attention_weight\n",
    "        for i in range(self.out_sequence_len):\n",
    "            prev_hidden_index = max(0, i-1)\n",
    "            cross = torch.tanh(context_attns + hiddens[prev_hidden_index] @ self.hidden_attention_weight)\n",
    "            attention = cross @ self.attention_weight.T\n",
    "\n",
    "            probs = torch.softmax(attention, 0).reshape(self.in_sequence_len)\n",
    "            positional_context = torch.sum(torch.diag(probs) @ context, dim=0).reshape(1, self.input_units)\n",
    "\n",
    "            input_x = positional_context @ self.input_attention_weight\n",
    "\n",
    "            hiddens.append(torch.tanh(input_x + hiddens[prev_hidden_index] @ self.hidden_weight + self.hidden_bias))\n",
    "            outputs.append(hiddens[i] @ self.output_weight + self.output_bias)\n",
    "        return torch.cat(hiddens[1:], dim=0), torch.cat(outputs[1:], dim=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, in_sequence_len, out_sequence_len):\n",
    "        super(Network, self).__init__()\n",
    "        self.encoder = Encoder(sequence_len=in_sequence_len, input_units=VOCAB_SIZE)\n",
    "        self.decoder = Decoder(in_sequence_len=in_sequence_len, out_sequence_len=out_sequence_len, output_units=VOCAB_SIZE)\n",
    "\n",
    "    def forward(self, x):\n",
    "        hiddens = self.encoder(x)\n",
    "        hiddens, outputs = self.decoder(hiddens)\n",
    "        return hiddens, outputs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6422it [03:02, 29.06it/s]"
     ]
    }
   ],
   "source": [
    "from statistics import mean\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "model = Network(CHUNK_LENGTH, CHUNK_LENGTH).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "EPOCHS = 5\n",
    "for epoch in range(EPOCHS):\n",
    "    for batch, (sequence, target) in tqdm(enumerate(train)):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        sequence = sequence.to(device)\n",
    "        hidden, pred = model(sequence[0,:,:])\n",
    "\n",
    "        pred = pred.reshape(1, CHUNK_LENGTH, VOCAB_SIZE)\n",
    "        loss = loss_fn(pred, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for batch, (sequence, target) in enumerate(valid):\n",
    "            sequence = sequence.to(device)\n",
    "            hidden, pred = model(sequence[0,:,:])\n",
    "            pred = pred.reshape(1, CHUNK_LENGTH, VOCAB_SIZE)\n",
    "            loss = loss_fn(pred, target)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    prompt = decode_ids(torch.argmax(sequence[0,:,:], dim=1))\n",
    "    text = decode_ids(torch.argmax(pred[0,:,:], dim=1))\n",
    "    print(f\"Epoch {epoch} valid loss: {mean(losses)}\")\n",
    "    print(f\"Example: {prompt} | {text}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
