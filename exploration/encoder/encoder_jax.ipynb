{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset cnn_dailymail (/Users/vik/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de)\n",
      "100%|██████████| 3/3 [00:00<00:00, 150.29it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load from Huggingface datasets module\n",
    "data = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
    "train = data[\"train\"][\"highlights\"]\n",
    "valid = data[\"validation\"][\"highlights\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "STOP_TOKEN = 2\n",
    "START_TOKEN = 1\n",
    "UNK_TOKEN = 0\n",
    "MAX_INPUT_CHARS = 192"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def split_words(text):\n",
    "    return list(text)\n",
    "\n",
    "def clean_text(text):\n",
    "    return re.sub(r'[^a-zA-Z0-9_\\-\\n ]', '', text)\n",
    "\n",
    "def compute_tokens(texts, min_count=100):\n",
    "    token_counts = {}\n",
    "    for text in texts:\n",
    "        words = split_words(clean_text(text))\n",
    "        for word in words:\n",
    "            if word not in token_counts:\n",
    "                token_counts[word] = 0\n",
    "            token_counts[word] += 1\n",
    "\n",
    "    token_map = {\"<unk>\": UNK_TOKEN, \"<s>\": START_TOKEN, \"</s>\": STOP_TOKEN}\n",
    "    token_counter = 3\n",
    "    for k, v in token_counts.items():\n",
    "        if v >= min_count:\n",
    "            token_map[k] = token_counter\n",
    "            token_counter += 1\n",
    "    return token_map\n",
    "\n",
    "def tokenize(text, token_map):\n",
    "    # Add end token to stop generation\n",
    "    words = split_words(clean_text(text))\n",
    "    list_tokens = [token_map.get(w, UNK_TOKEN) for w in words] + [STOP_TOKEN]\n",
    "    tokens = jnp.array(list_tokens)\n",
    "    # Remove leading zeros to avoid sequence issues\n",
    "    tokens = jnp.trim_zeros(tokens, 'f')\n",
    "    return tokens\n",
    "\n",
    "def reverse_tokenize(tokens, reverse_map):\n",
    "    words = map(lambda x: reverse_map[x], tokens)\n",
    "    return \" \".join(words)\n",
    "\n",
    "def encode(tokens, token_map, max_seq_len):\n",
    "    mat = np.zeros((max_seq_len, len(token_map)))\n",
    "    for i in range(tokens.shape[0]):\n",
    "        mat[i,tokens[i]] = 1\n",
    "    for i in range(tokens.shape[0], max_seq_len):\n",
    "        # Add padding token if sequence is too short\n",
    "        mat[i,STOP_TOKEN] = 1\n",
    "    return mat\n",
    "\n",
    "train = [t for t in train if len(t) < MAX_INPUT_CHARS]\n",
    "valid = [v for v in valid if len(v) < MAX_INPUT_CHARS]\n",
    "\n",
    "token_map = compute_tokens(train + valid, min_count=max(2, len(train) / 1000))\n",
    "reverse_token_map = {v:k for k,v in token_map.items()}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68\n",
      "35508\n"
     ]
    }
   ],
   "source": [
    "print(len(token_map))\n",
    "print(len(train))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap, tree_map, value_and_grad, debug\n",
    "from statistics import mean\n",
    "\n",
    "def init_params(layer_conf):\n",
    "    layers = []\n",
    "    for i in range(len(layer_conf)):\n",
    "        if layer_conf[i][\"type\"] == \"input\":\n",
    "            continue\n",
    "        elif layer_conf[i][\"type\"] == \"encoder\":\n",
    "            np.random.seed(0)\n",
    "            k = 1/math.sqrt(layer_conf[i][\"hidden\"])\n",
    "            input_weights = np.random.rand(layer_conf[i-1][\"units\"], layer_conf[i][\"hidden\"]) * 2 * k - k\n",
    "\n",
    "            hidden_weights = np.random.rand(layer_conf[i][\"hidden\"], layer_conf[i][\"hidden\"]) * 2 * k - k\n",
    "            hidden_bias = np.random.rand(1, layer_conf[i][\"hidden\"]) * 2 * k - k\n",
    "\n",
    "            output_weights = np.random.rand(layer_conf[i][\"hidden\"], layer_conf[i][\"output\"]) * 2 * k - k\n",
    "            output_bias = np.random.rand(1, layer_conf[i][\"output\"]) * 2 * k - k\n",
    "\n",
    "            layers.append(\n",
    "                [[input_weights], [hidden_weights, hidden_bias], [output_weights, output_bias]]\n",
    "            )\n",
    "    return layers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "def encoder_fwd(params, prev_hidden, x_example):\n",
    "    [i_weight], [h_weight, h_bias], [o_weight, o_bias] = params\n",
    "    input_x = x_example @ i_weight\n",
    "    hidden_x = input_x + prev_hidden @ h_weight + h_bias\n",
    "    # Activation.  tanh avoids outputs getting larger and larger.\n",
    "    hidden_x = jnp.tanh(hidden_x)\n",
    "\n",
    "    # Output layer\n",
    "    output_x = hidden_x @ o_weight + o_bias\n",
    "    return hidden_x, output_x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "def decoder_fwd(params, prev_hidden, context):\n",
    "    [c_weight], [h_weight, h_bias], [o_weight, o_bias] = params\n",
    "    input_x = context @ c_weight\n",
    "    hidden_x = input_x + prev_hidden @ h_weight + h_bias\n",
    "    # Activation.  tanh avoids outputs getting larger and larger.\n",
    "    hidden_x = jnp.tanh(hidden_x)\n",
    "\n",
    "    # Activation\n",
    "    output_x = hidden_x @ o_weight + o_bias\n",
    "    return hidden_x, output_x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "def softmax(preds):\n",
    "    tol = 1e-6\n",
    "    preds = jnp.exp(preds - jnp.max(preds))\n",
    "    return preds / (jnp.sum(preds) - tol)\n",
    "\n",
    "def log_loss(encoder_params, prev_hidden, x, y):\n",
    "    hidden, output = encoder_fwd(encoder_params, prev_hidden, x)\n",
    "    output = softmax(output)\n",
    "    tol = 1e-6\n",
    "    cross_entropy = jnp.multiply(y, jnp.log(output + tol))\n",
    "    loss = -jnp.sum(cross_entropy)\n",
    "    return loss, {\"hidden\": hidden, \"output\": output}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "def add_pytrees(pytree1, pytree2, x_ended):\n",
    "  return tree_map(lambda pt1, pt2: pt1 + (pt2 * x_ended), pytree1, pytree2)\n",
    "\n",
    "@jit\n",
    "def update(params, x, y, lr):\n",
    "    encoder_params = params[0]\n",
    "    # Initialize with hidden shape\n",
    "    prev_hidden = jnp.zeros((1, 512))\n",
    "    losses = []\n",
    "    total_grad = None\n",
    "    for j in range(x.shape[0]):\n",
    "        [loss, state], grad = value_and_grad(log_loss, has_aux=True)(encoder_params, prev_hidden, x[j,:], y[j,:])\n",
    "        prev_hidden = state[\"hidden\"]\n",
    "        losses.append(loss)\n",
    "\n",
    "        # If the sequence is over, we won't contribute to the gradient.  This happens when we detect a stop token.\n",
    "        x_ended = (np.argmax(x[j,:]) != STOP_TOKEN).astype(int)\n",
    "        if total_grad is None:\n",
    "            total_grad = grad\n",
    "        else:\n",
    "            total_grad = add_pytrees(total_grad, grad, x_ended)\n",
    "\n",
    "    params[0] = tree_map(\n",
    "        lambda param, grad: param - lr * grad, encoder_params, total_grad\n",
    "    )\n",
    "    return params, losses\n",
    "\n",
    "def batch_update(params, x, y, lr):\n",
    "    return vmap(update, in_axes=[None, 0, 0, None])(params, x, y, lr)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "tokenized = [tokenize(train[z], token_map) for z in range(len(train))]\n",
    "max_seq_len = max([len(t) for t in tokenized])\n",
    "\n",
    "train_x = np.stack([encode(tokenized[z], token_map, max_seq_len) for z in range(len(train))])\n",
    "train_y = np.zeros(train_x.shape)\n",
    "train_y[:,:-1,:] = train_x[:,1:,:]\n",
    "train_y[:,-1,STOP_TOKEN] = 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "epochs = 100\n",
    "lr = 1e-3\n",
    "batch_size = 1\n",
    "\n",
    "layer_conf = [\n",
    "    {\"type\":\"input\", \"units\": len(token_map)},\n",
    "    {\"type\": \"encoder\", \"hidden\": 512, \"output\": len(token_map)}\n",
    "]\n",
    "params = init_params(layer_conf)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35508/35508 [15:39<00:00, 37.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 2.848861174764411\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35508/35508 [16:04<00:00, 36.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss: 2.8117060742297233\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35508/35508 [15:40<00:00, 37.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 loss: 2.793207994907865\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35508/35508 [15:57<00:00, 37.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 loss: 2.8062659272911934\n",
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 492/35508 [00:13<15:32, 37.55it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[46], line 10\u001B[0m\n\u001B[1;32m      8\u001B[0m     seq_y \u001B[38;5;241m=\u001B[39m train_y[j,:,:]\n\u001B[1;32m      9\u001B[0m     \u001B[38;5;66;03m#params, loss = batch_update(params, seq_x, seq_y, lr)\u001B[39;00m\n\u001B[0;32m---> 10\u001B[0m     params, loss \u001B[38;5;241m=\u001B[39m \u001B[43mupdate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mseq_x\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mseq_y\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlr\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     11\u001B[0m     epoch_loss\u001B[38;5;241m.\u001B[39mappend(mean([\u001B[38;5;28mfloat\u001B[39m(jnp\u001B[38;5;241m.\u001B[39mmean(l)) \u001B[38;5;28;01mfor\u001B[39;00m l \u001B[38;5;129;01min\u001B[39;00m loss]))\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmean(epoch_loss)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(epochs):\n",
    "    sequence_len = 7\n",
    "    print(f\"Epoch {i}\")\n",
    "    epoch_loss = []\n",
    "    for j in tqdm(range(0, len(train), batch_size)):\n",
    "        batch_inds = list(range(j, j+batch_size))\n",
    "        seq_x = train_x[j,:,:]\n",
    "        seq_y = train_y[j,:,:]\n",
    "        #params, loss = batch_update(params, seq_x, seq_y, lr)\n",
    "        params, loss = update(params, seq_x, seq_y, lr)\n",
    "        epoch_loss.append(mean([float(jnp.mean(l)) for l in loss]))\n",
    "\n",
    "    print(f\"Epoch {i} loss: {mean(epoch_loss)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "gen_length = 60\n",
    "start_text = train[1]\n",
    "tokenized = tokenize(start_text, token_map)\n",
    "encoded = encode(tokenized, token_map, max_seq_len)\n",
    "\n",
    "prev_hidden = jnp.zeros((1, 512))\n",
    "out_seq = []\n",
    "out_text = \"\"\n",
    "for j in range(encoded.shape[0]):\n",
    "    hidden, output = encoder_fwd(params[0], prev_hidden, encoded[j,:])\n",
    "    prev_hidden = hidden\n",
    "    output = softmax(output)\n",
    "    out_ind = int(np.argmax(output))\n",
    "    out_seq.append(out_ind)\n",
    "    out_text += reverse_token_map[out_ind] + \"\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "data": {
      "text/plain": "'rosident \\natt aays \\nhry Mtownaitl bectee ooncer \\ntd aitgowrmbsf toosi wptoet ry oes been artieec por sudteow auys \\nwuow daaseng tf Tutoember 10 sotl be tercess d ty tirdd\\natocgndat          '"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_text"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
