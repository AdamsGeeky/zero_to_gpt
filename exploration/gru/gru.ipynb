{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vik/.virtualenvs/nnets/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "from torch import nn\n",
    "import os\n",
    "import math\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../../data\"))\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 64"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from regression_data import WeatherDatasetWrapper\n",
    "\n",
    "class WeatherDataset(WeatherDatasetWrapper):\n",
    "    predictors = [\"tmax\", \"tmin\", \"rain\"]\n",
    "    target = \"tmax_tomorrow\"\n",
    "    sequence_length = 7\n",
    "\n",
    "wrapper = WeatherDataset(DEVICE)\n",
    "datasets = wrapper.generate_datasets(BATCH_SIZE)\n",
    "train = datasets[\"train\"]\n",
    "valid = datasets[\"validation\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class GRUCell(nn.Module):\n",
    "    def __init__(self, input_units, hidden_units, output_units):\n",
    "        super(GRUCell, self).__init__()\n",
    "        self.input_units = input_units\n",
    "        self.hidden_units = hidden_units\n",
    "        self.output_units = output_units\n",
    "\n",
    "        k = math.sqrt(1/hidden_units)\n",
    "        self.input_weights = nn.Parameter(torch.rand(3, input_units, hidden_units) * 2 * k - k)\n",
    "        self.input_biases = nn.Parameter(torch.rand(3, 1, hidden_units) * 2 * k - k)\n",
    "\n",
    "        self.hidden_weights = nn.Parameter(torch.rand(3, hidden_units, hidden_units) * 2 * k - k)\n",
    "        self.hidden_biases = nn.Parameter(torch.rand(3, 1, hidden_units) * 2 * k - k)\n",
    "\n",
    "    def forward(self, x, prev_hidden):\n",
    "        # Compute the regular RNN forward pass\n",
    "        # Compute update and reset gates for GRU\n",
    "        reset_gate = torch.sigmoid(x @ self.input_weights[0,] + self.input_biases[0,] + prev_hidden @ self.hidden_weights[0,] + self.hidden_biases[0,])\n",
    "        update_gate = torch.sigmoid(x @ self.input_weights[1,] + self.input_biases[1,] + prev_hidden @ self.hidden_weights[1,] + self.hidden_biases[1,])\n",
    "        new_gate = torch.tanh(x @ self.input_weights[2,] + self.input_biases[2,] + torch.mul(reset_gate, prev_hidden @ self.hidden_weights[2,] + self.hidden_biases[2,]))\n",
    "\n",
    "        hidden_x = torch.mul((1 - update_gate), new_gate) + torch.mul(update_gate, new_gate)\n",
    "        return hidden_x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, sequence_len, input_units, output_units, hidden_units=512, layers=2):\n",
    "        super(Network, self).__init__()\n",
    "        self.sequence_len = sequence_len\n",
    "        self.hidden_units = hidden_units\n",
    "        self.input_units = input_units\n",
    "        self.output_units = output_units\n",
    "        self.layers = layers\n",
    "\n",
    "        self.linear_encode = nn.Linear(in_features=input_units, out_features=hidden_units)\n",
    "\n",
    "        self.gru = GRUCell(input_units=hidden_units, hidden_units=hidden_units, output_units=hidden_units)\n",
    "        self.linear_decode = nn.Linear(in_features=hidden_units, out_features=output_units)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        # Embed the input sequence to reduce dimensionality\n",
    "        encoded = self.linear_encode(x).swapaxes(0,1)\n",
    "\n",
    "        # Encode the input sequence\n",
    "        # Both tensors will have sequence then batch\n",
    "        hiddens = torch.zeros((1, batch_size, self.hidden_units), device=DEVICE)\n",
    "        outputs = torch.zeros((1, batch_size, self.output_units), device=DEVICE)\n",
    "        for j in range(self.sequence_len):\n",
    "            hidden = self.gru(encoded[j,:], hiddens[j,])\n",
    "            # Add first sequence axis\n",
    "            output = self.linear_decode(hidden).unsqueeze(0)\n",
    "            hidden = hidden.unsqueeze(0)\n",
    "            outputs = torch.cat((outputs, output), dim=0)\n",
    "            hiddens = torch.cat((hiddens, hidden), dim=0)\n",
    "\n",
    "        # Move batch back to axis 0, and trim first element\n",
    "        out_hiddens = hiddens[1:,:,:].swapaxes(0,1)\n",
    "        out_output = outputs[1:,:,:].swapaxes(0,1)\n",
    "        return out_output, out_hiddens"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "model = Network(wrapper.sequence_length, input_units=len(wrapper.predictors), output_units=1, hidden_units=512, layers=1).to(DEVICE)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "148it [00:07, 18.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss: 183.52500366520238 valid loss: 21.49485394358635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "148it [00:07, 19.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss: 22.28987576510455 valid loss: 21.571421563625336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "148it [00:07, 18.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 train loss: 21.92559670757603 valid loss: 20.713508784770966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "148it [00:07, 18.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 train loss: 21.726916983320905 valid loss: 20.949833065271378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "148it [00:08, 18.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 train loss: 21.62589624765757 valid loss: 21.080862045288086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "148it [00:08, 18.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 train loss: 21.673387424365895 valid loss: 21.142028331756592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "148it [00:08, 18.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 train loss: 21.579473650133288 valid loss: 20.560078501701355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "148it [00:08, 18.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 train loss: 21.614115334845877 valid loss: 20.712094992399216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "148it [00:08, 18.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 train loss: 21.584388036985654 valid loss: 21.313787639141083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "148it [00:08, 18.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 train loss: 21.719323970176077 valid loss: 20.758298337459564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "108it [00:05, 18.21it/s]"
     ]
    }
   ],
   "source": [
    "EPOCHS = 100\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Run over the training examples\n",
    "    train_loss = 0\n",
    "    for batch, (sequence, target) in tqdm(enumerate(train)):\n",
    "        optimizer.zero_grad()\n",
    "        pred, hidden = model(sequence)\n",
    "        loss = loss_fn(pred, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Compute validation loss.  Unless you have a lot of training data, the validation loss won't decrease.\n",
    "        valid_loss = 0\n",
    "        for batch, (sequence, target) in enumerate(valid):\n",
    "            # Only feed in the first token of the actual target\n",
    "            pred, hidden = model(sequence)\n",
    "            loss = loss_fn(pred, target)\n",
    "            valid_loss += loss.item()\n",
    "        print(f\"Epoch {epoch} train loss: {train_loss / len(train)} valid loss: {valid_loss / len(valid)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
