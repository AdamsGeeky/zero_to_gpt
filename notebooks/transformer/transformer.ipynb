{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Based on this paper - https://arxiv.org/pdf/1706.03762.pdf\n",
    "# Might want to move layer norm inside the residual block - https://arxiv.org/pdf/2002.04745.pdf\n",
    "# Layer normalization - https://arxiv.org/pdf/1607.06450.pdf\n",
    "#!pip install torch torchtext sentencepiece datasets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "sys.path.append(os.path.abspath(\"../../data\"))\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 32\n",
    "SP_VOCAB_SIZE = 5000\n",
    "TRAIN_SIZE = 5000"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from text_data import Opus100DatasetWrapper\n",
    "\n",
    "class Wrapper(Opus100DatasetWrapper):\n",
    "    split_lengths = [TRAIN_SIZE, math.floor(TRAIN_SIZE * .1), 100]\n",
    "    x_length = 40\n",
    "    target_length = 40\n",
    "\n",
    "wrapper = Wrapper(SP_VOCAB_SIZE)\n",
    "datasets = wrapper.generate_datasets(BATCH_SIZE)\n",
    "train = datasets[\"train\"]\n",
    "valid = datasets[\"validation\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, input_units, attention_heads, mask=False):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.input_units = input_units\n",
    "        self.attention_heads = attention_heads\n",
    "        self.head_units = int(input_units/attention_heads)\n",
    "        self.mask = mask\n",
    "\n",
    "        k = math.sqrt(1/self.head_units)\n",
    "        self.query_weights = nn.Parameter(torch.rand(self.attention_heads, input_units, self.head_units) * 2 * k - k)\n",
    "        self.key_weights = nn.Parameter(torch.rand(self.attention_heads, input_units, self.head_units) * 2 * k - k)\n",
    "        self.value_weights = nn.Parameter(torch.rand(self.attention_heads, input_units, self.head_units) * 2 * k - k)\n",
    "        self.final_weight = nn.Parameter(torch.rand(self.attention_heads * self.head_units, input_units) * 2 * k - k)\n",
    "        self.input_bias = nn.Parameter(torch.ones(3, self.attention_heads, 1, self.head_units) * 2 * k - k)\n",
    "        self.output_bias = nn.Parameter(torch.ones(1, self.input_units) * 2 * k - k)\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        # convert to 4d tensor with batch_size, attn_heads, seq_len, embedding_dim\n",
    "        exp_queries = queries.unsqueeze(1).expand(-1, self.attention_heads, -1,-1)\n",
    "        exp_keys = keys.unsqueeze(1).expand(-1, self.attention_heads, -1,-1)\n",
    "        exp_values = values.unsqueeze(1).expand(-1, self.attention_heads, -1,-1)\n",
    "\n",
    "        proj_queries = torch.einsum(\"base, aeh->bash\", exp_queries, self.query_weights) + self.input_bias[0]\n",
    "        proj_keys = torch.einsum(\"base, aeh->bash\", exp_keys, self.key_weights) + self.input_bias[1]\n",
    "        proj_values = torch.einsum(\"base, aeh->bash\", exp_values, self.value_weights) + self.input_bias[2]\n",
    "\n",
    "        attention = torch.einsum(\"bash, bahk->bask\", proj_queries, torch.transpose(proj_keys, -1, -2)) / np.sqrt(self.head_units)\n",
    "        if self.mask:\n",
    "            # Prevent decoder queries from looking at tokens that come after\n",
    "            # Do this by setting attention to negative infinity, so it is softmaxed to zero in the next step\n",
    "            mask = torch.zeros((attention.shape[-2], attention.shape[-1]), device=DEVICE)\n",
    "            mask_indices = np.triu_indices(attention.shape[-2], k=1, m=attention.shape[-1])\n",
    "            mask[mask_indices] = -torch.inf\n",
    "            attention += mask\n",
    "\n",
    "        # Softmax on last dimension\n",
    "        # Sequence-wise softmax, so attention between one sequence and other sequences sums to 1\n",
    "        attention = torch.softmax(attention, dim=-1)\n",
    "        weighted_values = torch.einsum(\"bash, bahe->base\", attention, proj_values)\n",
    "\n",
    "        # Swap attention head and sequence axis, then reshape to batch, seq, embedding\n",
    "        weighted_values = weighted_values.swapaxes(1,2).reshape(queries.shape[0], queries.shape[1], -1)\n",
    "        final_values = torch.einsum(\"bse, eo->bso\", weighted_values, self.final_weight) + self.output_bias\n",
    "        return final_values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, input_units, attention_heads, hidden_units=2048, dropout_p=.1):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        self.input_units = input_units\n",
    "        self.attention_heads = attention_heads\n",
    "        self.hidden_units = hidden_units\n",
    "\n",
    "        self.mha = MultiHeadAttention(self.input_units, self.attention_heads)\n",
    "        self.dropouts = nn.ModuleList(nn.Dropout(dropout_p) for _ in range(2))\n",
    "        self.linear1 = nn.Linear(self.input_units, hidden_units)\n",
    "        self.linear2 = nn.Linear(hidden_units, self.input_units)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.lns = nn.ModuleList(nn.LayerNorm(self.input_units) for _ in range(2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        weighted_values = self.dropouts[0](self.mha(x, x, x))\n",
    "        attn_output = self.lns[0](x + weighted_values)\n",
    "\n",
    "        reprojected = self.dropouts[1](self.linear2(self.relu(self.linear1(attn_output))))\n",
    "        block_output = self.lns[1](attn_output + reprojected)\n",
    "        return block_output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, input_units, attention_heads, hidden_units=2048, dropout_p=.1):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.input_units = input_units\n",
    "        self.attention_heads = attention_heads\n",
    "        self.hidden_units = hidden_units\n",
    "\n",
    "        self.in_attn = MultiHeadAttention(self.input_units, self.attention_heads, mask=True)\n",
    "        self.context_attn = MultiHeadAttention(self.input_units, self.attention_heads)\n",
    "        self.dropouts = nn.ModuleList([nn.Dropout(dropout_p) for _ in range(3)])\n",
    "        self.linear1 = nn.Linear(self.input_units, hidden_units)\n",
    "        self.linear2 = nn.Linear(hidden_units, self.input_units)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.lns = nn.ModuleList(nn.LayerNorm(self.input_units) for _ in range(3))\n",
    "\n",
    "    def forward(self, queries, context):\n",
    "        weighted_values = self.dropouts[0](self.in_attn(queries, queries, queries))\n",
    "        attn_output = self.lns[0](queries + weighted_values)\n",
    "\n",
    "        decoder_values = self.dropouts[1](self.context_attn(attn_output, context, context))\n",
    "        decoder_output = self.lns[1](attn_output + decoder_values)\n",
    "\n",
    "        reprojected = self.dropouts[2](self.linear2(self.relu(self.linear1(decoder_output))))\n",
    "        block_output = self.lns[2](decoder_output + reprojected)\n",
    "        return block_output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_units, hidden_units, attention_heads, enc_sequence_len, blocks=1):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.input_units = input_units\n",
    "        self.hidden_units = hidden_units\n",
    "        self.attention_heads = attention_heads\n",
    "        self.blocks = blocks\n",
    "\n",
    "        k = math.sqrt(1/self.hidden_units)\n",
    "        self.output_embedding = nn.Parameter(torch.rand(hidden_units, input_units) * 2 * k - k)\n",
    "        self.output_bias = nn.Parameter(torch.rand(1, input_units) * 2 * k - k)\n",
    "        self.embedding = nn.Embedding(input_units, hidden_units)\n",
    "        self.dropouts = nn.ModuleList(nn.Dropout(.1) for _ in range(2))\n",
    "        self.encoders = nn.ModuleList(EncoderBlock(hidden_units, attention_heads) for _ in range(self.blocks))\n",
    "        self.decoders = nn.ModuleList(DecoderBlock(hidden_units, attention_heads) for _ in range(self.blocks))\n",
    "        self.pos_encoding = self.encoding(enc_sequence_len, self.hidden_units).to(DEVICE)\n",
    "\n",
    "\n",
    "    def embed(self, indices, reverse=False):\n",
    "        if reverse:\n",
    "            return torch.einsum(\"bse, eo->bso\", indices, self.embedding.T)\n",
    "        else:\n",
    "            one_hot = F.one_hot(indices.to(torch.long), num_classes=self.input_units).to(torch.float)\n",
    "            return torch.einsum(\"bso, oe->bse\", one_hot, self.embedding)\n",
    "\n",
    "    def forward(self, x, y, enc_outputs=None):\n",
    "        if enc_outputs is None:\n",
    "            # 3D with batch, seq, embeddings\n",
    "            # TODO: Tie input and output embedding weights\n",
    "            enc_outputs = self.dropouts[0](self.embedding(x) + self.pos_encoding[:x.shape[1]])\n",
    "\n",
    "            for i in range(self.blocks):\n",
    "                enc_outputs = self.encoders[i](enc_outputs)\n",
    "\n",
    "        dec_outputs = self.dropouts[1](self.embedding(y) + self.pos_encoding[:y.shape[1]])\n",
    "        for i in range(self.blocks):\n",
    "            dec_outputs = self.decoders[i](dec_outputs, enc_outputs)\n",
    "\n",
    "        token_vectors = torch.einsum(\"bse, eo->bso\", dec_outputs, self.output_embedding) + self.output_bias\n",
    "        return token_vectors, enc_outputs\n",
    "\n",
    "    def encoding(self, seq_len, embed_len):\n",
    "        encodings = torch.zeros((seq_len, embed_len))\n",
    "        for i in range(seq_len):\n",
    "            all = torch.exp(torch.arange(0, embed_len, 2) * (-math.log(10000.0) / embed_len))\n",
    "            encodings[i, 0::2] = torch.sin(i * all)\n",
    "            encodings[i, 1::2] = torch.cos(i * all)\n",
    "        return encodings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "outputs": [],
   "source": [
    "def generate(sequence, pred, target, wrapper):\n",
    "    prompts = wrapper.decode_batch(sequence.cpu())\n",
    "    texts = wrapper.decode_batch(torch.argmax(pred, dim=2).cpu())\n",
    "    correct_texts = wrapper.decode_batch(target.cpu())\n",
    "\n",
    "    displays = []\n",
    "    for p, t, ct in zip(prompts, texts, correct_texts):\n",
    "        displays.append(f\"{p} | {ct} | {t}\")\n",
    "    return displays"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "# TODO: Profile and improve perf - https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html\n",
    "model = Transformer(wrapper.vocab_size, 512, 8, wrapper.y_length, blocks=1).to(DEVICE)\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=wrapper.pad_token)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127it [00:26,  4.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss: 0.17914133806397595\n",
      "48:14 Porque este Dios es Dios nuestro eternalmente y para siempre: El nos capitaneará hasta la muerte. | PS 48:14 For this God is our God forever and ever. He will be our guide even to death. | The the the the..............\n",
      "- Lo hiciste, eres la traga de la clase. | - You got a 4.0 GPA. | - I',......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14it [00:11,  1.18it/s]"
     ]
    }
   ],
   "source": [
    "EPOCHS = 100\n",
    "DISPLAY_BATCHES = 2\n",
    "OUT_SEQUENCE_LEN = wrapper.y_length\n",
    "PRINT_VALID = True\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Run over the training examples\n",
    "    train_loss = 0\n",
    "    match_pct = 0\n",
    "    for batch, (sequence, target, prev_target) in tqdm(enumerate(train)):\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        pred, _ = model(sequence.to(DEVICE), prev_target.to(DEVICE))\n",
    "\n",
    "        # If you use a batch, need to reshape pred to be batch * sequence, embedding_len to be compatible\n",
    "        # Similar reshape with target to be batch * sequence vector of class indices\n",
    "        loss = loss_fn(pred.reshape(-1, pred.shape[-1]), target.view(-1).to(DEVICE))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        print(f\"Epoch {epoch} train loss: {train_loss / len(train) / BATCH_SIZE}\")\n",
    "        sents = generate(sequence, pred, target, wrapper)\n",
    "        for sent in sents[:DISPLAY_BATCHES]:\n",
    "            print(sent)\n",
    "\n",
    "        if PRINT_VALID and epoch % 10 ==0:\n",
    "            # Compute validation loss.  Unless you have a lot of training data, the validation loss won't decrease.\n",
    "            valid_loss = 0\n",
    "            # Deactivate dropout layers\n",
    "            model.eval()\n",
    "            for batch, (sequence, target, prev_target) in tqdm(enumerate(valid)):\n",
    "                # Inference token by tokens\n",
    "                sequence = sequence.to(DEVICE)\n",
    "                outputs = prev_target[:,0].unsqueeze(1).to(DEVICE)\n",
    "                enc_outputs = None\n",
    "                # TODO: Investigate memory leak with valid generation\n",
    "                for i in range(OUT_SEQUENCE_LEN):\n",
    "                    pred, enc_outputs = model(sequence, outputs, enc_outputs=enc_outputs)\n",
    "                    last_output = torch.argmax(pred, dim=2)\n",
    "                    outputs = torch.cat((outputs, last_output[:,-1:]), dim=1)\n",
    "                loss = loss_fn(pred.reshape(-1, pred.shape[-1]), target.view(-1).to(DEVICE))\n",
    "                valid_loss += loss.item()\n",
    "            print(f\"Valid loss: {valid_loss / len(valid) / BATCH_SIZE}\")\n",
    "            sents = generate(sequence, pred, target, wrapper)\n",
    "            for sent in sents[:DISPLAY_BATCHES]:\n",
    "                print(sent)\n",
    "            # Reactivate dropout\n",
    "            model.train()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "print(summary(model))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "with profile(activities=[ProfilerActivity.CPU], record_shapes=True, ) as prof:\n",
    "    model(sequence.to(DEVICE), prev_target.to(DEVICE))\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "outputs": [],
   "source": [
    "queries = model.embedding(sequence.to(DEVICE))\n",
    "context = model.embedding(prev_target.to(DEVICE))\n",
    "attention_heads = 8\n",
    "query_weights = model.decoders[0].in_attn.query_weights\n",
    "key_weights = model.decoders[0].in_attn.key_weights\n",
    "value_weights = model.decoders[0].in_attn.value_weights\n",
    "\n",
    "# convert to 4d tensor with batch_size, attn_heads, seq_len, embedding_dim\n",
    "exp_queries = queries.unsqueeze(1).expand(-1, attention_heads, -1,-1)\n",
    "exp_context = context.unsqueeze(1).expand(-1, attention_heads, -1,-1)\n",
    "proj_queries = torch.einsum(\"base, aeh->bash\", exp_queries, query_weights)\n",
    "# Transpose keys\n",
    "proj_keys = torch.einsum(\"base, aeh->bahs\", exp_context, key_weights)\n",
    "proj_values = torch.einsum(\"base, aeh->bash\", exp_context, value_weights)\n",
    "\n",
    "attention = torch.einsum(\"bash, bahk->bask\", proj_queries, proj_keys) / np.sqrt(64)\n",
    "\n",
    "# Prevent decoder queries from looking at tokens that come after\n",
    "# Do this by setting attention to negative infinity, so it is softmaxed to zero in the next step\n",
    "mask = torch.zeros((attention.shape[-2], attention.shape[-1]), device=DEVICE)\n",
    "mask_indices = np.triu_indices(attention.shape[-2], k=1, m=attention.shape[-1])\n",
    "mask[mask_indices] = -torch.inf\n",
    "attention += mask\n",
    "\n",
    "# Softmax on last dimension\n",
    "# Sequence-wise softmax, so attention between one sequence and other sequences sums to 1\n",
    "attention = torch.softmax(attention, dim=-1)\n",
    "\n",
    "weighted_values = torch.einsum(\"bash, bahe->base\", attention, proj_values)\n",
    "\n",
    "# Swap attention head and sequence axis, then reshape to batch, seq, embedding\n",
    "weighted_values = weighted_values.swapaxes(1,2).reshape(queries.shape[0], queries.shape[1], -1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manual = torch.softmax((exp_queries[0,1] @ query_weights[1] @ (exp_context[0,1] @ key_weights[1]).T / np.sqrt(64)) + mask, -1) @ (exp_context[0,1] @ value_weights[1])\n",
    "\n",
    "torch.allclose(weighted_values[0,1,64:128], manual[1])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([32, 40, 512])"
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_values.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([1.1553e-03, 5.5856e-04, 7.2043e-02, 5.1467e-04, 7.6951e-03, 9.0066e-03,\n        9.1353e-02, 1.6192e-01, 1.0041e-05, 1.4945e-03, 2.5468e-04, 4.5007e-02,\n        1.9007e-03, 8.3639e-04, 1.1511e-02, 2.1658e-03, 1.6677e-03, 3.5177e-01,\n        1.6545e-05, 2.3807e-02, 1.9574e-02, 1.9574e-02, 1.9574e-02, 1.9574e-02,\n        1.9574e-02, 1.9574e-02, 1.9574e-02, 1.9574e-02, 1.9574e-02, 1.9574e-02,\n        1.9574e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n       grad_fn=<SelectBackward0>)"
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax((exp_queries[0,1] @ query_weights[1] @ (exp_context[0,1] @ key_weights[1]).T / np.sqrt(64)) + mask, -1)[30]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([-1.5150, -2.2417,  2.6179, -2.3235,  0.3813,  0.5386,  2.8554,  3.4278,\n        -6.2604,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n           -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n           -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n           -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n           -inf], grad_fn=<SelectBackward0>)"
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((exp_queries[0,1] @ query_weights[1] @ (exp_context[0,1] @ key_weights[1]).T / np.sqrt(64)) + mask)[8]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([  32, 1206, 2243,    6, 2425,   10, 5000, 5000, 5000, 5000, 5000, 5000,\n        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n        5000, 5000, 5000, 5000], dtype=torch.int32)"
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
