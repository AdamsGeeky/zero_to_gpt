{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vik/.virtualenvs/nnets/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "sys.path.append(os.path.abspath(\"../../data\"))\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 32\n",
    "SP_VOCAB_SIZE = 5000\n",
    "TRAIN_SIZE = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset opus100 (/Users/vik/.cache/huggingface/datasets/opus100/en-es/0.0.0/256f3196b69901fb0c79810ef468e2c4ed84fbd563719920b1ff1fdc750f7704)\n",
      "100%|██████████| 3/3 [00:00<00:00, 223.38it/s]\n",
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=tokens.txt --model_prefix=opus100 --vocab_size=5000 --model_type=unigram\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: tokens.txt\n",
      "  input_format: \n",
      "  model_prefix: opus100\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 5000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(319) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(174) LOG(INFO) Loading corpus: tokens.txt\n",
      "trainer_interface.cc(375) LOG(INFO) Loaded all 9999 sentences\n",
      "trainer_interface.cc(390) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(390) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(390) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(395) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(456) LOG(INFO) all chars count=531332\n",
      "trainer_interface.cc(467) LOG(INFO) Done: 99.9513% characters are covered.\n",
      "trainer_interface.cc(477) LOG(INFO) Alphabet size=100\n",
      "trainer_interface.cc(478) LOG(INFO) Final character coverage=0.999513\n",
      "trainer_interface.cc(510) LOG(INFO) Done! preprocessed 9999 sentences.\n",
      "unigram_model_trainer.cc(138) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(142) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(193) LOG(INFO) Initialized 46136 seed sentencepieces\n",
      "trainer_interface.cc(516) LOG(INFO) Tokenizing input sentences with whitespace: 9999\n",
      "trainer_interface.cc(526) LOG(INFO) Done! 25155\n",
      "unigram_model_trainer.cc(488) LOG(INFO) Using 25155 sentences for EM training\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=16156 obj=12.9249 num_tokens=52196 num_tokens/piece=3.23075\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=14349 obj=10.894 num_tokens=52633 num_tokens/piece=3.66806\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=10759 obj=10.9789 num_tokens=56071 num_tokens/piece=5.21154\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=10752 obj=10.9021 num_tokens=56145 num_tokens/piece=5.22182\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=8063 obj=11.2522 num_tokens=61513 num_tokens/piece=7.62905\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=8063 obj=11.1555 num_tokens=61558 num_tokens/piece=7.63463\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=6047 obj=11.5819 num_tokens=67657 num_tokens/piece=11.1885\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=6047 obj=11.4839 num_tokens=67654 num_tokens/piece=11.188\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=5500 obj=11.618 num_tokens=69591 num_tokens/piece=12.6529\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=5500 obj=11.5897 num_tokens=69593 num_tokens/piece=12.6533\n",
      "trainer_interface.cc(604) LOG(INFO) Saving model: opus100.model\n",
      "trainer_interface.cc(615) LOG(INFO) Saving vocabs: opus100.vocab\n"
     ]
    }
   ],
   "source": [
    "from text_data import Opus100DatasetWrapper\n",
    "\n",
    "class Wrapper(Opus100DatasetWrapper):\n",
    "    split_lengths = [TRAIN_SIZE, math.floor(TRAIN_SIZE * .1), 100]\n",
    "    x_length = 40\n",
    "    target_length = 40\n",
    "\n",
    "wrapper = Wrapper(SP_VOCAB_SIZE)\n",
    "datasets = wrapper.generate_datasets(BATCH_SIZE)\n",
    "train = datasets[\"train\"]\n",
    "valid = datasets[\"validation\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def generate(sequence, pred, target, wrapper):\n",
    "    prompts = wrapper.decode_batch(sequence.cpu())\n",
    "    texts = wrapper.decode_batch(torch.argmax(pred, dim=2).cpu())\n",
    "    correct_texts = wrapper.decode_batch(target.cpu())\n",
    "\n",
    "    displays = []\n",
    "    for p, t, ct in zip(prompts, texts, correct_texts):\n",
    "        displays.append(f\"{p} | {ct} | {t}\")\n",
    "    return displays"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "class TransformerNetwork(nn.Module):\n",
    "    def __init__(self, input_units, hidden_units, attention_heads, blocks=1):\n",
    "        super(TransformerNetwork, self).__init__()\n",
    "        self.transformer = nn.Transformer(hidden_units, attention_heads, num_encoder_layers=blocks, num_decoder_layers=blocks, batch_first=True)\n",
    "        self.embedding = nn.Embedding(input_units, hidden_units)\n",
    "        k = math.sqrt(1/hidden_units)\n",
    "        self.output_embedding = nn.Parameter(torch.rand(hidden_units, input_units) * 2 * k - k)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        embed_x = self.embedding(x)\n",
    "        embed_y = self.embedding(y)\n",
    "        output = self.transformer(embed_x, embed_y)\n",
    "        token_vectors = torch.einsum(\"bse, eo->bso\", output, self.output_embedding)\n",
    "        return token_vectors"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Finishing last run (ID:1681uxlp) before initializing another..."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>█▆▅▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>0.01172</td></tr></table><br/></div></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run <strong style=\"color:#cdcd00\">dry-sponge-6</strong> at: <a href=\"https://wandb.ai/vikp/transformer/runs/1681uxlp\" target=\"_blank\">https://wandb.ai/vikp/transformer/runs/1681uxlp</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>./wandb/run-20230122_183804-1681uxlp/logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Successfully finished last run (ID:1681uxlp). Initializing new run:<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.13.9"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>/Users/vik/Personal/nnets/notebooks/transformer/wandb/run-20230122_205512-megs18pd</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href=\"https://wandb.ai/vikp/transformer/runs/megs18pd\" target=\"_blank\">pt-2-blocks</a></strong> to <a href=\"https://wandb.ai/vikp/transformer\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href=\"https://wandb.ai/vikp/transformer\" target=\"_blank\">https://wandb.ai/vikp/transformer</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href=\"https://wandb.ai/vikp/transformer/runs/megs18pd\" target=\"_blank\">https://wandb.ai/vikp/transformer/runs/megs18pd</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[]"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import wandb\n",
    "\n",
    "wandb.init(project=\"transformer\", notes=\"Pytorch 2 blocks\", name=\"pt-2-blocks\")\n",
    "\n",
    "# TODO: Profile and improve perf - https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html\n",
    "model = TransformerNetwork(wrapper.vocab_size, 512, 8, blocks=2).to(DEVICE)\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=wrapper.pad_token)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "wandb.watch(model, log_freq=100)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127it [01:09,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss: 0.16928739047895267\n",
      "Nunca he estado mejor. | Never better. | No...\n",
      "Te vez tan tonta con tus pantaloncitos. | You are so annoying with your little shorts. | Youss.s.s.sss.s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127it [00:56,  2.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss: 0.14337334652819972\n",
      "Posee un módulo de comunicación por radio y la cámara integrada de 1Mpx le permite reconocer visualmente a los usuarios. | It is equipped with a radio communication module as well as an integrated 1Mpx camera, allowing it to visually recognize users. | It  a   a  , , , a  it a  and,       to. be ⁇   to ,,\n",
      "Ya veo. | Oh, I see. | Oh, I don,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127it [00:52,  2.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 train loss: 0.129019277008969\n",
      "Sire. | Sire. | Sio not\n",
      "Tim, eso es muy halagador, pero volvamos al trabajo. | Tim that is very flattering but let's just stick to business. | Ts.. not. is...'s. just is be.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127it [00:50,  2.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 train loss: 0.11689683683509902\n",
      "Parece que estacionarás en el sol de nuevo. | Looks like you're parking in the sun again. | Look...'s.. the the UK in'.\n",
      "La Sra. Coppins dijo que Ud. estuvo esperando en la calle a la ambulancia que asistió a la señora Davis. | Mrs Coppins said you were waiting in the street for the ambulance that attended to Mrs Davis. | Th to.... in. to to the. the Council. the Commission d. to.. the the to the to toss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127it [00:51,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 train loss: 0.10675911845888679\n",
      "Fui yo. | It was me. | It' me.\n",
      "¿Entiendes eso? | Do you understand that? | Do you doing??\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127it [00:51,  2.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 train loss: 0.09635369784719362\n",
      "Tenías razón. | You were right. | You were right.\n",
      "Estructura | Structure | Sttt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127it [00:59,  2.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 train loss: 0.08556136623846264\n",
      "Yo voy, Lam. | Mama Tina. I'm coming, Lam. | Taam.\n",
      "- Joder, no. | Fuck, no. | Fuck, no,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127it [00:56,  2.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 train loss: 0.07594454939675144\n",
      "Desde cuando? | Since when? | Since when?\n",
      "no le pedimos nada a la empresa, porque nosotros nos ocuparemos de Joe. | We ain't asking the company for nothing, cos we're gonna take care of Joe. | We' bu,t theing for ap, theing asing,ts,, a, the,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127it [00:56,  2.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 train loss: 0.06672253491958295\n",
      "¿La palabra clave? | The codeword? | The cowwrawra\n",
      "Sólo para ponerme a prueba. | - Only to test myself. | - Only....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "118it [00:54,  2.15it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[28], line 17\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;66;03m# If you use a batch, need to reshape pred to be batch * sequence, embedding_len to be compatible\u001B[39;00m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;66;03m# Similar reshape with target to be batch * sequence vector of class indices\u001B[39;00m\n\u001B[1;32m     16\u001B[0m loss \u001B[38;5;241m=\u001B[39m loss_fn(pred\u001B[38;5;241m.\u001B[39mreshape(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, pred\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]), target\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39mto(DEVICE))\n\u001B[0;32m---> 17\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     18\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     19\u001B[0m train_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n",
      "File \u001B[0;32m~/.virtualenvs/nnets/lib/python3.10/site-packages/torch/_tensor.py:488\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    478\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    479\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    480\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    481\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    486\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    487\u001B[0m     )\n\u001B[0;32m--> 488\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    489\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    490\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.virtualenvs/nnets/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    192\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    194\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[1;32m    195\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    196\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 197\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    198\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    199\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.virtualenvs/nnets/lib/python3.10/site-packages/wandb/wandb_torch.py:282\u001B[0m, in \u001B[0;36mTorchHistory._hook_variable_gradient_stats.<locals>.<lambda>\u001B[0;34m(grad)\u001B[0m\n\u001B[1;32m    279\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[1;32m    280\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlog_tensor_stats(grad\u001B[38;5;241m.\u001B[39mdata, name)\n\u001B[0;32m--> 282\u001B[0m handle \u001B[38;5;241m=\u001B[39m var\u001B[38;5;241m.\u001B[39mregister_hook(\u001B[38;5;28;01mlambda\u001B[39;00m grad: _callback(grad, log_track))\n\u001B[1;32m    283\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_hook_handles[name] \u001B[38;5;241m=\u001B[39m handle\n\u001B[1;32m    284\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m handle\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 25\n",
    "DISPLAY_BATCHES = 2\n",
    "OUT_SEQUENCE_LEN = wrapper.y_length\n",
    "PRINT_VALID = True\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Run over the training examples\n",
    "    train_loss = 0\n",
    "    match_pct = 0\n",
    "    for batch, (sequence, target, prev_target) in tqdm(enumerate(train)):\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        pred = model(sequence.to(DEVICE), prev_target.to(DEVICE))\n",
    "\n",
    "        # If you use a batch, need to reshape pred to be batch * sequence, embedding_len to be compatible\n",
    "        # Similar reshape with target to be batch * sequence vector of class indices\n",
    "        loss = loss_fn(pred.reshape(-1, pred.shape[-1]), target.view(-1).to(DEVICE))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        mean_loss = train_loss / len(train) / BATCH_SIZE\n",
    "        wandb.log({\"loss\": mean_loss})\n",
    "        print(f\"Epoch {epoch} train loss: {mean_loss}\")\n",
    "        sents = generate(sequence, pred, target, wrapper)\n",
    "        for sent in sents[:DISPLAY_BATCHES]:\n",
    "            print(sent)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
